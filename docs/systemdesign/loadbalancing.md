A server is essentially a computer running a program or algorithm. A server is defined as something that serves requests.

When a client (e.g., using a mobile phone) connects, they are technically sending a request to use the algorithm.

The server receives the request, runs the algorithm (e.g., facial recognition), and sends back a response (e.g., an image).

Initially, a single computer might handle requests happily. However, if the service becomes popular and receives thousands of requests, the original computer will be unable to handle the volume. When a system can no longer handle the load, the solution is to add new servers.

---

### **Concept of Load Balancing**

Once multiple servers (N servers) are operating, the system must decide where to send incoming requests.

Load refers to the requests which the server needs to process. The server carries this load.

The primary objective is to balance the load evenly on all N servers.

Load balancing is the concept of taking N servers and trying to distribute the load evenly on all of them. The aim is to evenly distribute the weight (load) across all available servers.

***Load Balancing Algorithms***:
    
1. Round Robin
2. Weighted Round Robin
3. Least Connections
4. Random
5. Hashing

---

### **The Initial Approach: Simple Hashing (Modulo Arithmetic)**

A simple approach to load distribution involves using a hash function and the modulo operation based on the number of servers (N).

1.  ***Request ID***: Each request comes with a request ID (r or I), which is expected to be uniformly random, generated by the client between zero and M-1.
2.  ***Hashing***: The request ID is hashed (H(r)) to obtain a number (M1).
3.  ***Mapping to Server***: This number (M1) is mapped to one of the N servers by calculating the remainder: M1 mod N. The remainder corresponds to the server index (e.g., S0, S1, S2, etc.).
4.  ***Uniform Distribution***: Because both the request ID and the hash function are generally considered uniformly random, this approach results in a uniform load distribution.
       If there are X total requests, each of the N servers is expected to have X/N load.
       The load factor for each server is 1/N.

!!! Example 
    
    using 4 servers (N=4):

    Request R1 = 10; H(10) = 3. $3 \text{ mod } 4 = 3$. R1 goes to server S3.
    
    Request R2 = 20; H(20) = 15. $15 \text{ mod } 4 = 3$. R2 goes to server S3.
    
    Request R3 = 35; H(35) = 12. $12 \text{ mod } 4 = 0$. R3 goes to server S0.

---

### **The Limitation: Adding More Servers**

The simple hashing approach fails when the number of servers (N) changes, specifically when more servers are added (e.g., adding S4).

When a fifth server is added, the modulus operation changes from mod 4 to mod 5. This change causes most request assignments to shift dramatically.

!!! Example
    
    R2 (Hash result 15) previously went to S3 ($15 \text{ mod } 4 = 3$). Now, it must go to S0 ($15 \text{ mod } 5 = 0$).
    
    R3 (Hash result 12) previously went to S0 ($12 \text{ mod } 4 = 0$). Now, it must go to S2 ($12 \text{ mod } 5 = 2$).

When transitioning from 4 to 5 servers, the cost of the change—meaning the total number of reassignments—can be 100. 

In this scenario, the change involves the entire sort space (100% of the assignments change). This massive shift means that the requests currently being served are "completely bamboozled".

---

### **Cache Invalidation and The Need for Minimal Change**

The massive shift caused by adding a server is problematic primarily because of how real-world applications utilize the request ID and caching.

In practice, the request ID is rarely random; it typically encapsulates information about the user, such as the user ID.

Because the hash function is constant, hashing a specific user ID (e.g., "Gaurav") will repeatedly yield the same result, sending that user to the same server.

This server stickiness is utilized for efficiency: if a user is repeatedly sent to the same server, that server can store relevant information (like a user profile) in its local cache, avoiding constant fetching from a database.

When the system changes (e.g., mod 4 to mod 5), all users are sent to new, different places. Consequently, the entire system changes, and almost all the useful cached information that was stored on the old servers is dumped or rendered useless.

To preserve cache efficacy, the system must avoid a huge change in the range of numbers (or buckets) being served. A small or tiny change is what is needed.

---

### **The Solution: Consistent Hashing**

The standard way of hashing (modulo N) fails in scaling scenarios due to the catastrophic assignment changes.

Consistent hashing is an advanced approach designed to address this failure.

If a new server is added (e.g., requiring a 20% share of the load), consistent hashing aims to take only a small amount of load from each existing server. The sum of these small losses across the old servers must equal the 20% assigned to the new server, but the overall change must be minimum.


---

### **Analogy**

To understand the difference between simple hashing and consistent hashing, imagine a library where books (requests) are sorted onto shelves (servers) based on the last digit of the book's ID number.

If you have 10 shelves, a book ending in '3' goes to shelf 3. If you decide to add an 11th shelf, under simple hashing, every single book in the library must be re-shelved according to the new "mod 11" rule, essentially destroying the order and making all previous organizational memory (cache) useless. 

Consistent hashing, however, would only require a small, localized shift of books (requests) from existing shelves to fill the new shelf, leaving the vast majority of the established assignments untouched.