
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Documentation for Data Engineering and Big Data Technologies">
      
      
        <meta name="author" content="AnalyticsWithManish">
      
      
      
        <link rel="prev" href="../../airflow/airflow/">
      
      
        <link rel="next" href="../../sql/Mongo/">
      
      
        
      
      
      <link rel="icon" href="../../assets/images/sqldeveloper-original.svg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Spark - AnalyticsWithManish</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="pink">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#spark" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="AnalyticsWithManish" class="md-header__button md-logo" aria-label="AnalyticsWithManish" data-md-component="logo">
      
  <img src="../../assets/images/sqldeveloper-original.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AnalyticsWithManish
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Spark
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="pink"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="custom" data-md-color-accent="light-green"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/manishchetpalli/AnalyticsWithManish" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    AnalyticsWithManish
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../DataEngineering/DE/" class="md-tabs__link">
          
  
  
    
  
  Fundamentals

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../projects/FlightBookingDataPipelinewithAirflow%26CICD/" class="md-tabs__link">
          
  
  
    
  
  Projects

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../hadoop/Bigdata/" class="md-tabs__link">
          
  
  
    
  
  BigData

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../cloud/overview/" class="md-tabs__link">
          
  
  
    
  
  Cloud computing

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../systemdesign/overview/" class="md-tabs__link">
          
  
  
    
  
  System Design

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../tooling/airflow_install_onpremise/" class="md-tabs__link">
          
  
  
    
  
  SetupDocs

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../hadoop/hadoopiq/" class="md-tabs__link">
          
  
  
    
  
  Interview Prep

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../about/" class="md-tabs__link">
        
  
  
    
  
  About Me

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="AnalyticsWithManish" class="md-nav__button md-logo" aria-label="AnalyticsWithManish" data-md-component="logo">
      
  <img src="../../assets/images/sqldeveloper-original.svg" alt="logo">

    </a>
    AnalyticsWithManish
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/manishchetpalli/AnalyticsWithManish" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    AnalyticsWithManish
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Fundamentals
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Fundamentals
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../DataEngineering/DE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../DataEngineering/DEcycle/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    DE Lifecycle
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../DataEngineering/DEundercurrent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    DE UnderCurrents
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../DataEngineering/DesigningGDA/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    DE Architecture & Design
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../airflow/dataorchestration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Data Orchestration
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../datawarehousing/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Data Warehousing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../DataEngineering/fileformat/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    File Formats
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sql/sql/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    SQL
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Projects
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Projects
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../projects/FlightBookingDataPipelinewithAirflow%26CICD/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Flight Booking Data Pipeline
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../projects/flink-ignite/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Flink Ignite Custom Dialect
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../projects/kafkaapi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    FastAPI Kafka Producer
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    BigData
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    BigData
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hadoop/Bigdata/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hadoop/HDFS/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    HDFS
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hadoop/mapreduce/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    MapReduce
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hadoop/yarn/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    YARN
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hadoop/kerberos/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Kerberos
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hive/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Hive
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../kafka/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Kafka
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ignite/ignite/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ignite
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../airflow/airflow/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Airflow
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Spark
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Spark
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#sparkoverview" class="md-nav__link">
    <span class="md-ellipsis">
      
        SparkOverview
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#data-type-schema" class="md-nav__link">
    <span class="md-ellipsis">
      
        Data Type &amp; Schema
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Transformations
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#actions" class="md-nav__link">
    <span class="md-ellipsis">
      
        Actions
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dag-and-lazy-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      
        DAG and Lazy Evaluation
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-query-plan" class="md-nav__link">
    <span class="md-ellipsis">
      
        Spark Query Plan
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rdd" class="md-nav__link">
    <span class="md-ellipsis">
      
        RDD
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#partitioning-and-bucketing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Partitioning and Bucketing
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparksession-vs-sparkcontext" class="md-nav__link">
    <span class="md-ellipsis">
      
        SparkSession vs SparkContext
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#repartition-vs-coaelesce" class="md-nav__link">
    <span class="md-ellipsis">
      
        Repartition vs Coaelesce
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-strategy-joins" class="md-nav__link">
    <span class="md-ellipsis">
      
        Spark Strategy Joins
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-memory-management" class="md-nav__link">
    <span class="md-ellipsis">
      
        Spark Memory Management
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Spark Memory Management">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#executor-memory" class="md-nav__link">
    <span class="md-ellipsis">
      
        Executor memory
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#driver-memory" class="md-nav__link">
    <span class="md-ellipsis">
      
        Driver memory
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-submit" class="md-nav__link">
    <span class="md-ellipsis">
      
        Spark Submit
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#aqe" class="md-nav__link">
    <span class="md-ellipsis">
      
        AQE
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cache-persist" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cache &amp; Persist
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#salting" class="md-nav__link">
    <span class="md-ellipsis">
      
        Salting
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dynamice-resource-allocation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dynamice Resource Allocation
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-streaming" class="md-nav__link">
    <span class="md-ellipsis">
      
        Spark Streaming
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-best-practises" class="md-nav__link">
    <span class="md-ellipsis">
      
        Spark Best Practises
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sql/Mongo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Mongo
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../delta/deltalake/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Delta Lake
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Cloud computing
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Cloud computing
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cloud/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
        
          
          <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Databricks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Databricks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cloud/databricks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    System Design
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    System Design
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../systemdesign/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../systemdesign/scaling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Horizonatal vs Vertical Scaling
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../systemdesign/loadbalancing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Load Balancing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../systemdesign/consistenthashing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Consistent Hashing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../systemdesign/messagingqueue/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Messaging Queue
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    SetupDocs
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    SetupDocs
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tooling/airflow_install_onpremise/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Airflow Setup
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tooling/igniteinstall/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ignite Multi Node Setup
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tooling/kafka_install_zookeeper/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Kafka Multi Node Setup
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tooling/kubernetes_install/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Kubernetes Setup
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tooling/spark3standalone/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Sparkstandalone Multi Node Setup
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Interview Prep
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            
  
    Interview Prep
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../hadoop/hadoopiq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Hadoop
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../kafka/iq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Kafka
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../airflow/airflowiq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Airflow
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../flink/iq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Flink
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../sql/mongoiq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    MongoDB
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../about/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    About Me
  

    
  </span>
  
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#sparkoverview" class="md-nav__link">
    <span class="md-ellipsis">
      
        SparkOverview
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#data-type-schema" class="md-nav__link">
    <span class="md-ellipsis">
      
        Data Type &amp; Schema
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Transformations
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#actions" class="md-nav__link">
    <span class="md-ellipsis">
      
        Actions
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dag-and-lazy-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      
        DAG and Lazy Evaluation
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-query-plan" class="md-nav__link">
    <span class="md-ellipsis">
      
        Spark Query Plan
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rdd" class="md-nav__link">
    <span class="md-ellipsis">
      
        RDD
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#partitioning-and-bucketing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Partitioning and Bucketing
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sparksession-vs-sparkcontext" class="md-nav__link">
    <span class="md-ellipsis">
      
        SparkSession vs SparkContext
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#repartition-vs-coaelesce" class="md-nav__link">
    <span class="md-ellipsis">
      
        Repartition vs Coaelesce
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-strategy-joins" class="md-nav__link">
    <span class="md-ellipsis">
      
        Spark Strategy Joins
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-memory-management" class="md-nav__link">
    <span class="md-ellipsis">
      
        Spark Memory Management
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Spark Memory Management">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#executor-memory" class="md-nav__link">
    <span class="md-ellipsis">
      
        Executor memory
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#driver-memory" class="md-nav__link">
    <span class="md-ellipsis">
      
        Driver memory
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-submit" class="md-nav__link">
    <span class="md-ellipsis">
      
        Spark Submit
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#aqe" class="md-nav__link">
    <span class="md-ellipsis">
      
        AQE
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cache-persist" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cache &amp; Persist
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#salting" class="md-nav__link">
    <span class="md-ellipsis">
      
        Salting
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dynamice-resource-allocation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dynamice Resource Allocation
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-streaming" class="md-nav__link">
    <span class="md-ellipsis">
      
        Spark Streaming
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-best-practises" class="md-nav__link">
    <span class="md-ellipsis">
      
        Spark Best Practises
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
                



  


  <nav class="md-path" aria-label="Navigation" >
    <ol class="md-path__list">
      
        
  
  
    <li class="md-path__item">
      <a href="../.." class="md-path__link">
        
  <span class="md-ellipsis">
    Home
  </span>

      </a>
    </li>
  

      
      
        
  
  
    
    
      <li class="md-path__item">
        <a href="../../hadoop/Bigdata/" class="md-path__link">
          
  <span class="md-ellipsis">
    BigData
  </span>

        </a>
      </li>
    
  

      
    </ol>
  </nav>

              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="spark">Spark<a class="headerlink" href="#spark" title="Permanent link">#</a></h1>
<h2 id="sparkoverview"><strong>SparkOverview</strong><a class="headerlink" href="#sparkoverview" title="Permanent link">#</a></h2>
<blockquote>
<p>--- <strong>Problems with MapReduce</strong></p>
</blockquote>
<ol>
<li>Batch Processing</li>
<li>Complexity</li>
<li>Data Movement</li>
<li>Fault Tolerance</li>
<li>No Support for Interactive Processing</li>
<li>Not Optimal for Small Files</li>
</ol>
<blockquote>
<p>--- <strong>Features of Spark</strong></p>
</blockquote>
<ol>
<li>Speed</li>
<li>Powerful Caching</li>
<li>Deployment</li>
<li>Real-Time Processing</li>
<li>Polyglot</li>
<li>Scalability</li>
</ol>
<blockquote>
<p>--- <strong>Spark Architecture</strong></p>
</blockquote>
<p><img alt="Steps" src="../sparkarc.svg" /></p>
<ul>
<li>
<p>Driver Program: The driver program is the heart of a Spark application. It runs the main() function of an application and is the place where the SparkContext is created. SparkContext is responsible for coordinating and monitoring the execution of tasks. The driver program defines datasets and applies operations (transformations &amp; actions) on them.</p>
</li>
<li>
<p>SparkContext: The SparkContext is the main entry point for Spark functionality. It represents the connection to a Spark cluster and can be used to create RDDs, accumulators, and broadcast variables on that cluster.</p>
</li>
<li>
<p>Cluster Manager: SparkContext connects to the cluster manager, which is responsible for the allocation of resources (CPU, memory, etc.) in the cluster. The cluster manager can be Spark's standalone manager, Hadoop YARN, Mesos, or Kubernetes.</p>
</li>
<li>
<p>Executors: Executors are worker nodes' processes in charge of running individual tasks in a given Spark job. They run concurrently across different nodes. Executors have two roles. Firstly, they run tasks that the driver sends. Secondly, they provide in-memory storage for RDDs.</p>
</li>
<li>
<p>Tasks: Tasks are the smallest unit of work in Spark. They are transformations applied to partitions. Each task works on a separate partition and is executed in a separate thread in executors.</p>
</li>
<li>
<p>RDD: Resilient Distributed Datasets (RDD) are the fundamental data structures of Spark. They are an immutable distributed collection of objects, which can be processed in parallel. RDDs can be stored in memory between queries without the necessity for serialization.</p>
</li>
<li>
<p>DAG (Directed Acyclic Graph): Spark represents a series of transformations on data as a DAG, which helps it optimize the execution plan. DAG enables pipelining of operations and provides a clear plan for task scheduling.
Spark Architecture &amp; Its components</p>
</li>
<li>
<p>DAG Scheduler: The Directed Acyclic Graph (DAG) Scheduler is responsible for dividing operator graphs into stages and sending tasks to the Task Scheduler. It translates the data transformations from the logical plan (which represents a sequence of transformations) into a physical execution plan. It optimizes the plan by rearranging and combining operations where possible, groups them into stages, and then submits the stages to the Task Scheduler.</p>
</li>
<li>
<p>Task Scheduler: The Task Scheduler launches tasks via cluster manager. Tasks are the smallest unit of work in Spark, sent by the DAG Scheduler to the Task Scheduler. The Task Scheduler then launches the tasks on executor JVMs. Tasks for each stage are launched in as many parallel operations as there are partitions for the dataset.</p>
</li>
<li>
<p>Master: The Master is the base of a Spark Standalone cluster (specific to Spark's standalone mode, not applicable if Spark is running on YARN or Mesos). It's the central point and entry point of the Spark cluster. It is responsible for managing and distributing tasks to the workers. The Master communicates with each of the workers periodically to check if it is still alive and if it has completed tasks.</p>
</li>
<li>
<p>Worker: The Worker is a node in the Spark Standalone cluster (specific to Spark's standalone mode). It receives tasks from the Master and executes them. Each worker has multiple executor JVMs running on it. It communicates with the Master and Executors to facilitate task execution.The worker is responsible for managing resources and providing an execution environment for the executor JVMs.</p>
</li>
</ul>
<blockquote>
<p>--- <strong>What happens behind the scenes</strong></p>
</blockquote>
<ol>
<li>You launch the application.</li>
<li>Spark creates a SparkContext in the Driver.</li>
<li>Spark connects to the Cluster Manager (e.g., YARN, standalone, k8s).</li>
<li>Cluster Manager allocates Workers and starts Executors.</li>
<li>RDD transformations are converted into a DAG (Directed Acyclic Graph.</li>
<li>Spark creates Stages, breaks them into Tasks (based on partitions).</li>
<li>Tasks are shipped to Executors.</li>
<li>Executors run the tasks and return results back to the Driver.</li>
<li>Final results (e.g., word count) are written to HDFS.</li>
</ol>
<blockquote>
<p>--- <strong>Spark Standalone</strong></p>
</blockquote>
<p>Spark Standalone mode is a built-in cluster manager in Apache Spark that enables you to set up a dedicated Spark cluster without needing external resource managers like Hadoop YARN or Kubernetes.</p>
<p>It is easy to deploy, suitable for development and testing, and supports distributed data processing across multiple nodes.</p>
<p>Advantages:</p>
<ol>
<li>Easy to set up and manage</li>
<li>No need for external resource managers</li>
<li>Built-in web UI for monitoring</li>
<li>Supports HA (High Availability) with ZooKeeper</li>
</ol>
<p>Limitations:</p>
<ol>
<li>Less fault-tolerant than YARN or Kubernetes</li>
<li>Limited support for resource isolation and fairness</li>
<li>Not recommended for large-scale production</li>
</ol>
<blockquote>
<p>---  <strong>Spark with YARN</strong></p>
</blockquote>
<p>Apache Spark on YARN means running Spark applications on top of Hadoop YARN (Yet Another Resource Negotiator) - the resource manager in Hadoop ecosystems. This setup allows Spark to share cluster resources with other big data tools (like Hive, HBase, MapReduce) in a multi-tenant environment.</p>
<p>YARN handles resource management, job scheduling, and container allocation, while Spark focuses on data processing.</p>
<ul>
<li>
<p>Resource Manager: It controls the allocation of system resources on all applications. A Scheduler and an Application Master are included. Applications receive resources from the Scheduler.</p>
</li>
<li>
<p>Node Manager: Each job or application needs one or more containers, and the Node Manager monitors these containers and their usage. Node Manager consists of an Application Master and Container. The Node Manager monitors the containers and resource usage, and this is reported to the Resource Manager.</p>
</li>
<li>
<p>Application Master: The ApplicationMaster (AM) is an instance of a framework-specific library and serves as the orchestrating process for an individual application in a distributed environment.</p>
</li>
</ul>
<p>Advantages:</p>
<ol>
<li>Leverages existing Hadoop cluster (no separate setup)</li>
<li>Resource sharing across Hadoop ecosystem</li>
<li>Supports HDFS, Hive, HBase integration natively</li>
<li>Production-grade scalability and stability</li>
</ol>
<p>Considerations:</p>
<ol>
<li>Slight overhead from YARNs container management</li>
<li>Configuration tuning (memory, executor placement) is important</li>
<li>YARN needs to be properly secured (Kerberos, ACLs)</li>
</ol>
<h2 id="data-type-schema"><strong>Data Type &amp; Schema</strong><a class="headerlink" href="#data-type-schema" title="Permanent link">#</a></h2>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">flight_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;csv&quot;</span><span class="p">)</span> \
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>                <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;header&quot;</span><span class="p">,</span> <span class="s2">&quot;false&quot;</span><span class="p">)</span> \
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>                <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;inferschema&quot;</span><span class="p">,</span><span class="s2">&quot;false&quot;</span><span class="p">)</span>\
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>                <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;mode&quot;</span><span class="p">,</span><span class="s2">&quot;FAILFAST&quot;</span><span class="p">)</span>\
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>                <span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;flightdata.csv&quot;</span><span class="p">)</span> 
</span></code></pre></div>
<blockquote>
<p>--- <strong><em>Read modes in spark</em></strong></p>
</blockquote>
<table>
<thead>
<tr>
<th>Mode</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>failFast</td>
<td>Terminates the query immediately if any malformed record is encountered. This is useful when data integrity is critical.</td>
</tr>
<tr>
<td>dropMalformed</td>
<td>Drops all rows containing malformed records. This can be useful when you prefer to skip bad data instead of failing the entire job.</td>
</tr>
<tr>
<td>permissive (default)</td>
<td>Tries to parse all records. If a record is corrupted or missing fields, Spark sets <code>null</code> values for corrupted fields and puts malformed data into a special column named <code>_corrupt_record</code>.</td>
</tr>
</tbody>
</table>
<p>There are two primary methods: using StructType and StructField classes, and using a DDL (Data Definition Language) string.</p>
<p>These are classes in Spark used to define the schema structure.</p>
<p>StructField represents a single column within a DataFrame. It holds information such as the column's name, its data type (e.g., String, Integer, Timestamp), and whether it can contain null values (nullable: True/False). If nullable is set to False, the column cannot contain NULL values, and an error will be thrown if it does.</p>
<p>StructType defines the overall structure of a DataFrame. It is essentially a list or collection of StructField objects.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>What happens if you set header=False when your data actually has a header? If you disable the header option (header=False) but your CSV file contains a header row, Spark will treat that header row as regular data.
If this header row's values do not match the data types defined in your manual schema (e.g., a string "Count" being read into an Integer column), it can lead to null values in that column if the read mode is set to permissive, or an error if the mode is failfast</p>
</div>
<blockquote>
<p>---  <strong><em>Handling corrupted records</em></strong></p>
</blockquote>
<p>When reading data, Spark offers different modes to handle corrupted records, which influence how the DataFrame is populated.</p>
<p>In permissive mode, all records are allowed to enter the DataFrame. If a record is corrupted, Spark sets the malformed values to null and does not throw an error. For the example data with five total records (two corrupted), permissive mode will result in five records in the DataFrame, with nulls where data is bad.</p>
<p>In dropMalformed mode, Spark discards any record it identifies as corrupted.</p>
<p>In failfast mode, Spark immediately throws an error and stops the job as soon as it encounters the first corrupted record. This mode will result in zero records in the DataFrame because the job will fail.</p>
<blockquote>
<p>---  <strong><em>Print bad records</em></strong></p>
</blockquote>
<p>To specifically identify and view the corrupted records, you need to define a manual schema that includes a special column named _corrupt_record. This column will capture the raw content of the corrupted record.</p>
<p>Where to store bad record For scenarios with a large volume of corrupted records (e.g., thousands), printing them is not practical. Spark provides the badRecordsPath option to store all corrupted records in a specified location. These records are saved in JSON format at the designated path.</p>
<blockquote>
<p>--- <strong>Modes in DataFrame Writer API</strong></p>
</blockquote>
<p>When working with Spark, after you have read data into a DataFrame and performed transformations, it is crucial to write the processed data back to disk to ensure its persistence. Currently, all the transformations and data processing occur in memory, so writing to disk makes the data permanent.</p>
<p>A typical flow looks like: df.write.format(...).option(...).mode(...).save(path).</p>
<p>The mode() method in the DataFrame Writer API is crucial as it dictates how Spark handles existing data at the target location. There are four primary modes:</p>
<ul>
<li>
<p>append: If files already exist at the specified location, the new data from the DataFrame will be added to the existing files.</p>
</li>
<li>
<p>overwrite: This mode deletes any existing files at the target location before writing the new DataFrame.</p>
</li>
<li>
<p>errorIfExists: Spark will check if a file or location already exists at the target path. If it does, the write operation will fail and throw an error. Useful when you want to ensure that you do not accidentally overwrite or append to existing data.</p>
</li>
<li>
<p>ignore: If a file or location already exists at the target path, Spark will skip the write operation entirely without throwing an error. The new file will not be written. This mode is suitable if you want to prevent new data from being written if data is already present, perhaps to avoid overwriting changes or to ensure data integrity</p>
</li>
</ul>
<h2 id="transformations"><strong>Transformations</strong><a class="headerlink" href="#transformations" title="Permanent link">#</a></h2>
<blockquote>
<p>--- <strong>Transformations</strong></p>
</blockquote>
<p>In Spark, a transformation is an operation applied on an RDD (Resilient Distributed Dataset) or DataFrame/Dataset to create a new RDD or DataFrame/Dataset.</p>
<p>Transformations refer to any processing done on data. They are operations that create a new DataFrame (or RDD) from an existing one, but they do not execute immediately. Spark is based on lazy evaluation, meaning transformations are only executed when an action is triggered</p>
<p>Transformations in Spark are categorized into two types: narrow and wide transformations.</p>
<p><img alt="Steps" src="../trans.svg" /></p>
<ul>
<li>
<p>Narrow Transformations</p>
<p>In these transformations, all elements that are required to compute the records in a single partition live in the same partition of the parent RDD. Data doesn't need to be shuffled across partitions.</p>
<p>These are transformations that do not require data movement between partitions. In a distributed setup, each executor can process its partition of data independently without needing to communicate with other executors</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>map, filter, flatmap, sample</p>
</div>
</li>
<li>
<p>Wide Transformations</p>
<p>These transformations will have input data from multiple partitions. This typically involves shuffling all the data across multiple partitions.</p>
<p>These transformations require data movement or "shuffling" between partitions. This means an executor might need data from another executor's partition to complete its computation. This data movement makes wide transformations expensive operations</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>groupbykey, reducebykey, join, distinct, coalesce, repartition</p>
</div>
</li>
</ul>
<h2 id="actions"><strong>Actions</strong><a class="headerlink" href="#actions" title="Permanent link">#</a></h2>
<p>Actions in Apache Spark are operations that provide non-RDD values; they return a final value to the driver program or write data to an external system. Actions trigger the execution of the transformation operations accumulated in the Directed Acyclic Graph (DAG).</p>
<p>Actions are operations that trigger the execution of all previous transformations and produce a result. When an action is hit, Spark creates a job.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>collect, count, save, show</p>
</div>
<blockquote>
<p>--- <strong>Read &amp; Write operation in Spark are Transformation/Action?</strong></p>
</blockquote>
<p>Reading and writing operations in Spark are often viewed as actions, but they're a bit unique.</p>
<p>Read Operation:Transformations , especially read operations can behave in two ways according to the arguments you provide</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul>
<li>Lazily evaluated - It will be performed only when an action is called.</li>
<li>Eagerly evaluated - A job will be triggered to do some initial evaluations. In case of read.csv()</li>
</ul>
</div>
<p>If it is called without defining the schema and inferSchema is disabled, it determines the columns as string types and it reads only the first line to determine the names (if header=True, otherwise it gives default column names) and the number of fields. 
Basically it performs a collect operation with limit 1, which means one new job is created instantly</p>
<p>Now if you specify inferSchema=True, Here above job will be triggered first as well as one more job will be triggered which will scan through entire record to determine the schema, that's why you are able to see 2 jobs in spark UI</p>
<p>Now If you specify schema explicitly by providing StructType() schema object to 'schema' argument of read.csv(), then you can see no jobs will be triggered here. This is because, we have provided the number of columns and type explicitly and catalogue of spark will store that information and now it doesn't need to scan the file to get that information and this will be validated lazily at the time of calling action.</p>
<p>Write Operation: Writing or saving data in Spark, on the other hand, is considered an action. Functions like saveAsTextFile(), saveAsSequenceFile(), saveAsObjectFile(), or DataFrame write options trigger computation and result in data being written to an external system.</p>
<hr />
<h2 id="dag-and-lazy-evaluation"><strong>DAG and Lazy Evaluation</strong><a class="headerlink" href="#dag-and-lazy-evaluation" title="Permanent link">#</a></h2>
<p>Spark represents a sequence of transformations on data as a DAG, a concept borrowed from mathematics and computer science. A DAG is a directed graph with no cycles, and it represents a finite set of transformations on data with multiple stages. The nodes of the graph represent the RDDs or DataFrames/Datasets, and the edges represent the transformations or operations applied.</p>
<p>Each action on an RDD (or DataFrame/Dataset) triggers the creation of a new DAG. The DAG is optimized by the Catalyst optimizer (in case of DataFrame/Dataset) and then it is sent to the DAG scheduler, which splits the graph into stages of tasks.</p>
<blockquote>
<p>--- <strong>Job, Stage and Task in Spark</strong></p>
</blockquote>
<p><img alt="Steps" src="../job.svg" /></p>
<ul>
<li>
<p>Application
An application in Spark refers to any command or program that you submit to your Spark cluster for execution.
Typically, one spark-submit command creates one Spark application. You can submit multiple applications, but each spark-submit initiates a distinct application.</p>
</li>
<li>
<p>Job
Within an application, jobs are created based on "actions" in your Spark code.
An action is an operation that triggers the computation of a result, such as collect(), count(), write(), show(), or save().
If your application contains five actions, then five separate jobs will be created.
Every job will have a minimum of one stage and one task associated with it.</p>
</li>
<li>
<p>Stage
A job is further divided into smaller parts called stages.
Stages represent a set of operations that can be executed together without shuffling data across the network. Think of them as logical steps in a job's execution plan. Stages are primarily defined by "wide dependency transformations".</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Wide Dependency Transformations (e.g., repartition(), groupBy(), join()) require shuffling data across partitions, meaning data from one partition might be needed by another. Each wide dependency transformation typically marks the end of one stage and the beginning of a new one.</p>
<p>Narrow Dependency Transformations (e.g., filter(), select(), map()) do not require data shuffling; an output partition can be computed from only one input partition. Multiple narrow transformations can be grouped into a single stage.</p>
</div>
</li>
<li>
<p>Task
A task is the actual unit of work that is executed on an executor.
It performs the computations defined within a stage on a specific partition of data.
The number of tasks within a stage is directly determined by the number of partitions the data has at that point in the execution. If a stage operates on 200 partitions, it will typically launch 200 tasks.</p>
</li>
</ul>
<p>Relationship Summary:</p>
<ul>
<li>One Application can contain Multiple Jobs.</li>
<li>One Job can contain Multiple Stages.</li>
<li>One Stage can contain Multiple Tasks</li>
</ul>
<blockquote>
<p>--- <strong>What if our cluster capacity is less than the size of data to be processed?</strong></p>
</blockquote>
<p>If your cluster memory capacity is less than the size of the data to be processed, Spark can still handle it by leveraging its ability to perform computations on disk and spilling data from memory to disk when necessary.</p>
<p>Let's break down how Spark will handle a 60 GB data load with a 30 GB memory cluster:</p>
<ol>
<li>
<p>Data Partitioning: When Spark reads a 60 GB file from HDFS, it partitions the data into manageable blocks, according to the Hadoop configuration parameter dfs.blocksize or manually specified partitions. These partitions can be processed independently.</p>
</li>
<li>
<p>Loading Data into Memory: Spark will load as many partitions as it can fit into memory. It starts processing these partitions. The size of these partitions is much smaller than the total size of your data (60 GB), allowing Spark to work within the confines of your total memory capacity (30 GB in this case).</p>
</li>
<li>
<p>Spill to Disk: When the memory is full, and Spark needs to load new partitions for processing, it uses a mechanism called "spilling" to free up memory. Spilling means writing data to disk. The spilled data is the intermediate data generated during shuffling operations, which needs to be stored for further stages.</p>
</li>
<li>
<p>On-Disk Computation: Spark has the capability to perform computations on data that is stored on disk, not just in memory. Although computations on disk are slower than in memory, it allows Spark to handle datasets that are larger than the total memory capacity.</p>
</li>
<li>
<p>Sequential Processing: The stages of the job are processed sequentially, meaning Spark doesn't need to load the entire dataset into memory at once. Only the data required for the current stage needs to be in memory or disk.</p>
</li>
</ol>
<blockquote>
<p>--- <strong>How spark perform data partitioning</strong></p>
</blockquote>
<p><img alt="Steps" src="../datatrans.svg" /></p>
<ul>
<li>Data Partitioning: Apache Spark partitions data into logical chunks during reading from sources like HDFS, S3, etc.</li>
<li>Data Distribution: These partitions are distributed across the Spark cluster nodes, allowing for parallel processing.</li>
<li>Custom Partitioning: Users can control data partitioning using Spark's repartition(), coalesce() and partitionBy() methods, optimizing data locality or skewness.</li>
</ul>
<p>When Apache Spark reads data from a file on HDFS or S3, the number of partitions is determined by the size of the data and the default block size of the file system. In general, each partition corresponds to a block in HDFS or an object in S3.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>If HDFS is configured with a block size of 128MB and you have a 1GB file, it would be divided into 8 blocks in HDFS. Therefore, when Spark reads this file, it would create 8 partitions, each corresponding to a block.</p>
</div>
<blockquote>
<p>--- <strong>Lazy Evaluation in Spark</strong></p>
</blockquote>
<p>Lazy evaluation in Spark means that the execution doesn't start until an action is triggered. In Spark, transformations are lazily evaluated, meaning that the system records how to compute the new RDD (or DataFrame/Dataset) from the existing one without performing any transformation. The transformations are only actually computed when an action is called and the data is required. </p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>spark.read.csv() 
will not actually read the data until an action like .show() or .count() is performed</p>
</div>
<hr />
<h2 id="spark-query-plan"><strong>Spark Query Plan</strong><a class="headerlink" href="#spark-query-plan" title="Permanent link">#</a></h2>
<p>The Spark SQL Engine is fundamentally the Catalyst Optimizer. Its primary role is to convert user code (written in DataFrames, SQL, or Datasets) into Java bytecode for execution. This conversion and optimization process occurs in four distinct phases. It's considered a compiler because it transforms your code into Java bytecode. It plays a key role in optimizing code leveraging concepts like lazy evaluation</p>
<p><img alt="Steps" src="../sparksqlengine.svg" /></p>
<blockquote>
<p>--- <strong>Phase 1: Unresolved Logical Plan</strong></p>
</blockquote>
<p>This is the initial stage where you write your code using DataFrames, SQL, or Datasets APIs.</p>
<p>When you write transformations (e.g., select, filter, join), Spark creates an "unresolved logical plan". This plan is like a blueprint or a "log" of transformations, indicating what operations need to be performed in what order.</p>
<p>At this stage, the plan is "unresolved" because Spark has not yet checked if the tables, columns, or files referenced actually exist</p>
<blockquote>
<p>--- <strong>Phase 2: Analysis</strong></p>
</blockquote>
<p>To resolve the logical plan by checking the existence and validity of all referenced entities.</p>
<p>This phase heavily relies on the Catalog. The Catalog is where Spark stores metadata (data about data). It contains information about tables, files, databases, their names, creation times, sizes, column names, and data types. For example, if you read a CSV file, the Catalog knows its path, name, and column headers.</p>
<p>The Analysis phase queries the Catalog to verify if the files, columns, or tables specified in the unresolved logical plan actually exist.</p>
<p>If everything is found and validated, the plan becomes a "Resolved Logical Plan". If any entity is not found (e.g., a non-existent file path or a misspelled column name), Spark throws an AnalysisException</p>
<blockquote>
<p>--- <strong>Phase 3: Logical Optimization</strong></p>
</blockquote>
<p>To optimize the "Resolved Logical Plan" without considering the physical execution aspects. It focuses on making the logical operations more efficient.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Predicate Pushdown: If you apply multiple filters, the optimizer might combine them or push them down closer to the data source to reduce the amount of data processed early.</p>
<p>Column Pruning: If you select all columns (SELECT *) but then only use a few specific columns in subsequent operations, the optimizer will realize this and modify the plan to only fetch the necessary columns from the start, saving network I/O and processing.</p>
</div>
<p>This phase benefits from Spark's lazy evaluation, allowing it to perform these optimizations before any actual computation begins. An "Optimized Logical Plan"</p>
<blockquote>
<p>--- <strong>Phase 4: Physical Planning</strong></p>
</blockquote>
<p>The "Optimized Logical Plan" is converted into multiple possible "Physical Plans". Each physical plan represents a different strategy for executing the logical operations (e.g., different join algorithms).</p>
<p>Spark applies a "Cost-Based Model" to evaluate these physical plans. It estimates the resources (memory, CPU, network I/O) each plan would consume if executed.</p>
<p>The plan that offers the best resource utilization and lowest estimated cost (e.g., least data shuffling, fastest execution time) is selected as the "Best Physical Plan".</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>For joins, if one table is significantly smaller than the other, Spark might choose a Broadcast Join. This involves sending the smaller table to all executor nodes where the larger table's partitions reside. This avoids data shuffling (expensive network operations) of the larger table across the cluster, leading to significant performance gains.</p>
</div>
<p>The Best Physical Plan, which is essentially a set of RDDs (Resilient Distributed Datasets) ready to be executed on the cluster.</p>
<blockquote>
<p>--- <strong>Phase 5:Whole-Stage Code Generation</strong></p>
</blockquote>
<p>This is the final step where the "Best Physical Plan" (the RDD operations) is translated into Java bytecode.</p>
<p>This bytecode is then sent to the individual executors on the cluster to be executed. This direct bytecode generation improves performance by eliminating interpretation overhead and allowing the JVM to further optimize the code.</p>
<blockquote>
<p>--- <strong>In what cases will predicate pushdown not work?</strong></p>
</blockquote>
<ul>
<li>Complex Data Types</li>
</ul>
<p>Spark's Parquet data source does not push down filters that involve complex types, such as arrays, maps, and struct. This is because these complex data types can have complicated nested structures that the Parquet reader cannot easily filter on.</p>
<p>Here's an example:</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>root
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a> |-- Name: string (nullable = true)
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a> |-- properties: map (nullable = true)
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a> |    |-- key: string
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a> |    |-- value: string (valueContainsNull = true)
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>+----------+-----------------------------+
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>|Name      |properties                   |
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>+----------+-----------------------------+
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>|Afaque    |[eye -&gt; black, hair -&gt; black]|
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>|Naved     |[eye -&gt;, hair -&gt; brown]      |
</span><span id="__span-1-12"><a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>|Ali       |[eye -&gt; black, hair -&gt; red]  |
</span><span id="__span-1-13"><a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>|Amaan     |[eye -&gt; grey, hair -&gt; grey]  |
</span><span id="__span-1-14"><a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>|Omaira    |[eye -&gt; , hair -&gt; brown]     |
</span><span id="__span-1-15"><a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>+----------+-----------------------------+
</span></code></pre></div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">properties</span><span class="o">.</span><span class="n">getItem</span><span class="p">(</span><span class="s2">&quot;eye&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;brown&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></code></pre></div>
<div class="language-text highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>== Physical Plan ==
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>*(1) Filter (metadata#123[key] = value)
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>+- *(1) ColumnarToRow
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>   +- FileScan parquet [id#122,metadata#123] Batched: true, DataFilters: [(metadata#123[key] = value)], Format: Parquet, ...
</span></code></pre></div>
<ul>
<li>Unsupported Expressions </li>
</ul>
<p>In Spark, <code>Parquet</code> data source does not support pushdown for filters involving a <code>.cast</code> operation.</p>
<p>The reason for this behaviour is as follows: <code>.cast</code> changes the datatype of the column, and the Parquet data source may not be able to perform the filter operation correctly on the cast data.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This behavior may vary based on the data source. For example, if you're working with a JDBC data source connected to a database that supports SQL-like operations, the <code>.cast</code> filter could potentially be pushed down to the database.</p>
</div>
<hr />
<h2 id="rdd"><strong>RDD</strong><a class="headerlink" href="#rdd" title="Permanent link">#</a></h2>
<p>RDDs are the building blocks of any Spark application.</p>
<p>RDDs Stands for</p>
<ul>
<li>Resilient: Fault tolerant and is capable of rebuilding data on failure</li>
<li>Distributed: Distributed data among the multiple nodes in a cluster</li>
<li>Dataset: Collection of partitioned data with values</li>
</ul>
<blockquote>
<p>--- <strong>Here are some key points about RDDs and their properties:</strong></p>
</blockquote>
<ul>
<li>
<p>Fundamental Data Structure: RDD is the fundamental data structure of Spark, which allows it to efficiently operate on large-scale data across a distributed environment.</p>
</li>
<li>
<p>Immutability: Once an RDD is created, it cannot be changed. Any transformation applied to an RDD creates a new RDD, leaving the original one untouched.</p>
</li>
<li>
<p>Resilience: RDDs are fault-tolerant, meaning they can recover from node failures. This resilience is provided through a feature known as lineage, a record of all the transformations applied to the base data.</p>
</li>
<li>
<p>Lazy Evaluation: RDDs follow a lazy evaluation approach, meaning transformations on RDDs are not executed immediately, but computed only when an action (like count, collect) is performed. This leads to optimized computation.</p>
</li>
<li>
<p>Partitioning: RDDs are partitioned across nodes in the cluster, allowing for parallel computation on separate portions of the dataset.</p>
</li>
<li>
<p>In-Memory Computation: RDDs can be stored in the memory of worker nodes, making them readily available for repeated access, and thereby speeding up computations.</p>
</li>
<li>
<p>Distributed Nature: RDDs can be processed in parallel across a Spark cluster, contributing to the overall speed and scalability of Spark.</p>
</li>
<li>
<p>Persistence: Users can manually persist an RDD in memory, allowing it to be reused across parallel operations. This is useful for iterative algorithms and fast interactive use.</p>
</li>
<li>
<p>Operations: Two types of operations can be performed on RDDs - transformations (which create a new RDD) and actions (which return a value to the driver program or write data to an external storage system).</p>
</li>
</ul>
<blockquote>
<p>--- <strong>When to Use RDDs (Advantages)</strong></p>
</blockquote>
<p>Despite the general recommendation to use DataFrames/Datasets, RDDs have specific use cases where they are advantageous:</p>
<ul>
<li>
<p>Unstructured Data: RDDs are particularly well-suited for processing unstructured data where there is no predefined schema, such as streams of text, media, or arbitrary bytes. For structured data, DataFrames and Datasets are generally better.</p>
</li>
<li>
<p>Full Control and Flexibility: If you need fine-grained control over data processing at a very low level and want to optimize the code manually, RDDs provide that flexibility. This means the developer has more control over how data is transformed and distributed.</p>
</li>
<li>
<p>Type Safety (Compile-Time Errors): RDDs are type-safe. This means that if there's a type mismatch (e.g., trying to add an integer to a string), you will get an error during compile time, before the code even runs. This can help catch errors earlier in the development cycle, unlike DataFrames or SQL queries which might only show errors at runtime</p>
</li>
</ul>
<blockquote>
<p>--- <strong>Why You Should NOT Use RDDs (Disadvantages)</strong></p>
</blockquote>
<p>For most modern Spark applications, especially with structured or semi-structured data, RDDs are generally discouraged due to several drawbacks:</p>
<ul>
<li>
<p>No Automatic Optimization by Spark: Spark's powerful Catalyst Optimizer does not perform optimizations automatically for RDD operations. This means the responsibility for writing optimized and efficient code falls entirely on the developer.</p>
</li>
<li>
<p>Complex and Less Readable Code: Writing RDD code can be complex and less readable compared to DataFrames, Datasets, or SQL. The code often requires explicit handling of data transformations and aggregations, which can be verbose.</p>
</li>
<li>
<p>Potential for Inefficient Operations: Expensive Shuffling: Without Spark's internal optimizations, RDD operations can lead to inefficient data shuffling. In contrast, DataFrames/Datasets using the "what to" approach allow Spark to rearrange operations (e.g., filter first, then shuffle) to optimize performance, saving significant computational resources.</p>
</li>
</ul>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>If you perform a reduceByKey (which requires shuffling data across nodes) before a filter operation, Spark will shuffle all the data first, then filter it. If the filter significantly reduces the dataset size, shuffling the larger pre-filtered dataset becomes a very expensive operation.</p>
</div>
<ul>
<li>Developer Burden: Because Spark doesn't optimize RDDs, the developer must have a deep understanding of distributed computing and Spark's internals to write performant RDD code. This makes development harder and slower compared to using higher-level APIs</li>
</ul>
<blockquote>
<p>--- <strong>Difference</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th>Criteria</th>
<th>RDD (Resilient Distributed Dataset)</th>
<th>DataFrame</th>
<th>DataSet</th>
</tr>
</thead>
<tbody>
<tr>
<td>Abstraction</td>
<td>Low level, provides a basic and simple abstraction.</td>
<td>High level, built on top of RDDs. Provides a structured and tabular view on data.</td>
<td>High level, built on top of DataFrames. Provides a structured and strongly-typed view on data.</td>
</tr>
<tr>
<td>Type Safety</td>
<td>Provides compile-time type safety, since it is based on objects.</td>
<td>Doesn't provide compile-time type safety, as it deals with semi-structured data.</td>
<td>Provides compile-time type safety, as it deals with structured data.</td>
</tr>
<tr>
<td>Optimization</td>
<td>Optimization needs to be manually done by the developer (like using <code>mapreduce</code>).</td>
<td>Makes use of Catalyst Optimizer for optimization of query plans, leading to efficient execution.</td>
<td>Makes use of Catalyst Optimizer for optimization.</td>
</tr>
<tr>
<td>Processing Speed</td>
<td>Slower, as operations are not optimized.</td>
<td>Faster than RDDs due to optimization by Catalyst Optimizer.</td>
<td>Similar to DataFrame, it's faster due to Catalyst Optimizer.</td>
</tr>
<tr>
<td>Ease of Use</td>
<td>Less easy to use due to the need of manual optimization.</td>
<td>Easier to use than RDDs due to high-level abstraction and SQL-like syntax.</td>
<td>Similar to DataFrame, it provides SQL-like syntax which makes it easier to use.</td>
</tr>
<tr>
<td>Interoperability</td>
<td>Easy to convert to and from other types like DataFrame and DataSet.</td>
<td>Easy to convert to and from other types like RDD and DataSet.</td>
<td>Easy to convert to and from other types like DataFrame and RDD.</td>
</tr>
</tbody>
</table>
<h2 id="partitioning-and-bucketing"><strong>Partitioning and Bucketing</strong><a class="headerlink" href="#partitioning-and-bucketing" title="Permanent link">#</a></h2>
<p>Partitioning and Bucketing are data optimization techniques used in Spark during the data writing process to improve performance for future read operations, joins, and filters. By deciding how data is organized on disk, you can significantly reduce the amount of data Spark needs to scan.</p>
<blockquote>
<p>--- <strong>Partitioning</strong></p>
</blockquote>
<p>Partitioning organizes data into a hierarchical folder structure based on the distinct values of one or more columns. </p>
<p>For every unique value in the partitioned column, Spark creates a separate folder. For example, partitioning by "Address" (Country) would create folders like <code>Address=India</code>, <code>Address=USA</code>, etc..</p>
<p>When you query data using a filter (e.g., <code>WHERE Country = 'India'</code>), Spark skip all other folders and only reads the relevant one, avoiding a full table scan.</p>
<p>You can partition by multiple columns. The order of columns matters; for example, partitioning by <code>Address</code> then <code>Gender</code> creates gender folders inside each country folder.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="c1"># Partitioning by a single column (Address)</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a><span class="n">df</span><span class="o">.</span><span class="n">write</span> \
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a>  <span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s2">&quot;Address&quot;</span><span class="p">)</span> \
</span><span id="__span-4-4"><a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a>  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;csv&quot;</span><span class="p">)</span> \
</span><span id="__span-4-5"><a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a>  <span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;/path/to/destination&quot;</span><span class="p">)</span>
</span><span id="__span-4-6"><a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a>
</span><span id="__span-4-7"><a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a><span class="c1"># Partitioning by multiple columns (Address and then Gender)</span>
</span><span id="__span-4-8"><a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a><span class="n">df</span><span class="o">.</span><span class="n">write</span> \
</span><span id="__span-4-9"><a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a>  <span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s2">&quot;Address&quot;</span><span class="p">,</span> <span class="s2">&quot;Gender&quot;</span><span class="p">)</span> \
</span><span id="__span-4-10"><a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a>  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;csv&quot;</span><span class="p">)</span> \
</span><span id="__span-4-11"><a id="__codelineno-4-11" name="__codelineno-4-11" href="#__codelineno-4-11"></a>  <span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;/path/to/destination_multi&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p>Partitioning is inefficient for high-cardinality columns (columns with many unique values, like a User ID). If you partition by ID, Spark might create millions of tiny files, which degrades performance.</p>
<blockquote>
<p>--- <strong>Bucketing</strong></p>
</blockquote>
<p>Bucketing is used when partitioning is not suitable, particularly for high-cardinality columns or to optimize joins.</p>
<p>Spark uses a hash function on a specific column to distribute data into a fixed number of "buckets" (files).</p>
<p>Unlike partitioning, bucketing is not supported by the standard <code>.save()</code> method on file systems; it requires using <code>saveAsTable</code> because the metadata must be stored in the Hive Metastore.</p>
<p>If two tables are bucketed on the same column with the same number of buckets, Spark can perform a join without "shuffling" data across the network, which is a very expensive operation.</p>
<p>Spark knows exactly which bucket contains a specific value (e.g., a specific Aadhaar ID), allowing it to search only a small fraction of the data.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="c1"># Bucketing by ID into 3 buckets</span>
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a><span class="n">df</span><span class="o">.</span><span class="n">write</span> \
</span><span id="__span-5-3"><a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a>  <span class="o">.</span><span class="n">bucketBy</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;ID&quot;</span><span class="p">)</span> \
</span><span id="__span-5-4"><a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>  <span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s2">&quot;bucketed_table&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p>If you have 200 tasks running and you ask for 5 buckets, Spark might create <span class="arithmatex">\(200 \times 5 = 1,000\)</span> files. To prevent this, repartition the data to match the number of buckets before writing.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="c1"># Optimize by matching partitions to buckets</span>
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="n">df</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> \
</span><span id="__span-6-3"><a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a>  <span class="o">.</span><span class="n">write</span> \
</span><span id="__span-6-4"><a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a>  <span class="o">.</span><span class="n">bucketBy</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;ID&quot;</span><span class="p">)</span> \
</span><span id="__span-6-5"><a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a>  <span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s2">&quot;optimized_bucket_table&quot;</span><span class="p">)</span>
</span></code></pre></div>
<blockquote>
<p>--- <strong>Summary Comparison</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Partitioning</th>
<th style="text-align: left;">Bucketing</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Logic</td>
<td style="text-align: left;">Groups data into folders based on column values.</td>
<td style="text-align: left;">Groups data into a fixed number of files via hashing.</td>
</tr>
<tr>
<td style="text-align: left;">Best For</td>
<td style="text-align: left;">Low-cardinality columns (e.g., Country, Gender).</td>
<td style="text-align: left;">High-cardinality columns (e.g., ID) and Join optimization.</td>
</tr>
<tr>
<td style="text-align: left;">Output</td>
<td style="text-align: left;">Created as directory structures on the file system.</td>
<td style="text-align: left;">Created as specific bucketed files within a table.</td>
</tr>
<tr>
<td style="text-align: left;">Constraint</td>
<td style="text-align: left;">Can lead to "too many small files" if cardinality is high.</td>
<td style="text-align: left;">Must be saved as a table (<code>saveAsTable</code>).</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="sparksession-vs-sparkcontext"><strong>SparkSession vs SparkContext</strong><a class="headerlink" href="#sparksession-vs-sparkcontext" title="Permanent link">#</a></h2>
<p>Both Spark Session and Spark Context serve as the entry point into a Spark cluster, similar to how a <code>main</code> method serves as the entry point for code execution in languages like C++ or Java. This means that to run any Spark code, you first need to establish one of these entry points.</p>
<blockquote>
<p>--- <strong>Spark Session</strong></p>
</blockquote>
<p>The Spark Session is the unified entry point introduced in Spark 2.0. It is now the primary way to interact with Spark.</p>
<p>Prior to Spark 2.0 (specifically up to Spark 1.4), if you wanted to work with different Spark functionalities like SQL, Hive, or Streaming, you had to create separate contexts for each (e.g., <code>SQLContext</code>, <code>HiveContext</code>, <code>StreamingContext</code>). The Spark Session encapsulates all these different contexts, providing a single object to access them. This simplifies development as you only need to create a Spark Session to gain access to all necessary functionalities.</p>
<p>When you create a Spark Session, you can pass configurations for resources needed, such as the amount of memory or the number of executors. The Spark Session takes these values and communicates with the Resource Manager (like YARN or Mesos) to request and allocate the necessary driver memory and executors. Once these resources are secured, the Spark Session facilitates the execution of your Spark jobs within that allocated environment.</p>
<p>If you've been using Databricks notebooks, you might have implicitly been using a Spark Session without realizing it. Databricks typically provides a default <code>spark</code> object, which is an instance of <code>SparkSession</code>, allowing you to directly write code like <code>spark.read.format(...)</code>. This is why the local setup is demonstrated in the source, as the default session is not automatically provided outside environments like Databricks.</p>
<p>You can configure properties like <code>spark.driver.memory</code> by using the <code>.config()</code> method when building the Spark Session.</p>
<blockquote>
<p>--- <strong>Spark Context</strong></p>
</blockquote>
<p>The Spark Context (<code>SparkContext</code>) was the original entry point for Spark applications before Spark 2.0.</p>
<p>n earlier versions of Spark (up to Spark 1.4), <code>SparkContext</code> was the primary entry point for general Spark operations. However, for specific functionalities like SQL, you needed additional context objects like <code>SQLContext</code>.</p>
<p>While Spark Session has become the dominant entry point, <code>SparkContext</code> is still relevant for RDD (Resilient Distributed Dataset) level operations. If you need to perform low-level transformations directly on RDDs (e.g., <code>flatMap</code>, <code>map</code>), you would typically use the Spark Context. An example provided is writing a word count program using RDDs, where <code>SparkContext</code> comes into use.</p>
<p>With the advent of Spark Session, you do not create a <code>SparkContext</code> directly as a separate entry point anymore. Instead, you can access the <code>SparkContext</code> object through the <code>SparkSession</code> instance. This means that the <code>SparkContext</code> is now encapsulated within the <code>SparkSession</code>.</p>
<blockquote>
<p>--- <strong>Code Example</strong></p>
</blockquote>
<p>Heres an example demonstrating how to create a Spark Session and then obtain a Spark Context from it, based on the provided transcript:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="c1"># First, import the SparkSession class from pyspark.sql</span>
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">pyspark.sql</span><span class="w"> </span><span class="kn">import</span> <span class="n">SparkSession</span>
</span><span id="__span-7-3"><a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a>
</span><span id="__span-7-4"><a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a><span class="c1"># Create a SparkSession builder</span>
</span><span id="__span-7-5"><a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a><span class="c1"># The .builder() method is used to construct a SparkSession instance</span>
</span><span id="__span-7-6"><a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a><span class="n">spark_builder</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span>
</span><span id="__span-7-7"><a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a>
</span><span id="__span-7-8"><a id="__codelineno-7-8" name="__codelineno-7-8" href="#__codelineno-7-8"></a><span class="c1"># Configure the SparkSession</span>
</span><span id="__span-7-9"><a id="__codelineno-7-9" name="__codelineno-7-9" href="#__codelineno-7-9"></a><span class="c1"># .master(&quot;local&quot;): Specifies that Spark should run in local mode.</span>
</span><span id="__span-7-10"><a id="__codelineno-7-10" name="__codelineno-7-10" href="#__codelineno-7-10"></a><span class="c1">#                  This means Spark will use your local machine&#39;s resources.</span>
</span><span id="__span-7-11"><a id="__codelineno-7-11" name="__codelineno-7-11" href="#__codelineno-7-11"></a><span class="c1"># .appName(&quot;Testing&quot;): Sets the name of your Spark application.</span>
</span><span id="__span-7-12"><a id="__codelineno-7-12" name="__codelineno-7-12" href="#__codelineno-7-12"></a><span class="c1">#                     This can be any descriptive name for your project.</span>
</span><span id="__span-7-13"><a id="__codelineno-7-13" name="__codelineno-7-13" href="#__codelineno-7-13"></a><span class="c1"># .config(&quot;spark.driver.memory&quot;, &quot;12g&quot;): An optional configuration to request specific resources,</span>
</span><span id="__span-7-14"><a id="__codelineno-7-14" name="__codelineno-7-14" href="#__codelineno-7-14"></a><span class="c1">#                                       here requesting 12GB for the driver memory.</span>
</span><span id="__span-7-15"><a id="__codelineno-7-15" name="__codelineno-7-15" href="#__codelineno-7-15"></a><span class="n">spark_session_config</span> <span class="o">=</span> <span class="n">spark_builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;Testing&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.driver.memory&quot;</span><span class="p">,</span> <span class="s2">&quot;12g&quot;</span><span class="p">)</span>
</span><span id="__span-7-16"><a id="__codelineno-7-16" name="__codelineno-7-16" href="#__codelineno-7-16"></a>
</span><span id="__span-7-17"><a id="__codelineno-7-17" name="__codelineno-7-17" href="#__codelineno-7-17"></a><span class="c1"># Get or Create the SparkSession</span>
</span><span id="__span-7-18"><a id="__codelineno-7-18" name="__codelineno-7-18" href="#__codelineno-7-18"></a><span class="c1"># .getOrCreate(): This is a crucial method. If a SparkSession with the specified</span>
</span><span id="__span-7-19"><a id="__codelineno-7-19" name="__codelineno-7-19" href="#__codelineno-7-19"></a><span class="c1">#                name and configuration already exists, it will retrieve it.</span>
</span><span id="__span-7-20"><a id="__codelineno-7-20" name="__codelineno-7-20" href="#__codelineno-7-20"></a><span class="c1">#                Otherwise, it will create a new one.</span>
</span><span id="__span-7-21"><a id="__codelineno-7-21" name="__codelineno-7-21" href="#__codelineno-7-21"></a><span class="n">spark</span> <span class="o">=</span> <span class="n">spark_session_config</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span><span id="__span-7-22"><a id="__codelineno-7-22" name="__codelineno-7-22" href="#__codelineno-7-22"></a>
</span><span id="__span-7-23"><a id="__codelineno-7-23" name="__codelineno-7-23" href="#__codelineno-7-23"></a><span class="c1"># Print the SparkSession object to verify it&#39;s created</span>
</span><span id="__span-7-24"><a id="__codelineno-7-24" name="__codelineno-7-24" href="#__codelineno-7-24"></a><span class="nb">print</span><span class="p">(</span><span class="n">spark</span><span class="p">)</span>
</span><span id="__span-7-25"><a id="__codelineno-7-25" name="__codelineno-7-25" href="#__codelineno-7-25"></a>
</span><span id="__span-7-26"><a id="__codelineno-7-26" name="__codelineno-7-26" href="#__codelineno-7-26"></a><span class="c1"># Access the SparkContext from the created SparkSession</span>
</span><span id="__span-7-27"><a id="__codelineno-7-27" name="__codelineno-7-27" href="#__codelineno-7-27"></a><span class="c1"># The SparkContext is encapsulated within the SparkSession object.</span>
</span><span id="__span-7-28"><a id="__codelineno-7-28" name="__codelineno-7-28" href="#__codelineno-7-28"></a><span class="c1"># This is how you get the sc (SparkContext) object in modern Spark applications.</span>
</span><span id="__span-7-29"><a id="__codelineno-7-29" name="__codelineno-7-29" href="#__codelineno-7-29"></a><span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>
</span><span id="__span-7-30"><a id="__codelineno-7-30" name="__codelineno-7-30" href="#__codelineno-7-30"></a>
</span><span id="__span-7-31"><a id="__codelineno-7-31" name="__codelineno-7-31" href="#__codelineno-7-31"></a><span class="c1"># Print the SparkContext object</span>
</span><span id="__span-7-32"><a id="__codelineno-7-32" name="__codelineno-7-32" href="#__codelineno-7-32"></a><span class="nb">print</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
</span><span id="__span-7-33"><a id="__codelineno-7-33" name="__codelineno-7-33" href="#__codelineno-7-33"></a>
</span><span id="__span-7-34"><a id="__codelineno-7-34" name="__codelineno-7-34" href="#__codelineno-7-34"></a><span class="c1"># Example of using SparkSession to read data (common operation)</span>
</span><span id="__span-7-35"><a id="__codelineno-7-35" name="__codelineno-7-35" href="#__codelineno-7-35"></a><span class="c1"># This is what you often do in Databricks without explicitly creating a session.</span>
</span><span id="__span-7-36"><a id="__codelineno-7-36" name="__codelineno-7-36" href="#__codelineno-7-36"></a><span class="c1"># employee_df = spark.read.format(&quot;csv&quot;).load(&quot;path/to/employee_data.csv&quot;)</span>
</span></code></pre></div>
<hr />
<h2 id="repartition-vs-coaelesce"><strong>Repartition vs Coaelesce</strong><a class="headerlink" href="#repartition-vs-coaelesce" title="Permanent link">#</a></h2>
<p>The need for repartition and coalesce arises from issues faced when processing large datasets in Spark, particularly concerning data partitioning.</p>
<p><img alt="Steps" src="../repart.svg" /></p>
<p>When a DataFrame is created, it's often divided into multiple partitions. Sometimes, these partitions can be of uneven sizes (e.g., 10MB, 20MB, 40MB, 100MB).</p>
<p>Processing smaller partitions (e.g., 10MB) takes less time than larger ones (e.g., 100MB). This leads to idle Spark executors: while one executor is busy with a large partition, others might finish their tasks quickly and then wait for the large partition to complete. This causes time delays and underutilization of allocated resources (e.g., RAM)</p>
<p>This situation often arises after operations like join transformations. For instance, if a join operation is performed on a product column, and one product is a "best-selling product" with a high number of records, all those records might get grouped into a single partition, making it very large. This phenomenon is called data skew.</p>
<p>Users often see messages like "199 out of 200 partitions processed," where the last remaining partition takes a significantly longer time to complete due to its large size.</p>
<p>To deal with these scenarios and optimize performance, Spark provides repartition and coalesce methods</p>
<blockquote>
<p>--- <strong>repartition</strong></p>
</blockquote>
<p>Repartition shuffles the entire dataset across the cluster. This means data from existing partitions can be moved to new partitions.</p>
<p>The primary goal of repartition is to evenly distribute data across the specified number of new partitions. For example, if you have 200MB of data across five uneven partitions and repartition it into five, it will aim for 40MB per partition.</p>
<p>Repartition can increase or decrease the number of partitions. If you initially have 5 partitions but need 10, repartition is the only choice.It can be used when you want to increase the number of partitions to allow for more concurrent tasks and increase parallelism when the cluster has more resources.</p>
<p>Due to the shuffling operation, repartition is generally more expensive and involves more I/O operations compared to coalesce.</p>
<p>Pros and Cons of Repartition are Evenly distributed data. More I/O (Input/Output) because of shuffling. More expensive.</p>
<p>In certain scenarios, you may want to partition based on a specific key to optimize your job. For example, if you frequently filter by a certain key, you might want all records with the same key to be on the same partition to minimize data shuffling. In such cases, you can use repartition() with a column name.</p>
<blockquote>
<p>--- <strong>coalesce</strong></p>
</blockquote>
<p>Coalesce merges existing partitions to reduce the total number of partitions.</p>
<p>Crucially, coalesce tries to avoid full data shuffling. It achieves this by moving data from some partitions to existing ones, effectively merging them locally on the same executor if possible. This makes it less expensive than repartition.</p>
<p>Because it avoids full shuffling, coalesce does not guarantee an even distribution of data across the new partitions. It might result in an uneven distribution, especially if the original partitions were already skewed.</p>
<p>Coalesce can only decrease the number of partitions. It cannot be used to increase the number of partitions. If you need more partitions, you must use repartition.</p>
<p>Pros and Cons of Coalesce: No shuffling (or minimal shuffling). Not expensive (cost-effective). Uneven data distribution.</p>
<p>However, it can lead to  data skew if you have fewer partitions than before, because it combines existing partitions to reduce the total number.</p>
<blockquote>
<p>--- <strong>When to Choose Which?</strong></p>
</blockquote>
<p>The choice between repartition and coalesce is use-case dependent</p>
<p><strong>Choose repartition when</strong>:</p>
<p>You need to evenly distribute data across partitions, which is crucial for balanced workload across executors.</p>
<p>You need to increase the number of partitions (e.g., if you have too few partitions or want to process data in smaller chunks in parallel).</p>
<p>You are okay with the overhead of a full shuffle, as the benefit of even distribution outweighs the cost.</p>
<p>Dealing with severe data skew is a primary concern.</p>
<p><strong>Choose coalesce when</strong>:</p>
<p>You primarily need to decrease the number of partitions (e.g., after filter operations drastically reduce data, or before writing to a single file).</p>
<p>You want to minimize shuffling and I/O costs.</p>
<p>You can tolerate slightly uneven data distribution across partitions, or the data skew is minimal and won't significantly impact performance.</p>
<p>You want to save processing time and cost by avoiding a full shuffle</p>
<blockquote>
<p>--- <strong>Why doesn't <code>.coalesce()</code> explicitly show the partitioning scheme?</strong></p>
</blockquote>
<p><code>.coalesce</code> doesn't show the partitioning scheme e.g. <code>RoundRobinPartitioning</code> because the operation only minimizes data movement by merging into fewer partitions, it doesn't do any shuffling. Because no shuffling is done, the partitioning scheme remains the same as the original DataFrame and Spark doesn't include it explicitly in it's plan as the partitioning scheme is unaffected by <code>.coalesce</code></p>
<hr />
<h2 id="spark-strategy-joins"><strong>Spark Strategy Joins</strong><a class="headerlink" href="#spark-strategy-joins" title="Permanent link">#</a></h2>
<p>It is important to distinguish between Join Types and Join Strategies.</p>
<p>Join Types refers to the logical result of the join (e.g., Left Join, Right Join, Inner Join, etc.).</p>
<p>Join Strategies refers to the internal implementation or "strategy" Spark uses to execute the join across a cluster (e.g., Shuffle Sort Merge Join, Broadcast Join).</p>
<blockquote>
<p>--- <strong>Why Joins are Expensive: The Shuffling Process</strong></p>
</blockquote>
<p>Joins are considered "expensive" operations in Spark because they often require shuffling (or "saapling" as referred to in the source), which involves moving data across the network between executors.</p>
<p>If you have two DataFrames of 500MB each with a default HDFS block size of 128MB, Spark will create 4 partitions for each DataFrame. These partitions are distributed across different executors/worker nodes.</p>
<p>To join data on a specific key (e.g., <code>ID</code>), the data for that same key must reside on the same executor. If <code>ID 1</code> is on Executor A and its corresponding match is on Executor B, Spark must move that data to a common location.</p>
<p>For wide transformations like joins, Spark defaults to creating 200 partitions.</p>
<p>Spark uses a hash-based approach to determine where data goes. For example, it might calculate <code>ID % 200</code> to determine the partition number. This ensures that the same ID from both DataFrames always ends up in the same partition.</p>
<p>Conceptual Code Example (Shuffling):
<div class="language-python highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="c1"># Spark defaults to 200 partitions for shuffle operations</span>
</span><span id="__span-8-2"><a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.shuffle.partitions&quot;</span><span class="p">,</span> <span class="s2">&quot;200&quot;</span><span class="p">)</span>
</span><span id="__span-8-3"><a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>
</span><span id="__span-8-4"><a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a><span class="c1"># Performing a join triggers shuffling to align keys across the cluster</span>
</span><span id="__span-8-5"><a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a><span class="n">df_joined</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">id</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="s2">&quot;inner&quot;</span><span class="p">)</span>
</span></code></pre></div></p>
<p>Spark utilizes five primary strategies to perform joins:</p>
<ol>
<li>Shuffle Sort Merge Join (SSMJ)</li>
<li>Shuffle Hash Join (SHJ)</li>
<li>Broadcast Hash Join (BHJ)</li>
<li>Cartesian Join</li>
<li>Broadcast Nested Loop Join (BNLJ)</li>
</ol>
<blockquote>
<p>--- <strong>Shuffle Sort Merge Join (SSMJ)</strong></p>
</blockquote>
<p>This is the default join strategy in Spark.</p>
<p>Data with the same keys are moved to the same partitions. The data within each partition is sorted by the join key. Spark iterates through the sorted data and merges matching keys. The sorting phase typically takes <span class="arithmatex">\(O(n \log n)\)</span> time.It is CPU-intensive due to sorting but very stable for large datasets because it doesn't require the entire table to fit in memory.</p>
<blockquote>
<p>--- <strong>Shuffle Hash Join (SHJ)</strong></p>
</blockquote>
<p>Data is moved to the same partitions. Spark builds a hash table of the smaller DataFrame in memory. It then probes this hash table using keys from the larger DataFrame. The join/lookup phase is <span class="arithmatex">\(O(1)\)</span>. It is Memory-intensive. If the hash table exceeds the executor's memory, it will trigger an Out of Memory (OOM) exception.</p>
<blockquote>
<p>--- <strong>Broadcast Nested Loop Join (BNLJ)</strong></p>
</blockquote>
<p>This is considered the most expensive join strategy. It is used when there is no equality condition (non-equi joins), such as <code>df1.id &gt; df2.id</code>. It operates at <span class="arithmatex">\(O(n^2)\)</span> because it essentially involves two nested loops to compare every row of one table with every row of the other.</p>
<p>Conceptual Code Example (Non-Equi Join):
<div class="language-python highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="c1"># A non-equi join often forces a Broadcast Nested Loop Join</span>
</span><span id="__span-9-2"><a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a><span class="n">df_non_equi</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">id</span> <span class="o">&gt;</span> <span class="n">df2</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="s2">&quot;inner&quot;</span><span class="p">)</span>
</span></code></pre></div></p>
<blockquote>
<p>--- <strong>Summary of Trade-offs</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align: left;">Strategy</th>
<th style="text-align: left;">Resource Focus</th>
<th style="text-align: left;">Best For</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sort Merge Join</td>
<td style="text-align: left;">CPU (Sorting)</td>
<td style="text-align: left;">Large datasets; Very stable; Default</td>
</tr>
<tr>
<td style="text-align: left;">Shuffle Hash Join</td>
<td style="text-align: left;">Memory (Hash Table)</td>
<td style="text-align: left;">When one table is small enough to fit in memory</td>
</tr>
<tr>
<td style="text-align: left;">Broadcast Nested Loop</td>
<td style="text-align: left;">CPU/Memory (Nested Loop)</td>
<td style="text-align: left;">Non-equi joins (e.g., <code>&gt;</code>, <code>&lt;</code>); Very slow</td>
</tr>
</tbody>
</table>
<blockquote>
<p>--- <strong>How Broadcast Join Works</strong></p>
</blockquote>
<p>Broadcast Hash Join is a specialized join strategy used to optimize performance by eliminating the need for data shuffling,. While standard joins like Shuffle Sort Merge Join (the Spark default) move data across the network to align keys, a Broadcast Join sends the entire smaller dataset to every worker node,.</p>
<p>The core mechanism involves the Driver and the Executors - The Spark Driver identifies a table that is small enough to be broadcast. It must have sufficient memory to store this table locally before distributing it. The Driver sends a complete copy of the small table to every Executor in the cluster.</p>
<p>Once the small table is residing on every Executor, each Executor can perform the join locally using its own partition of the large table. This makes the Executors "self-sufficient" because they no longer need to fetch data from other nodes via shuffling.</p>
<blockquote>
<p>--- <strong>When to Use Broadcast Joins</strong></p>
</blockquote>
<p>It is ideal when you have one large table (e.g., 1GB) and one small table (e.g., less than 10MB). Use it to prevent "cluster choking" caused by moving massive amounts of data across the network. While not the primary focus of this source, the transcript notes that avoiding shuffling is the main goal of this strategy.</p>
<blockquote>
<p>--- <strong>Checking and Setting the Broadcast Threshold</strong></p>
</blockquote>
<p>Spark uses a default threshold to decide if a table should be automatically broadcast. This is typically 10 MB.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="c1"># To get the current broadcast threshold (returns value in bytes)</span>
</span><span id="__span-10-2"><a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a><span class="n">current_threshold</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;spark.sql.autoBroadcastJoinThreshold&quot;</span><span class="p">)</span>
</span><span id="__span-10-3"><a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a><span class="nb">print</span><span class="p">(</span><span class="n">current_threshold</span><span class="p">)</span> <span class="c1"># Default is 10485760 (10 MB)</span>
</span><span id="__span-10-4"><a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a>
</span><span id="__span-10-5"><a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a><span class="c1"># To change the threshold (e.g., to 20 MB)</span>
</span><span id="__span-10-6"><a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a><span class="c1"># You must convert MB to bytes</span>
</span><span id="__span-10-7"><a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.autoBroadcastJoinThreshold&quot;</span><span class="p">,</span> <span class="s2">&quot;20971520&quot;</span><span class="p">)</span>
</span><span id="__span-10-8"><a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a>
</span><span id="__span-10-9"><a id="__codelineno-10-9" name="__codelineno-10-9" href="#__codelineno-10-9"></a><span class="c1"># To disable automatic broadcasting</span>
</span><span id="__span-10-10"><a id="__codelineno-10-10" name="__codelineno-10-10" href="#__codelineno-10-10"></a><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.autoBroadcastJoinThreshold&quot;</span><span class="p">,</span> <span class="s2">&quot;-1&quot;</span><span class="p">)</span>
</span></code></pre></div>
<blockquote>
<p>--- <strong>Forcing a Broadcast Join with Hints</strong></p>
</blockquote>
<p>If Spark does not automatically choose a broadcast join, you can provide a hint in your code to force it.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">pyspark.sql.functions</span><span class="w"> </span><span class="kn">import</span> <span class="n">broadcast</span>
</span><span id="__span-11-2"><a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a>
</span><span id="__span-11-3"><a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a><span class="c1"># Standard join might result in a Sort Merge Join</span>
</span><span id="__span-11-4"><a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a><span class="c1"># df_joined = df_large.join(df_small, &quot;id&quot;)</span>
</span><span id="__span-11-5"><a id="__codelineno-11-5" name="__codelineno-11-5" href="#__codelineno-11-5"></a>
</span><span id="__span-11-6"><a id="__codelineno-11-6" name="__codelineno-11-6" href="#__codelineno-11-6"></a><span class="c1"># Using the broadcast hint to force a Broadcast Hash Join</span>
</span><span id="__span-11-7"><a id="__codelineno-11-7" name="__codelineno-11-7" href="#__codelineno-11-7"></a><span class="n">df_joined</span> <span class="o">=</span> <span class="n">df_large</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">broadcast</span><span class="p">(</span><span class="n">df_small</span><span class="p">),</span> <span class="n">df_large</span><span class="o">.</span><span class="n">id</span> <span class="o">==</span> <span class="n">df_small</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="s2">&quot;inner&quot;</span><span class="p">)</span>
</span><span id="__span-11-8"><a id="__codelineno-11-8" name="__codelineno-11-8" href="#__codelineno-11-8"></a>
</span><span id="__span-11-9"><a id="__codelineno-11-9" name="__codelineno-11-9" href="#__codelineno-11-9"></a><span class="c1"># Inspecting the physical plan to verify the join strategy</span>
</span><span id="__span-11-10"><a id="__codelineno-11-10" name="__codelineno-11-10" href="#__codelineno-11-10"></a><span class="n">df_joined</span><span class="o">.</span><span class="n">explain</span><span class="p">()</span> 
</span></code></pre></div>
<blockquote>
<p>--- <strong>Identifying Joins in the Spark Web UI</strong></p>
</blockquote>
<p>In the Spark Web UI (SQL tab), you can distinguish between join types by looking at the execution graph:
Will show an <code>Exchange</code> (Shuffle), followed by a <code>Sort</code>, and then a <code>SortMergeJoin</code>. Will show a <code>BroadcastExchange</code> and then a <code>BroadcastHashJoin</code>, with no <code>Exchange</code> (shuffle) for the large table.</p>
<blockquote>
<p>--- <strong>Potential Risks and Failure Points</strong></p>
</blockquote>
<p>Despite its efficiency, Broadcast Join can fail in the following scenarios:</p>
<p>Since the small table must first be collected by the Driver, if the table is too large for the Driver's memory, the application will crash. If the Executor's memory is already near capacity, adding a broadcasted table (even a 100MB one) can trigger an OOM during the join operation. Attempting to broadcast a very large file (e.g., 1GB) will saturate the network because that file must be sent to every single executor.</p>
<blockquote>
<p>--- <strong>Summary Table</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Shuffle Sort Merge Join (SSMJ)</th>
<th style="text-align: left;">Broadcast Hash Join (BHJ)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Data Movement</td>
<td style="text-align: left;">Shuffles both tables</td>
<td style="text-align: left;">Broadcasts only the small table</td>
</tr>
<tr>
<td style="text-align: left;">Network Cost</td>
<td style="text-align: left;">High (Shuffle)</td>
<td style="text-align: left;">Low (if table is small)</td>
</tr>
<tr>
<td style="text-align: left;">Default Size</td>
<td style="text-align: left;">Any size</td>
<td style="text-align: left;">&lt; 10 MB (Default)</td>
</tr>
<tr>
<td style="text-align: left;">Stability</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Risky if small table is too large</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="spark-memory-management"><strong>Spark Memory Management</strong><a class="headerlink" href="#spark-memory-management" title="Permanent link">#</a></h2>
<p><img alt="Steps" src="../memory.svg" /></p>
<p>When the Spark application is launched, the Spark cluster will start two processes  Driver and Executor.</p>
<p>The driver is a master process responsible for creating the Spark context, submission of Spark jobs, and translation of the whole Spark pipeline into computational units  tasks. It also coordinates task scheduling and orchestration on each Executor.</p>
<p>Driver memory management is not much different from the typical JVM process.</p>
<p>The executor is responsible for performing specific computational tasks on the worker nodes and returning the results to the driver, as well as providing storage for RDDs. And its internal memory management is very interesting.</p>
<h3 id="executor-memory"><strong>Executor memory</strong><a class="headerlink" href="#executor-memory" title="Permanent link">#</a></h3>
<p><img alt="Steps" src="../executormemory.svg" /></p>
<p>A Spark executor container has three major components of memory:</p>
<blockquote>
<p>--- <strong>On-Heap Memory</strong></p>
</blockquote>
<p>This occupies the largest block and is where most of Spark's operations run .The On-Heap memory is managed by the JVM (Java Virtual Machine). Even though Spark is written in Scala and you might write code in Python using PySpark (which uses a wrapper around Java APIs), the underlying execution still happens on the JVM.</p>
<p>The On-Heap memory is further divided into four sections:</p>
<ul>
<li>
<p><strong>Execution Memory</strong>: 
    It is mainly used to store temporary data in the shuffle, join, sort, aggregation, etc. Most likely, if your pipeline runs too long, the problem lies in the lack of space here.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Execution Memory = usableMemory * spark.memory.fraction * (1 - spark.memory.storageFraction).</p>
<p>As Storage Memory, Execution Memory is also equal to 30% of all system memory by default (1 * 0.6 * (1 - 0.5) = 0.3).</p>
</div>
</li>
<li>
<p><strong>Storage Memory</strong>: 
    This is where caching (for RDDs or DataFrames) occurs, and it's also used for storing broadcast variables.
    Storage Memory is used for caching and broadcasting data. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Storage Memory = usableMemory * spark.memory.fraction * spark.memory.storageFraction</p>
<p>Storage Memory is 30% of all system memory by default (1 * 0.6 * 0.5 = 0.3).</p>
</div>
</li>
<li>
<p><strong>User Memory</strong>:
    Used for storing user objects such as variables, collections (lists, sets, dictionaries) defined in your program, or User Defined Functions (UDFs).
    It is mainly used to store data needed for RDD conversion operations, such as lineage. You can store your own data structures there that will be used inside transformations. It's up to you what would be stored in this memory and how. Spark makes completely no accounting on what you do there and whether you respect this boundary or not.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>User Memory = usableMemory * (1 - spark.memory.fraction)</p>
<p>It is 1 * (1 - 0.6) = 0.4 or 40% of available memory by default.</p>
</div>
</li>
<li>
<p><strong>Reserved Memory</strong>: 
    This is the memory Spark needs for running itself and storing internal objects
    The most boring part of the memory. Spark reserves this memory to store internal objects. It guarantees to reserve sufficient memory for the system even for small JVM heaps.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Reserved Memory is hardcoded and equal to 300 MB (value RESERVED_SYSTEM_MEMORY_BYTES in source code). In the test environment (when spark.testing set) we can modify it with spark.testing.reservedMemory.</p>
<p>usableMemory = spark.executor.memory - RESERVED_SYSTEM_MEMORY_BYTES</p>
</div>
</li>
<li>
<p><strong>Unified memory</strong>:</p>
<p>Unified memory refers to the Execution memory and Storage memory combined.</p>
<ul>
<li><strong>Why it's "Unified"</strong>: It's due to Spark's dynamic memory management strategy.This means if execution memory needs more space, it can use some of the storage memory, and vice-versa. There is a priority given to execution memory because critical operations like joins, shuffles, sorting, and group by happen there. The division between execution and storage is represented as a movable "slider".</li>
</ul>
<p>Evolution of Unified Memory (Pre-Spark 1.6 vs. Post-Spark 1.6):</p>
<ul>
<li><strong>Before Spark 1.6</strong>: The space allocated to execution and storage memory was fixed.
    If execution needed more memory but its fixed allocation was full, it could not use available space in storage memory, leading to wasted memory.</li>
<li><strong>After Spark 1.6 (&gt;= Spark 1.6)</strong>: The "slider" became movable, allowing dynamic allocation based on needs.</li>
</ul>
</li>
<li>
<p><strong>Rules for Slider Movement (Dynamic Allocation)</strong>:</p>
<p>Execution needs more memory, and Storage has vacant space: If storage is not using all its allocated space, execution can simply use that vacant portion of memory.</p>
<p>Execution needs more memory, and Storage is occupied: If storage is using its blocks, it will evict some of its blocks (least recently used or LRU algorithm) to make room for execution memory.</p>
<p>Storage needs more memory: In this case, because execution has priority, none of the execution blocks will be evicted. Storage must evict its own blocks (based on LRU) to free up space for new cached data</p>
</li>
</ul>
<blockquote>
<p>--- <strong>Off-Heap Memory</strong></p>
</blockquote>
<p>Off-Heap memory is often the least talked about and least used, but it can be very useful in certain situations.
- Default State: It is disabled by default (spark.memory.offHeap.enabled is set to zero).</p>
<ul>
<li>Enabling and Sizing: You can enable it by setting spark.memory.offHeap.enabled to true and specify its size using spark.memory.offHeap.size. A good starting point for its size is 10% to 20% of your executor memory.</li>
<li>Structure: Similar to unified memory, off-heap memory also has two parts: execution and storage.</li>
<li>Purpose/Use Case: It becomes useful when the on-heap memory is full.<ul>
<li>When on-heap memory is full, a garbage collection (GC) cycle occurs, which pauses the program's operation to clean up unwanted objects. These GC pauses can negatively impact program performance.</li>
<li>Off-Heap memory is managed by the Operating System, not the JVM. Therefore, it is not subject to the JVM's GC cycles.</li>
</ul>
</li>
<li>Developer Responsibility: Since it's not subject to GC, the Spark developer is responsible for both the allocation and deallocation of memory in the off-heap space. This adds complexity and requires caution to avoid memory leaks.</li>
<li>Performance: Off-heap memory is slower than on-heap memory. However, if Spark had to choose between spilling data to disk or using off-heap memory, using off-heap memory would be a better choice because writing to disk is several orders of magnitude slower</li>
</ul>
<blockquote>
<p>---  <strong>Overhead Memory</strong></p>
</blockquote>
<p>Used for internal system-level operations</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Calculation: The overhead memory is defined as the maximum of 384 MB or 10% of the spark.executor.memory.
If spark.executor.memory is 10 GB, 10% of it is 1 GB.
max(384 MB, 1 GB) = <strong>1 GB</strong>. So, the overhead memory would be 1 GB.</p>
</div>
<p>It's important to note that the spark.executor.memory parameter only allocates for on-heap memory. When Spark requests memory from a cluster manager (like YARN), it adds the executor memory and the overhead memory. If off-heap memory is enabled, it will also add that amount to the request.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>If spark.executor.memory is 10 GB, and overhead is 1 GB (and off-heap is disabled), Spark will request 10 GB + 1 GB = <strong>11 GB</strong> from the cluster manager for that container</p>
</div>
<blockquote>
<p>--- <strong>Why Out of Memory Occurs Even When Spillage is Possible</strong>
Despite the ability to spill data to disk from the Execution Memory Pool, an Out of Memory Exception can still occur, especially during operations like joins or aggregations:</p>
</blockquote>
<ul>
<li>
<p><strong>The Problem of Data Skew</strong>: If data for a single key (e.g., ID=1) becomes excessively large (e.g., 3GB), exceeding the available Execution Memory Pool (e.g., 2.9GB), it cannot be processed.</p>
</li>
<li>
<p><strong>Impossibility of Partial Spillage</strong>: During operations like joins, all data related to a specific key must be present on the same executor for the operation to complete correctly. If a 3GB chunk of data for a single ID has to be processed, and only 2.9GB is available, it's impossible to spill just a portion of that key's data. Spilling half of the 3GB data would mean the join would not yield the correct result for that key. Therefore, if a single partition or a single key's data exceeds the physical memory capacity of the executor's Execution Memory Pool (even with potential spill), an Out of Memory Exception is inevitable.</p>
</li>
</ul>
<blockquote>
<p>--- <strong>Solutions to Out of Memory Exception</strong></p>
</blockquote>
<ul>
<li><strong>Repartitioning</strong>: Redistributing data across more partitions.</li>
<li><strong>Salting</strong>: A technique to add a "salt" to skewed keys to distribute them more evenly during shuffles.</li>
<li><strong>Sorting</strong>: Pre-sorting data can sometimes help with certain types of joins (e.g., Sort-Merge Join) to reduce memory pressure.</li>
</ul>
<h3 id="driver-memory"><strong>Driver memory</strong><a class="headerlink" href="#driver-memory" title="Permanent link">#</a></h3>
<p>The Spark driver has its own dedicated memory. You can configure the driver's memory when starting a PySpark session.</p>
<p>Requesting Driver Memory: To request a specific amount of driver memory, you can use the pyspark command with the --driver-memory flag: pyspark --driver-memory 1g</p>
<p>This command requests 1 GB of driver memory from your local setup. After starting the session, you can verify the Spark driver memory configuration by navigating to localhost:4040/jobs in your web browser.</p>
<blockquote>
<p>---  <strong>Types of Driver Memory</strong>
Within the Spark driver, there are two main types of memory that work together</p>
</blockquote>
<ul>
<li>
<p><strong>JVM Heap Memory (spark.driver.memory)</strong>:This is the primary memory allocated for the driver's Java Virtual Machine (JVM) processes. All JVM-related operations, such as scheduling tasks and handling responses from executors, primarily use this memory.This is what you configure using --driver-memory or spark.driver.memory.</p>
</li>
<li>
<p><strong>Memory Overload (spark.driver.memoryOverhead)</strong>: This memory is dedicated to non-JVM processes.
    It handles objects created by your application that are not part of the JVM heap.
    It also accounts for the memory requirements of the application master container itself, which hosts the driver.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By default, spark.driver.memoryOverhead is calculated as 10% of spark.driver.memory.
However, there's a minimum threshold: if 10% of spark.driver.memory is less than 384 MB, then spark.driver.memoryOverhead will default to 384 MB. The system picks whichever value is higher.</p>
<ul>
<li>Example 1 (1GB driver memory): 10% of 1GB is 100 MB. Since 100 MB is less than 384 MB, the memoryOverhead will be 384 MB.</li>
<li>Example 2 (4GB driver memory): 10% of 4GB is 400 MB. Since 400 MB is greater than 384 MB, the memoryOverhead will be 400 MB.</li>
<li>Example 3 (20GB driver memory): 10% of 20GB is 2GB. In this case, the memoryOverhead would be 2GB memory**</li>
</ul>
</div>
</li>
</ul>
<blockquote>
<p>--- <strong>Common Reasons for Driver Out of Memory</strong></p>
</blockquote>
<p>Besides the collect() method, several other common scenarios can lead to driver OOM:</p>
<ul>
<li>
<p>Using collect() Method on Large Datasets: As demonstrated, attempting to pull all data to the driver's memory will cause an OOM if the data size exceeds the driver's capacity.</p>
</li>
<li>
<p><strong>Broadcasting Large DataFrames/Tables</strong>:
    Broadcasting is a technique used in Spark to optimize joins by sending a smaller DataFrame or table to all executors so that the larger DataFrame can be joined locally without shuffling data.
     When you broadcast data (e.g., df2 and df3 in the example), the driver first merges and holds this data in its memory.
     Then, the driver sends this combined data to all executors.
     If you broadcast multiple large DataFrames (e.g., five 50 MB DataFrames, totaling 250 MB) and the driver doesn't have enough memory to hold them before distributing them, it will lead to a driver OOM error. This is why broadcasting is recommended only for small tables/DataFrames.</p>
</li>
<li><strong>Excessive Object Creation and Heavy Non-JVM Processing</strong>:
     If your Spark application creates many objects or performs heavy processing that falls under non-JVM operations, it consumes the memoryOverhead.
     If the memoryOverhead is insufficient, it can lead to OOM errors often indicated as being "due to memory overhead".</li>
<li>
<p><strong>Incorrect Memory Configuration</strong>:
    Manually setting spark.driver.memory or spark.driver.memoryOverhead to values that are too low for the workload can lead to OOM.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>If you have a 20 GB driver but incorrectly set spark.driver.memoryOverhead to 1 GB when it should ideally be 2 GB (10% of 20GB), you might encounter an OOM error related to memoryOverhead</p>
</div>
</li>
</ul>
<blockquote>
<p>--- <strong>Handling and Solving Driver Out of Memory</strong>
Based on the reasons for OOM, the solutions are often direct:</p>
</blockquote>
<ul>
<li><strong>Avoid collect() on Large Data</strong>:
    For large datasets, never use df.collect() unless you are absolutely certain the data size is small enough to fit within the driver's memory.
    Instead, use df.show() for quick inspection.
    If you need to process all data, consider writing it to a file system (like HDFS or S3) or processing it in a distributed manner across executors.</li>
<li><strong>Manage Broadcasted Data Carefully</strong>:
    Only broadcast DataFrames or tables that are genuinely small.
    Before broadcasting, ensure the driver's memory (specifically the JVM heap) is large enough to hold the combined size of all dataframes you plan to broadcast.</li>
<li><strong>Increase Driver Memory and Memory Overhead</strong>:
    If your application performs extensive non-JVM operations or creates many objects, you might need to increase spark.driver.memory and/or spark.driver.memoryOverhead.
    If you observe "due to memory overhead" errors, explicitly increasing spark.driver.memoryOverhead beyond its default 10% (while respecting system limits) might resolve the issue</li>
</ul>
<hr />
<h2 id="spark-submit"><strong>Spark Submit</strong><a class="headerlink" href="#spark-submit" title="Permanent link">#</a></h2>
<p>Spark Submit is a command-line tool that allows you to trigger or run your Spark applications on a Spark cluster. It packages all the required files and JARs (Java Archive files) and deploys them to the Spark cluster for execution. It is used to run jobs on various types of Spark clusters</p>
<blockquote>
<p>--- <strong>Where is Your Spark Cluster Located?</strong></p>
</blockquote>
<p>Spark clusters can be deployed in multiple environments. When using Spark Submit, you specify the location of your master node.</p>
<p>Common cluster types include:</p>
<ul>
<li>Standalone Cluster: A simple, self-contained Spark cluster. An example master configuration for a standalone cluster could look like spark://10.160.78.10:7077, where 7077 is the default port.</li>
<li>Local Mode: For running Spark applications on your local machine, typically for development or testing. The master configuration is simply local.</li>
<li>YARN (Yet Another Resource Negotiator): A popular resource management system in the Hadoop ecosystem. The master configuration is yarn.</li>
<li>Kubernetes: A container orchestration system.</li>
<li>Mesos: Another cluster management platform</li>
</ul>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>spark-submit<br />
--master {stanadlone,yarn.mesos,kubernetes}<br />
--deploy-mode {client/cluster}<br />
--class mainclass.scala <br />
--jars mysql-connector.jar <br />
--conf spark.dynamicAllocation.enabled=true  <br />
--conf spark.dynamicAllocation.minExecutors=1  <br />
--conf spark.dynamicAllocation.maxExecutors=10  <br />
--conf spark.sql.broadcastTimeout=3600  <br />
--conf spark.sql.autobroadcastJoinThreshold=100000  <br />
--conf spark.executor.cores=2  <br />
--conf spark.executor.instances=5  <br />
--conf spark.default.parallelism=20  <br />
--conf spark.driver.maxResultSize=1G  <br />
--conf spark.network.timeout=800 <br />
--conf spark.driver.maxResultSize=1G  <br />
--conf spark.network.timeout=800  <br />
--driver-memory 1G  <br />
--executor-memory 2G  <br />
--num-executors 5  <br />
--executor-cores 2  <br />
--py-files /path/to/other/python/files.zip 
/path/to/your/python/wordcount.py    /path/to/input/textfile.txt </p>
</div>
<ul>
<li>
<p>master: This is the master URL for the cluster. It can be a URL for any Spark-supported cluster manager. For example, local for local mode, spark://HOST:PORT for standalone mode, mesos://HOST:PORT for Mesos, or yarn for YARN.</p>
</li>
<li>
<p>deploy-mode: This can be either client (default) or cluster. In client mode, the driver runs on the machine from which the job is submitted. In cluster mode, the framework launches the driver inside the cluster.</p>
</li>
<li>
<p>class: This is the entry point for your application, i.e., where your main method runs. For Java and Scala, this would be a fully qualified class name.</p>
</li>
<li>
<p>jars: This argument allows you to provide paths to external JAR files that your Spark application depends on. You can provide multiple JAR files as a comma-separated list. It's recommended to use absolute paths for JAR files to prevent future issues, even if they are in the same directory</p>
</li>
<li>
<p>conf: This is used to set any Spark property. For example, you can set Spark properties like spark.executor.memory, spark.driver.memory, etc.</p>
<ul>
<li>spark.dynamicAllocation.enabled true: Enables dynamic memory allocation.</li>
<li>spark.dynamicAllocation.minExecutors 1: Sets the minimum number of executors to 1.</li>
<li>spark.dynamicAllocation.maxExecutors 10: Sets the maximum number of executors to 10. This prevents a single process from hogging all resources. This is beneficial because if a process reserves memory but doesn't use it, dynamic allocation can free up that idle memory for other processes.</li>
<li>Broadcast Threshold: This configuration determines the maximum size of data that Spark will automatically broadcast to all worker nodes when performing a join. The default is 10MB.</li>
<li>Broadcast Timeout: This sets the maximum time (in seconds) that a broadcast operation is allowed to take before timing out. A common general setting might be 600 seconds (10 minutes) or 1200 seconds (20 minutes), while 3600 seconds (1 hour) is considered very long and can significantly delay job completion</li>
<li>spark.executor.cores=2 sets the number of cores to use on each executor.</li>
<li>spark.executor.instances=5: sets the number of executor instances</li>
<li>spark.default.parallelism=20: sets the default number of partitions in RDDs returned by transformations like join(), reduceByKey(), and parallelize() when not set by user.</li>
<li>spark.driver.maxResultSize=1G:  limits the total size of the serialized results of all partitions for each Spark action (e.g., collect). This should be at least as large as the largest object you want to collect.</li>
<li>spark.network.timeout=800:  sets the default network timeout value to 800 seconds. This configuration plays a vital role in cases where you deal with large shuffles.</li>
</ul>
</li>
<li>
<p>driver-memory: Specifies the amount of memory allocated to the Spark Driver program.</p>
</li>
<li>
<p>executor-memory: Specifies the amount of memory allocated to each Spark Executor.</p>
</li>
<li>
<p>num-executors: Specifies the total number of executors to launch for the application. Combined with --executor-memory, this implies the total executor memory required (e.g., 2 GB/executor  5 executors = 10 GB total executor memory).</p>
</li>
<li>
<p>executor-cores: Specifies the number of CPU cores allocated to each executor. This determines how many parallel tasks an executor can run</p>
</li>
<li>
<p>files: This argument is used to specify non-Python files (e.g., configuration files like .ini, .json, .csv) that your Spark application needs. Similar to --py-files, these are bundled and distributed to all worker nodes</p>
</li>
</ul>
<p>After all the Spark Submit configurations, you provide the path to your main application script (e.g., main.py). Any values provided after the main script are treated as command-line arguments that can be accessed within your script.</p>
<ul>
<li>
<p>main.py: This is typically accessed as sys.argv in Python.</p>
</li>
<li>
<p>Subsequent arguments: These are sys.argv, sys.argv, and so on. They are useful for passing dynamic parameters like environment names (e.g., dev, qa, prod) to control execution flow within your script without changing the script itself</p>
</li>
<li>
<p>application-jar: This is a path to your compiled Spark application.</p>
</li>
<li>
<p>application-arguments: These are arguments that you need to pass to your Spark application</p>
</li>
</ul>
<p><img alt="Steps" src="../sparkmode.svg" /></p>
<blockquote>
<p>--- <strong>Client Mode</strong></p>
</blockquote>
<p><img alt="Steps" src="../sparkclient.svg" /></p>
<p>In Client mode, the Spark Driver runs directly on the edge node (or the machine from which the spark-submit command is executed).</p>
<p>The Executors, however, still run on the worker nodes within the cluster.</p>
<ul>
<li>Advantages:<ul>
<li>Easy Debugging and Real-time Logs: Logs (STD OUT and STD ERR) are generated directly on the client machine (edge node). This makes it very easy for developers to monitor the process, see real-time output, debug issues, and observe errors as they occur. This mode is highly suitable for development and testing of small code snippets.</li>
</ul>
</li>
<li>Disadvantages:<ul>
<li>Vulnerability to Edge Node Shutdown: If the edge node is shut down, either accidentally or intentionally, the Spark Driver (running on it) will be terminated. Since the Driver coordinates the entire application, its termination will cause all associated Executors to be killed, leading to the entire Spark job stopping abruptly and incompletely.</li>
<li>High Network Latency: Communication between the Driver (on the edge node) and the Executors (on worker nodes in the cluster) involves two-way communication across the network. This can introduce network latency, especially for operations like Broadcaster, where data needs to be first sent to the Driver and then distributed to Executors.</li>
<li>Potential for Driver Out of Memory (OOM) Errors: If multiple users submit jobs in Client mode from the same edge node, and their collective Driver memory requirements exceed the edge node's physical memory capacity (which is typically lower than worker nodes), processes may fail to start or encounter Driver OOM errors</li>
</ul>
</li>
</ul>
<p>When u start a spark shell, application driver creates the spark session in your local machine which request to Resource Manager present in cluster to create Yarn application. YARN Resource Manager start an Application Master (AM container). For client mode Application Master acts as the Executor launcher. Application Master will reach to Resource Manager and request for further containers. 
Resource manager will allocate new containers. These executors will directly communicate with Drivers which is present in the system in which you have submitted the spark application.</p>
<blockquote>
<p>--- <strong>Cluster Mode</strong></p>
</blockquote>
<p><img alt="Steps" src="../sparkcluster.svg" /></p>
<p>For cluster mode, theres a small difference compare to client mode in place of driver. Here Application Master will create driver in it and driver will reach to Resource Manager.</p>
<p>In Cluster mode, the Spark Driver (Application Master container) is launched and runs on one of the worker nodes within the Spark cluster. The Executors also run on other worker nodes in the cluster.</p>
<ul>
<li>Advantages:<ul>
<li>Resilience and Disconnect-ability: Once a Spark job is submitted in Cluster mode, the Driver runs independently within the cluster. This means the user can disconnect from or even shut down their edge node machine without affecting the running Spark application. This makes it ideal for long-running jobs.</li>
<li>Low Network Latency: Both the Driver and the Executors are running within the same cluster. This proximity significantly reduces network latency between them, leading to more efficient data transfer and communication.</li>
<li>Scalability and Resource Utilization: Worker nodes are provisioned with significant memory and processing capabilities. By running the Driver on a worker node, the application can leverage the cluster's robust resources, reducing the likelihood of Driver OOM issues, even with many concurrent jobs.</li>
<li>Suitable for Production Workloads: Cluster mode is the recommended deployment mode for production workloads, especially for scheduled jobs that run automatically and do not require constant real-time monitoring on the client side.</li>
</ul>
</li>
<li>Disadvantages:
    Indirect Log Access: Logs and output are not directly displayed on the client machine. When a job is submitted in Cluster mode, an Application ID is generated. Users must use this Application ID to access the Spark Web UI (User Interface) to track the job's status, progress, and logs. This adds an extra step for monitoring compared to Client mode</li>
</ul>
<blockquote>
<p>--- <strong>Local Mode</strong></p>
</blockquote>
<p><img alt="Steps" src="../localmode.svg" /></p>
<p>In local mode, Spark runs on a single machine, using all the cores of the machine. It is the simplest mode of deployment and is mostly used for testing and debugging.</p>
<blockquote>
<p>--- <strong>Comparison</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Client Mode</th>
<th>Cluster Mode</th>
</tr>
</thead>
<tbody>
<tr>
<td>Driver Location</td>
<td>Edge Node (or client machine)</td>
<td>Worker Node within the cluster</td>
</tr>
<tr>
<td>Log Generation</td>
<td>On client machine (STD OUT, STD ERR)</td>
<td>Application ID generated; view via Spark Web UI</td>
</tr>
<tr>
<td>Debugging</td>
<td>Easy, real-time feedback</td>
<td>Requires checking Spark Web UI</td>
</tr>
<tr>
<td>Network Latency</td>
<td>High (Driver &lt;-&gt; Executors across network)</td>
<td>Low (Driver &lt;-&gt; Executors within cluster)</td>
</tr>
<tr>
<td>Edge Node Shutdown</td>
<td>Application stops (Driver killed)</td>
<td>Application continues to run</td>
</tr>
<tr>
<td>Driver Out of Memory</td>
<td>Higher chance if many users/low edge node memory</td>
<td>Lower chance (cluster has more resources)</td>
</tr>
<tr>
<td>Use Case</td>
<td>Development, small code snippets, debugging</td>
<td>Production workloads, long-running jobs</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="aqe"><strong>AQE</strong><a class="headerlink" href="#aqe" title="Permanent link">#</a></h2>
<blockquote>
<p>--- <strong>Key Features of AQE</strong></p>
</blockquote>
<p>AQE provides three main capabilities to improve performance:
1.  Dynamically Coalescing Shuffle Partitions.
2.  Dynamically Switching Join Strategies.
3.  Dynamically Optimizing Skew Joins.</p>
<blockquote>
<p>--- <strong>Dynamically Coalescing Shuffle Partitions</strong></p>
</blockquote>
<p>When Spark shuffles data (e.g., during a <code>groupBy</code> or <code>join</code>), it creates a default number of shuffle partitionsusually 200.</p>
<p>If the dataset is small, 200 partitions result in many empty or tiny partitions. This wastes resources because the Spark scheduler must still manage, schedule, and monitor tasks for these empty partitions, leading to unnecessary overhead.</p>
<p>AQE's Shuffle Reader observes the data size after the shuffle. If it finds many small partitions, it merges (coalesces) them into a smaller number of larger partitions at runtime.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>A 25 MB dataset originally split into 200 partitions might be coalesced by AQE into a single partition, reducing the number of tasks from 200 to 1. This saves CPU cores and scheduling time.</p>
</div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-12-1"><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="c1"># External Information: Enabling AQE and Coalescing</span>
</span><span id="__span-12-2"><a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.adaptive.enabled&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span>
</span><span id="__span-12-3"><a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.adaptive.coalescePartitions.enabled&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span>
</span></code></pre></div>
<blockquote>
<p>--- <strong>Dynamically Switching Join Strategies</strong></p>
</blockquote>
<p>Spark usually decides a join strategy (like Sort-Merge Join or Broadcast Hash Join) during the initial planning phase based on the estimated size of the tables.</p>
<p>Initial estimates can be wrong. For example, a 10 GB table might be reduced to 5 MB after several filters and transformations. Without AQE, Spark would stick to the slower Sort-Merge Join.</p>
<p>AQE monitors the actual size of the data after transformations. If one side of the join becomes small enough (e.g., less than 10 MB), AQE dynamically switches the strategy from Sort-Merge Join to Broadcast Hash Join at runtime. Broadcast joins are significantly faster as they avoid the expensive shuffling and sorting required by Sort-Merge joins.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-13-1"><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="c1"># External Information: Conceptual Spark SQL example</span>
</span><span id="__span-13-2"><a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a><span class="c1"># Initial tables are large (10GB and 20GB), triggering Sort-Merge Join</span>
</span><span id="__span-13-3"><a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a><span class="n">df1</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;fact_sales&quot;</span><span class="p">)</span> <span class="c1"># 10GB</span>
</span><span id="__span-13-4"><a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a><span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s2">&quot;dim_products&quot;</span><span class="p">)</span> <span class="c1"># 20GB</span>
</span><span id="__span-13-5"><a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a>
</span><span id="__span-13-6"><a id="__codelineno-13-6" name="__codelineno-13-6" href="#__codelineno-13-6"></a><span class="c1"># A filter is applied that reduces dim_products to 5MB</span>
</span><span id="__span-13-7"><a id="__codelineno-13-7" name="__codelineno-13-7" href="#__codelineno-13-7"></a><span class="n">filtered_df2</span> <span class="o">=</span> <span class="n">df2</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">category</span> <span class="o">==</span> <span class="s2">&quot;Electronics&quot;</span><span class="p">)</span> 
</span><span id="__span-13-8"><a id="__codelineno-13-8" name="__codelineno-13-8" href="#__codelineno-13-8"></a>
</span><span id="__span-13-9"><a id="__codelineno-13-9" name="__codelineno-13-9" href="#__codelineno-13-9"></a><span class="c1"># With AQE enabled, Spark switches to Broadcast Join at runtime </span>
</span><span id="__span-13-10"><a id="__codelineno-13-10" name="__codelineno-13-10" href="#__codelineno-13-10"></a><span class="c1"># because filtered_df2 is now &lt; 10MB.</span>
</span><span id="__span-13-11"><a id="__codelineno-13-11" name="__codelineno-13-11" href="#__codelineno-13-11"></a><span class="n">result</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">filtered_df2</span><span class="p">,</span> <span class="s2">&quot;product_id&quot;</span><span class="p">)</span>
</span></code></pre></div>
<blockquote>
<p>--- <strong>Dynamically Optimizing Skew Joins</strong></p>
</blockquote>
<p>Data skew occurs when data is distributed unevenly across partitions. For instance, if "Sugar" accounts for 80% of sales, the partition containing "Sugar" will be much larger than others.</p>
<p>One large partition causes a "199 out of 200 tasks completed" scenario, where one task takes a very long time or fails with an Out of Memory (OOM) error.</p>
<p>AQE identifies skewed partitions and splits them into smaller sub-partitions.
    1.  The large partition (e.g., Sugar data) is split into multiple smaller parts.
    2.  To ensure the join still works, the corresponding data on the other side of the join (the non-skewed table) is duplicated for each new sub-partition.
   Thresholds for Skew: AQE identifies a partition as skewed if:
    1.  The partition size is greater than 256 MB.
    2.  The partition size is more than 5 times the median partition size.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-14-1"><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="c1"># External Information: AQE Skew Join Configurations</span>
</span><span id="__span-14-2"><a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.adaptive.skewJoin.enabled&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span>
</span><span id="__span-14-3"><a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.adaptive.skewJoin.skewedPartitionFactor&quot;</span><span class="p">,</span> <span class="s2">&quot;5&quot;</span><span class="p">)</span>
</span><span id="__span-14-4"><a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes&quot;</span><span class="p">,</span> <span class="s2">&quot;256MB&quot;</span><span class="p">)</span>
</span></code></pre></div>
<h2 id="cache-persist"><strong>Cache &amp; Persist</strong><a class="headerlink" href="#cache-persist" title="Permanent link">#</a></h2>
<p>The methods persist() and cache() in Apache Spark are used to save the RDD, DataFrame, or Dataset in memory for faster access during computation. They are effectively ways to optimize the execution of your Spark jobs, especially when you have repeated transformations on the same data. However, they differ in how they handle the storage:</p>
<blockquote>
<p>--- <strong>cache</strong></p>
</blockquote>
<ul>
<li>
<p>What is Caching in Spark?
Caching is an optimization technique in Spark that allows you to store intermediate results in memory. This prevents Spark from re-calculating the same data repeatedly when it is used multiple times in subsequent transformations.</p>
</li>
<li>
<p>Where is Cached Data Stored?
When data is cached, it is stored within the Storage Memory Pool of a Spark Executor. An executor's memory is divided into three parts: User Memory, Spark Memory, and Reserved Memory. Spark Memory, in turn, contains two pools: Storage Memory Pool and Execution Memory Pool. Cached data specifically resides in the Storage Memory Pool. If the Storage Memory Pool fills up, Spark might evict (remove) data that is not frequently used (using an LRU - Least Recently Used - fashion) or spill it to disk.</p>
</li>
<li>
<p>Why Do We Need Caching?
Spark operates with lazy evaluation, meaning transformations are not executed until an action is called. When an action is triggered, Spark builds a Directed Acyclic Graph (DAG) to determine the lineage of the data. If a DataFrame (DF) is used multiple times, Spark will re-calculate it from the beginning each time it's referenced, because DataFrames are immutable and executors' memories are short-lived</p>
</li>
<li>
<p>How Caching Helps: By calling .cache() on df, its intermediate result is stored in the Storage Memory Pool. Now, whenever df is needed again, Spark directly retrieves it from memory instead of re-calculating it. This significantly reduces computation time and improves efficiency</p>
</li>
<li>
<p>When Not to Cache?
You should avoid caching data when the DataFrame is very small or when its re-calculation time is negligible. Caching consumes memory, and if the benefits of caching don't outweigh the memory consumption, it's better to avoid it.</p>
</li>
<li>
<p>Limitations of Caching:
 If the cached data's partitions are larger than the available Storage Memory Pool, the excess partitions will not be stored in memory and will either be re-calculated on the fly or spilled to disk if using a storage level that supports it. Spark does not store partial partitions; a partition is stored entirely or not at all.
 If a cached partition is lost (e.g., due to an executor crash), Spark will re-calculate it using the DAG lineage</p>
</li>
<li>
<p>How to Uncache Data - To remove data from the cache, you can use the .unpersist() method.</p>
</li>
</ul>
<p>When you call df.cache(), it internally calls df.persist() with a default storage level of MEMORY_AND_DISK.</p>
<p>persist() offers more flexibility because it allows you to specify the desired storage level as an argument</p>
<blockquote>
<p>--- <strong>persist(storageLevel)</strong></p>
</blockquote>
<p>Storage levels define where data is stored (memory, disk, or both) and how it is stored (serialized or deserialized, and with replication). These levels provide fine-grained control over how cached data is managed, balancing performance, fault tolerance, and memory usage.</p>
<p>To use StorageLevel with persist(), you need to import it: from pyspark import StorageLevel</p>
<p>Here are the different storage levels explained:</p>
<ul>
<li>
<p>MEMORY_ONLY:</p>
<p>Stores data only in RAM (deserialized form). If memory is insufficient, partitions will be re-calculated when needed. Fastest processing because data is in memory and readily accessible. High memory utilization, potentially limiting other operations.</p>
<p>For small to medium-sized datasets that fit entirely in memory and where re-calculation overhead is high.</p>
</li>
<li>
<p>MEMORY_AND_DISK:</p>
<p>Default for cache(). Attempts to store data in RAM first (deserialized form).
If RAM is full, excess partitions are spilled to disk (serialized form). Provides a good balance of speed and resilience; data is less likely to be re-calculated.</p>
<p>DisDisk access is slower than memory. Data read from disk (serialized) requires CPU to deserialize it, leading to higher CPU utilization.
For larger datasets that might not fully fit in memory but where performance is still critical.</p>
</li>
<li>
<p>MEMORY_ONLY_SER:</p>
<p>Stores data in RAM only, but in a serialized form.
Serialization saves memory space, allowing more data to be stored in the same amount of RAM (e.g., 5GB uncompressed might become 8GB serialized).
DisData needs to be deserialized by the CPU when accessed, leading to higher CPU utilization and slightly slower access compared to MEMORY_ONLY.
This serialization specifically works for Java and Scala objects, and not for Python objects (though Python has its own pickling mechanisms, the _SER storage levels in Spark are typically for JVM objects).
When memory is a major constraint and you can tolerate increased CPU usage for deserialization.</p>
</li>
<li>
<p>MEMORY_AND_DISK_SER:</p>
<p>Stores data first in RAM (serialized), then spills to disk (serialized) if memory is full.
Combines memory saving of serialization with resilience of disk storage.
DisHigh CPU usage due to deserialization for both memory and disk reads.
For very large datasets where memory constraints are severe and some CPU overhead for deserialization is acceptable.</p>
</li>
<li>
<p>DISK_ONLY:</p>
<p>Stores data only on disk (serialized form).
Slowest storage level due to reliance on disk I/O.
Good for extremely large datasets that don't fit in memory, or for fault tolerance where data needs to be durable across executor restarts.
DisSignificantly slower than memory-based storage levels.
When performance is less critical than fault tolerance or when datasets are too large for memory.</p>
</li>
<li>
<p>Replicated Storage Levels (e.g., MEMORY_ONLY_2, DISK_ONLY_2):</p>
<p>These levels store two copies (2x replicated) of each partition across different nodes.
For example, MEMORY_ONLY_2 stores two copies in RAM on different executors.
Provides fault tolerance. If one executor or worker node goes down, the data can still be accessed from its replica, avoiding re-calculation from the DAG.
DisDoubles memory/disk consumption compared to non-replicated versions.
For highly critical data that is complex to calculate and must be readily available even if a node fails. Generally, cache() (which is MEMORY_AND_DISK) is preferred unless specific fault tolerance is required</p>
</li>
</ul>
<blockquote>
<p>--- <strong>Choosing the Right Storage Level</strong></p>
</blockquote>
<p>The choice of storage level depends on your specific needs:</p>
<ul>
<li>Start with MEMORY_ONLY: If your data fits in memory and transformations are simple, this is the fastest.</li>
<li>Move to MEMORY_AND_DISK: If data is larger and might not fit entirely in memory, or if re-calculation is expensive. This is the most commonly used for general caching.</li>
<li>Consider _SER options: Only if memory is a severe bottleneck and you can tolerate increased CPU usage for serialization/deserialization. Note that in Python, direct serialization using _SER levels like in Java/Scala might not provide the same benefits.</li>
<li>Use _2 options: Only for critical, complex data where high fault tolerance is a must and you can afford the doubled storage cost.</li>
</ul>
<h2 id="salting"><strong>Salting</strong><a class="headerlink" href="#salting" title="Permanent link">#</a></h2>
<p>Data Skew occurs when data is not distributed evenly across partitions. In a join operation, if one specific key (e.g., ID 1) has a massive number of records compared to others, all those records are sent to a single executor based on their hash.</p>
<p>The executor handling the skewed key becomes a bottleneck, taking significantly longer to finish while others sit idle, or it may even crash the application.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>A "best-selling product" might account for 90% of sales data, causing a massive skew for that specific product ID during a join.</p>
</div>
<blockquote>
<p>--- <strong>Why Other Methods Might Fail</strong></p>
</blockquote>
<ul>
<li>Re-partitioning: Standard re-partitioning often fails to solve skew because records with the same key are still hashed to the same partition.</li>
<li>Broadcasting: While broadcasting the smaller table avoids shuffling, it is only viable if the table is small (e.g., &lt;10MB). If both tables are large, broadcasting is not an option.</li>
<li>AQE (Adaptive Query Execution): While Sparks AQE provides some optimizations, it may not always resolve complex skew issues manually.</li>
</ul>
<p>Salting involves adding a random value (the "salt") to the join key to break a single large partition into multiple smaller ones.</p>
<ul>
<li>
<p>Step 1: Modifying the Skewed Table (Left Table)</p>
<p>In the skewed table, you append a random number (e.g., between 1 and 10) to the join key. This forces the records for the same ID to be distributed across 10 different partitions instead of one.</p>
</li>
<li>
<p>Step 2: Modifying the Reference Table (Right Table)</p>
<p>If you only salt the left table, the join will fail because the keys no longer match (e.g., "ID_1" vs "ID_1_5"). To fix this, you must replicate (explode) every record in the right table for every possible salt value used in the left table.</p>
</li>
<li>
<p>Before Salting: A task might show a massive gap between the minimum duration (e.g., 0.2 seconds) and the maximum duration (e.g., 6 seconds) because one executor is struggling with the skewed partition.</p>
</li>
<li>
<p>After Salting: The workload is balanced. The average time might be 9 seconds with a maximum of 12 seconds. While the total work might slightly increase due to replication, the overall job finishes faster because executors work in parallel rather than waiting for one skewed task to finish.</p>
</li>
</ul>
<h2 id="dynamice-resource-allocation"><strong>Dynamice Resource Allocation</strong><a class="headerlink" href="#dynamice-resource-allocation" title="Permanent link">#</a></h2>
<p>Dynamic Resource Allocation refers to the ability of a Spark application to dynamically increase or decrease the number of executors it uses based on the workload. This means resources are added when tasks are queued or existing ones need more processing power, and released when they become idle.</p>
<p>To make processes run faster and ensure other processes also get sufficient resources.</p>
<p>DRA is a cluster-level optimization technique, contrasting with code-level optimizations like join tuning, caching, partitioning, and coalescing.</p>
<blockquote>
<p>--- <strong>Static vs. Dynamic Resource Allocation Techniques</strong></p>
</blockquote>
<p>Spark offers two primary resource allocation techniques:</p>
<ul>
<li>
<p>Static Resource Allocation:
    In this approach, the application requests a fixed amount of memory (e.g., 100 GB) at the start, and it retains that allocated memory for the entire duration of the application's run, regardless of whether the memory is actively used or idle.
    Default behavior in Spark if DRA is not explicitly enabled.</p>
<p>Can lead to resource wastage if the application doesn't constantly utilize all allocated resources, making them unavailable for other jobs. This is particularly problematic for smaller jobs that have to wait for large, static jobs to complete, even if the larger job is underutilizing its resources.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>A heavy job requests 980 GB for executors and 20 GB for the driver (total 1 TB) on a 1 TB cluster. This saturates the entire cluster, leaving no resources for other users' jobs, even small ones. The resource manager, often operating on a First-In, First-Out (FIFO) policy, will make subsequent jobs wait until resources are freed.</p>
</div>
</li>
<li>
<p>Dynamic Resource Allocation:
    Dynamically adjusts resources by acquiring more executors when needed and releasing them when they become idle.</p>
<p>Optimizes cluster utilization by making resources available to other applications when they are not actively being used by a particular job.</p>
</li>
</ul>
<blockquote>
<p>--- <strong>How Dynamic Resource Allocation Works in Detail</strong></p>
</blockquote>
<ul>
<li>
<p>Initial Resource Request and Configuration</p>
<p>A Spark application typically requests resources using the spark-submit command. For DRA to work, specific configurations must be set.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul>
<li>spark.dynamicAllocation.enabled=true: This is the primary configuration to enable Dynamic Resource Allocation. By default, it is set to false (disabled).</li>
<li>spark.dynamicAllocation.minExecutors: Specifies the minimum number of executors that the application will always retain, even if idle. This helps prevent the process from failing if it releases too many resources and cannot re-acquire them when needed later. For example, setting it to 20 ensures at least 20 executors are always available.</li>
<li>spark.dynamicAllocation.maxExecutors: Specifies the maximum number of executors that the application can acquire. This acts as an upper limit for resource consumption.</li>
<li>--executor-memory 20G: Sets the memory for each executor.</li>
<li>--executor-cores 4: Sets the number of CPU cores for each executor, determining how many parallel tasks each executor can run.</li>
<li>--driver-memory 20G: Sets the memory for the driver program.</li>
</ul>
</div>
</li>
<li>
<p>Resource Release Mechanism</p>
<p>When an application no longer needs all its allocated resources, Spark can release them.</p>
<p>Resources are released when executors become idle, meaning they are not actively performing tasks.</p>
<p>By default, an executor will be released if it remains idle for 60 seconds. This can be configured using spark.dynamicAllocation.executorIdleTimeout.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Setting spark.dynamicAllocation.executorIdleTimeout=45s will release idle executors after 45 seconds.</p>
</div>
<p>Spark internally manages the release of resources. The resource manager (e.g., YARN, Mesos) does not directly request Spark applications to release resources.
For instance, if a process initially needed 1000 GB for a join but then transitions to a filter operation requiring only 500 GB, the extra 500 GB can be released, making them available for other processes.</p>
</li>
<li>
<p>Resource Acquisition (Demand) Mechanism</p>
<p>When an application's workload increases and it needs more resources, Spark will request them.</p>
<p>The driver program identifies the need for more memory/executors (e.g., for a large join operation).
Spark starts requesting more resources if it experiences a backlog of pending tasks for a certain duration. The default is 1 second. This can be configured using spark.dynamicAllocation.schedulerBacklogTimeout.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Configuration: Setting spark.dynamicAllocation.schedulerBacklogTimeout=2s will cause Spark to request resources after 2 seconds of a task backlog.</p>
</div>
<p>Spark does not request all needed resources at once. Instead, it requests them in a 2-fold manner (doubling the requested executors each time):
Initially, it might request 1 additional executor. If that's not enough, it will request 2 more. Then 4, then 8, then 16, and so on, until the required resources are met or maxExecutors is reached.</p>
</li>
</ul>
<blockquote>
<p>--- <strong>Challenges and Solutions in Dynamic Resource Allocation</strong></p>
</blockquote>
<p>While DRA offers significant benefits, it also presents challenges that need to be addressed:</p>
<ul>
<li>
<p>Challenge 1: Process Failure Due to Resource Unavailability
    If an application releases too many resources, and other processes quickly acquire them, the original application might not be able to get back the needed resources on demand, potentially leading to process failure, especially for memory-intensive operations like joins.</p>
<p>Solution: Configure spark.dynamicAllocation.minExecutors and spark.dynamicAllocation.maxExecutors.</p>
<ul>
<li>By setting a minExecutors value (e.g., 20 out of 49), you ensure that a baseline number of executors is always available to your application, preventing it from completely running out of resources and failing even if it releases others.</li>
<li>maxExecutors prevents over-allocation and resource monopolization.</li>
</ul>
</li>
<li>
<p>Challenge 2: Loss of Cached Data or Shuffle Output
    When executors are released, any cached data or shuffle output written to the local disk of those executors would be lost. This would necessitate re-calculation of that data, negating the performance benefits of DRA.</p>
<p>Solution: External Shuffle Service and Shuffle Tracking.</p>
<ul>
<li>External Shuffle Service (spark.shuffle.service.enabled=true): This service runs independently on worker nodes and is responsible for storing shuffle data. This ensures that even if an executor or worker node is released, the shuffle data persists and can be retrieved later by other executors or if the original executor is re-acquired.</li>
<li>Shuffle Tracking (spark.dynamicAllocation.shuffleTracking.enabled=true): This configuration ensures that shuffle output data is not deleted even when an executor is released. It works in conjunction with the external shuffle service to prevent the need for re-calculation of shuffled data throughout the Spark application's lifetime.</li>
</ul>
</li>
</ul>
<blockquote>
<p>--- <strong>When to Avoid Dynamic Resource Allocation</strong></p>
</blockquote>
<p>While DRA is generally beneficial, there are specific scenarios where it should be avoided:</p>
<ul>
<li>Critical Production Processes: For critical production jobs where any delay or potential failure due to resource fluctuation is unacceptable, it is advisable to use Static Memory Allocation. This ensures predictable resource availability and minimizes risk.</li>
<li>Non-Critical Processes / Development: For processes that have some bandwidth for resource fluctuations, or for development and testing environments, Dynamic Resource Allocation is highly recommended</li>
</ul>
<blockquote>
<p>--- <strong><em>Dynamic Partition Pruning (DPP)</em></strong></p>
</blockquote>
<p>Dynamic Partition Pruning (DPP) is an optimization technique in Apache Spark that enhances query performance, especially when dealing with partitioned data and join operations.</p>
<ol>
<li>
<p>Understanding Partition Pruning</p>
<p>Before diving into Dynamic Partition Pruning, it's essential to understand standard Partition Pruning.</p>
<p>Partition pruning is a mechanism where Spark avoids reading unnecessary data partitions based on filter conditions. It "prunes" or removes data that is not relevant to the query.</p>
<p>When data is partitioned on a specific column (e.g., sales_date), and a query applies a filter directly on that partitioning column, Spark can identify and read only the partitions that contain the relevant data. This significantly reduces the amount of data scanned.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<ul>
<li>Imagine a large sales_data dataset partitioned by sales_date. Each date has its own partition.</li>
<li>If you run a query like SELECT  FROM sales_data WHERE sales_date = '2019-04-19', Spark, with partition pruning enabled, will only read the data for April 19, 2019.</li>
<li>Observed in Spark UI: In this example, if there are 123 total partitions, Spark will only read 1 file. The Spark UI's "SQL" tab details will show "Partition Filter" applied, indicating that the date has been cast and used for filtering.</li>
</ul>
</div>
</li>
<li>
<p>The Issue: When Standard Partition Pruning Fails</p>
<p>Standard partition pruning works efficiently when the filter condition is directly applied to the partitioning column of the table being queried. However, a common scenario where it fails is when:
You have two dataframes (or tables), say df1 and df2.
df1 is a partitioned table (e.g., partitioned by date).
You need to join df1 and df2.
The filter condition originates from df2 (the non-partitioned table or the table that is not the primary partitioned table being filtered).
In such a case, because the filter is applied on df2 and not directly on df1's partitioning column, Spark's optimizer (without DPP) won't know which partitions of df1 to prune at the planning stage.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<ul>
<li>Data: df1 is sales_data (partitioned by sales_date) and df2 is a date_dimension table (containing date and week_of_year columns).</li>
<li>Goal: Find sales data for a specific week, e.g., week = 16.</li>
<li>Query Concept: df1 is joined with df2 (e.g., on date columns), and then df2 is filtered for week_of_year = 16.</li>
<li>Configuration for Demonstration: To observe this issue, Spark's default behavior needs to be overridden by explicitly disabling Dynamic Partition Pruning (spark.sql.set('spark.sql.optimizer.dynamicPartitionPruning.enabled', 'false')) and also potentially disabling broadcast joins.</li>
<li>Observed in Spark UI: When this query is run with DPP disabled, Spark will scan all 123 files of the sales_data table, even though only a few dates (and thus partitions) might be relevant for week 16. The "Partition Filter" section in the Spark UI for df1 will show no effective pruning related to the join condition. This leads to performance degradation.</li>
</ul>
</div>
</li>
<li>
<p>Dynamic Partition Pruning (DPP): The Solution</p>
<p>Dynamic Partition Pruning (DPP) addresses the performance issue described above by enabling Spark to prune partitions at runtime.</p>
<p>DPP is an optimization technique that allows Spark to update filter conditions dynamically at runtime.</p>
<ul>
<li>Filter Small Table: Spark first filters the smaller table (df2, e.g., date_dimension for week = 16) to identify the relevant values (e.g., specific dates that fall in week 16).</li>
<li>Broadcast: The relevant values (e.g., the list of specific dates) from the filtered smaller table are broadcasted to all executor nodes. Broadcasting makes this small dataset available on all nodes where the larger table is processed.</li>
<li>Subquery Injection: At runtime, Spark then uses these broadcasted values to create a subquery (similar to an IN clause) for the partitioned table (df1). For instance, it essentially transforms the query to look like: SELECT  FROM big_table WHERE sales_date IN (SELECT dates FROM small_table).</li>
<li>Dynamic Pruning: This subquery allows Spark to dynamically identify and prune the irrelevant partitions of the large table (df1), reading only the necessary ones.</li>
</ul>
<div class="admonition example">
<p class="admonition-title">Example</p>
<ul>
<li>Using the same sales_data (df1) and date_dimension (df2) tables, and the join with week = 16 filter.</li>
<li>Configuration: DPP is enabled (by default in Spark 3.0+ or explicitly enabled) and the broadcast mechanism is active.</li>
<li>Observed in Spark UI: When run with DPP enabled, Spark will only read a small subset of files (e.g., 3 files out of 123 total partitions), as only those files contain the dates relevant to week 16. The "Partition Filter" in the Spark UI will clearly show a "Dynamic Pruning Expression" applied to sales_date. You will also see "Broadcast Exchange" in the execution plan, indicating that the smaller table was broadcasted.</li>
</ul>
</div>
</li>
<li>
<p>Key Conditions for Dynamic Partition Pruning</p>
<p>For Dynamic Partition Pruning to work effectively, two primary conditions must be met:</p>
<ol>
<li>Partitioned Data: The data in the larger table (df1 in our example) must be partitioned on the column used in the join and filter condition (e.g., sales_date). If the data is not partitioned, DPP cannot apply.</li>
<li>Broadcastable Second Table: The second table (df2), which provides the filter condition, must be broadcastable. This means it should be small enough to fit into memory and be efficiently broadcasted to all executor nodes. If it's too large, it won't be broadcasted, and DPP might not engage. You can also adjust Spark's broadcast threshold value if needed.</li>
</ol>
</li>
</ol>
<h2 id="spark-streaming"><strong>Spark Streaming</strong><a class="headerlink" href="#spark-streaming" title="Permanent link">#</a></h2>
<blockquote>
<p>--- <strong>Converting Batch to Streaming Code</strong></p>
</blockquote>
<p>One of the core benefits of Spark is that converting batch code to streaming is straightforward.</p>
<ol>
<li>Reading: Change <code>.read</code> to <code>.readStream</code>.</li>
<li>Source: Change the format from <code>text</code> to <code>socket</code> and provide <code>host</code> and <code>port</code> options.</li>
<li>Transformations: The logic for <code>split</code>, <code>explode</code>, and <code>groupBy</code> remains exactly the same.</li>
<li>Writing: Instead of <code>.show()</code>, use <code>.writeStream</code> with a defined <code>format</code> (e.g., <code>console</code>) and an <code>outputMode</code>.</li>
<li>Driver Connection: Add <code>.awaitTermination()</code> to ensure the driver stays active while the streaming application runs on executors.</li>
</ol>
<blockquote>
<p>--- <strong>Key Streaming Concepts</strong></p>
</blockquote>
<ul>
<li>
<p>Output Mode ("complete"): In this mode, the entire updated result table is displayed in the console every time a new batch is processed.</p>
</li>
<li>
<p>Micro-Batches: When you type "hello world" into the terminal, Spark processes it as the first batch. If you type "world" again, it processes a second batch and updates the count for "world" to 2.</p>
</li>
<li>
<p>Checkpoints: Spark automatically creates a temporary checkpoint location to store metadata for the streaming application. Checkpoints ensure that the application can track its progress and state.</p>
</li>
<li>
<p>Schema Handling: For socket sources, Spark defaults to a schema with a single column called <code>value</code> of type string.</p>
</li>
</ul>
<blockquote>
<p>--- <strong>Spark Streaming Output Modes</strong></p>
</blockquote>
<p>Spark offers three primary output modes that define how the result of a transformation is written to the output sink.</p>
<ul>
<li>Complete Mode</li>
</ul>
<p>In Complete Mode, the entire updated result table is written to the sink every time a micro-batch is processed.Even if a specific record was not present in the current micro-batch, it will still appear in the output if it was part of a previous batch.</p>
<ul>
<li>Update Mode</li>
</ul>
<p>In Update Mode, Spark only outputs the rows that were updated or newly added in the most recent micro-batch. If a record from a previous batch is not updated by the new data, it is excluded from the current output.</p>
<ul>
<li>Append Mode</li>
</ul>
<p>In Append Mode, only new rows added to the result table since the last trigger are outputted. Once data is written, it is "locked" and cannot be updated. This makes it ideal for logging where data is only added at the bottom. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Append mode is often used in conjunction with watermarks to handle stateful data, which is discussed in more advanced sessions.</p>
</div>
<p>So in summary:</p>
<ul>
<li>Complete mode gives you the entire Result Table every time. It's like taking a complete snapshot of your computation's current state after every batch.</li>
<li>Append mode only gives you completely new rows. It's suitable for when your Result Table is effectively growing over time with new data, such as when you're just adding new rows and not updating existing ones.</li>
<li>Update mode gives you any rows that are new or have been updated. It's a middle ground between complete and append mode, giving you a view of what's changed in your Result Table since the last batch.</li>
</ul>
<blockquote>
<p>--- <strong>Lambda Architecture</strong></p>
</blockquote>
<p>Lambda architecture is defined by having two distinct processing pipelines: a Batch Layer and a Speed (Streaming) Layer.</p>
<ul>
<li>How it Works:
    Batch Layer: Processes raw data in large volumes at scheduled intervals (e.g., daily or weekly). This layer is used when high latency is acceptable.
    Speed Layer: Also known as the streaming pipeline, it processes data in real-time for immediate insights.
    Serving Layer: A single, common layer that connects to various applications (real-time, batch, or mixed) and queries the results from both the batch and speed layers.</li>
<li>Challenges:
    Code Duplication: Because there are two separate pipelines, you often have to write and maintain the same logic twiceonce for the batch code and once for the streaming code.
    Latency: Data processed through the batch pipeline is subject to delays because it only runs based on a schedule.</li>
</ul>
<blockquote>
<p>--- <strong>Kappa Architecture</strong></p>
</blockquote>
<p>Kappa architecture is a simpler alternative that utilizes only one processing pipeline: the Speed Layer.</p>
<ul>
<li>
<p>How it Works:</p>
<p>Unified Pipeline: The same streaming pipeline is used to process both real-time data and historical (batch) data.
Single Flow: All raw data flows through the streaming pipeline and is served through a common data serving layer to all applications.
Efficiency: It solves the problem of code duplication because you only maintain a single codebase.
- Challenges:</p>
<p>Out-of-Order Data: Because it is solely based on speed and continuous streaming, data may arrive out of chronological order, which requires handling via "watermarks" (a topic for future discussion).</p>
</li>
</ul>
<blockquote>
<p>--- <strong>Comparison Summary</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Lambda Architecture</th>
<th style="text-align: left;">Kappa Architecture</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pipelines</td>
<td style="text-align: left;">Two (Batch + Speed)</td>
<td style="text-align: left;">One (Speed/Streaming)</td>
</tr>
<tr>
<td style="text-align: left;">Complexity</td>
<td style="text-align: left;">Higher (Code duplication)</td>
<td style="text-align: left;">Lower (Single codebase)</td>
</tr>
<tr>
<td style="text-align: left;">Latency</td>
<td style="text-align: left;">High in batch layer</td>
<td style="text-align: left;">Low (Real-time delivery)</td>
</tr>
<tr>
<td style="text-align: left;">Data Order</td>
<td style="text-align: left;">Handled by batch reprocessing</td>
<td style="text-align: left;">Challenges with out-of-order data</td>
</tr>
</tbody>
</table>
<p>When setting up the Spark session for streaming, a specific configuration is recommended to ensure data integrity during shutdowns.</p>
<ul>
<li>Graceful Shutdown: Setting <code>spark.streaming.stopGracefullyOnShutdown</code> to <code>true</code> ensures that if the application is shut down, it finishes processing any data already "in line" before stopping the pipeline.</li>
<li>Schema Inference: For streaming, you must explicitly enable schema inference to allow Spark to identify the JSON structure at runtime.</li>
</ul>
<div class="language-python highlight"><pre><span></span><code><span id="__span-15-1"><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a><span class="c1"># Configuration for streaming schema inference and graceful shutdown</span>
</span><span id="__span-15-2"><a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.sql.streaming.schemaInference&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span>
</span><span id="__span-15-3"><a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a><span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.streaming.stopGracefullyOnShutdown&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span>
</span></code></pre></div>
<blockquote>
<p>--- <strong>Advanced File Streaming Options</strong></p>
</blockquote>
<ul>
<li><code>cleanSource</code>: Controls what happens to the input file after it is read. Options include <code>off</code> (default), <code>delete</code>, or <code>archive</code>.</li>
<li><code>sourceArchiveDir</code>: Required when using <code>archive</code>; it specifies the folder where processed files are moved.</li>
<li><code>maxFilesPerTrigger</code>: Determines how many files are consumed in a single micro-batch. Setting this to <code>1</code> ensures Spark processes only one file at a time, which is useful for controlling resource usage in production.</li>
</ul>
<blockquote>
<p>--- <strong>Spark Streaming Trigger Types</strong></p>
</blockquote>
<p>Triggers define the timing of streaming data processing. The sources outline three main types:</p>
<ul>
<li>Once (or <code>availableNow</code>)</li>
</ul>
<p>This trigger causes the streaming pipeline to behave like a batch job. 
It consumes all data currently available in the source, processes it, and then automatically shuts down the pipeline. Ideal for Kappa architectures where you want to use the same streaming code to process historical data as a one-time batch.</p>
<ul>
<li>Processing Time</li>
</ul>
<p>This is the standard micro-batch trigger where you specify a recurring time interval.
Spark triggers a micro-batch at the defined interval (e.g., every 10 seconds). It processes any new data that arrived during that window.</p>
<ul>
<li>Continuous (Experimental)</li>
</ul>
<p>Currently experimental in Spark 3.3.0, this mode offers the lowest latency.
It does not use micro-batches. Instead, it processes data continuously. The time interval provided (e.g., '10 seconds') defines how often Spark records a checkpoint, not how often it processes data.
It does not yet support all transformations (such as <code>explode</code>) or all sinks. For example, it is demonstrated using a memory sink.</p>
<blockquote>
<p>--- <strong>The Problem with Multiple <code>writeStream</code> Commands</strong></p>
</blockquote>
<p>A common mistake is trying to call <code>writeStream</code> multiple times on the same streaming DataFrame to send data to 
different locations. This approach causes several issues:</p>
<p>Each <code>writeStream</code> command triggers its own Spark job for every micro-batch. This means the entire DAG (Directed Acyclic Graph) is re-computed, and the source data is read twice for two different streams.</p>
<p>Each stream must maintain its own checkpoint location to track metadata.Because different sinks have different latencies, one stream might process data faster than the other, leading to them processing different offsets at any given time.</p>
<ul>
<li>The Solution: <code>foreachBatch</code></li>
</ul>
<p>To handle multiple sinks effectively, Spark provides the <code>foreachBatch</code> function. This function takes a Python function as input and executes it for every micro-batch.</p>
<p>The data is read from the source and processed through the transformations only once per micro-batch. You only need to maintain one checkpoint location for the entire process. Within the provided Python function, you can use standard batch <code>write</code> commands to send data to as many locations as needed.</p>
<blockquote>
<p>--- <strong>Event Time vs. Processing Time</strong></p>
</blockquote>
<p>Understanding the distinction between these two timestamps is vital for accurate streaming analytics.</p>
<ul>
<li>
<p>Event Time: 
The time at which the data was actually generated at the source (e.g., the moment a sensor in Sydney records a temperature).</p>
</li>
<li>
<p>Processing Time: 
The time at which the data arrives at the processing engine (e.g., Spark) to be ingested.</p>
</li>
</ul>
<blockquote>
<p>--- <strong>The Problem of Late Arrival</strong></p>
</blockquote>
<p>Due to geographical distances or network latency, data generated at the same time might arrive at the processing center at different times. 
   Scenario: A device in Delhi (D1) and a device in Sydney (D2) both record a temperature at 12:04 (Event Time).
   Result: D1 arrives almost instantly, while D2 arrives at 1:10 (Processing Time).
   Miscalculation: If you aggregate by processing time, D1 falls into the 12:001:00 window, while D2 falls into the 1:002:00 window. This leads to incorrect averages because D2 should have been included in the 12:00 window based on its actual generation time.</p>
<blockquote>
<p>--- <strong>Stateful Processing</strong></p>
</blockquote>
<p>To perform aggregations like "hourly average temperature" correctly, Spark must perform stateful processing.</p>
<p>Logic: Spark holds the "state" (the current sum and count of temperatures) for specific time windows in its memory.
   Updating State: When data generated at 12:04 arrives late (even hours later), Spark must go back into its memory, find the 12:001:00 window, and update the calculation.</p>
<blockquote>
<p>--- <strong>Managing Memory with Watermarks</strong></p>
</blockquote>
<p>Spark cannot hold state in its memory indefinitely; otherwise, it would eventually run out of memory (OOM), especially if data arrives days or weeks late.</p>
<p>Watermarks: A watermark is a threshold or "timeout" that tells Spark how long to keep the state for a specific window in memory.</p>
<p>Discarding Late Data: If you define a watermark of 2 hours, Spark will wait for late data for up to two hours after the window closes. Any data arriving after that period is automatically discarded to free up memory.</p>
<blockquote>
<p>--- <strong>Summary Table</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align: left;">Concept</th>
<th style="text-align: left;">Definition</th>
<th style="text-align: left;">Importance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Event Time</td>
<td style="text-align: left;">When data was generated.</td>
<td style="text-align: left;">Essential for accurate analytics.</td>
</tr>
<tr>
<td style="text-align: left;">Processing Time</td>
<td style="text-align: left;">When data arrived at Spark.</td>
<td style="text-align: left;">Easier to track but can lead to miscalculations.</td>
</tr>
<tr>
<td style="text-align: left;">State</td>
<td style="text-align: left;">Data kept in Spark's memory.</td>
<td style="text-align: left;">Allows updates to old windows when late data arrives.</td>
</tr>
<tr>
<td style="text-align: left;">Watermark</td>
<td style="text-align: left;">A "timeout" for late data.</td>
<td style="text-align: left;">Prevents memory exhaustion by discarding very late data.</td>
</tr>
</tbody>
</table>
<blockquote>
<p>--- <strong>Overview of Window Operations</strong></p>
</blockquote>
<p>Window operations are essential for stateful processing, allowing you to perform group aggregations (like word counts or temperature averages) over specific time segments. These operations rely on event time to ensure that data is placed into the correct chronological bucket, even if it arrives out of order.</p>
<ul>
<li>Tumbling Windows (Fixed Windows)</li>
</ul>
<p>Tumbling windows are of a fixed, constant size and do not overlap. Once a window ends, a new one begins immediately. Each event belongs to exactly one window.</p>
<p>Example Logic: With a 10-minute window and a 5-minute trigger, Spark processes data in non-overlapping blocks (e.g., 12:0012:10, 12:1012:20).</p>
<p>Results for a specific window only get updated when events with an event time falling within that window arrive.</p>
<ul>
<li>Sliding Windows (Overlapping Windows)</li>
</ul>
<p>Sliding windows are also of a fixed size but overlap each other for a specific duration.
Because they overlap, a single event can fall into multiple windows simultaneously.</p>
<p>Example Logic: If the window size is 10 minutes and the "slide" duration is 5 minutes, the windows will overlap by 5 minutes (e.g., W1: 12:0012:10 and W2: 12:0512:15).</p>
<p>When an event arrives at 12:07, Spark must update both W1 and W2 because 12:07 falls within both ranges.</p>
<ul>
<li>Session Windows</li>
</ul>
<p>Session windows do not have a fixed size. Instead, they are defined by a session gap or a period of inactivity.
A session stays "open" as long as events are flowing. It automatically terminates once the specified gap duration passes without any new activity.</p>
<p>Example Logic: If the session gap is 5 minutes and the last event occurred at 12:09, the window will terminate at 12:14. If a new user logs in at 12:15 and has no further activity, that session terminates at 12:20.</p>
<blockquote>
<p>--- <strong>The Role of Watermarks in Windowing</strong></p>
</blockquote>
<p>In stateful windowing, Spark must keep aggregations in memory to update them when late data arrives. Watermarks prevent memory exhaustion by defining a threshold for how long Spark should wait for late events.</p>
<p>The watermark is calculated as the <code>Latest Event Time - Watermark Duration</code>.</p>
<p>If the latest event is 12:17 and the watermark is 10 minutes, the threshold is 12:07. Any event generated before 12:07 that arrives at 12:20 will be ignored and will not update previous windows. This allows Spark to clear old aggregations from its memory.</p>
<ul>
<li>Summary Table</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">Window Type</th>
<th style="text-align: left;">Fixed Size?</th>
<th style="text-align: left;">Overlapping?</th>
<th style="text-align: left;">Trigger Basis</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Tumbling</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Fixed time intervals</td>
</tr>
<tr>
<td style="text-align: left;">Sliding</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Fixed time with overlap</td>
</tr>
<tr>
<td style="text-align: left;">Session</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Inactivity/Session gap</td>
</tr>
</tbody>
</table>
<blockquote>
<p>--- <strong>Implementing Windows and Watermarks</strong></p>
</blockquote>
<ul>
<li>
<p>Tumbling (Fixed) Window: A window of a fixed duration (e.g., 10 minutes) where windows do not overlap.</p>
</li>
<li>
<p>Sliding (Overlapping) Window: Created by adding a sliding interval (e.g., a 10-minute window that slides every 5 minutes). This causes windows to overlap.</p>
</li>
<li>
<p>Watermarks: A threshold used to handle late events. A watermark of 10 minutes tells Spark to discard any data arriving more than 10 minutes after the latest event timestamp processed.</p>
</li>
</ul>
<p>Aggregation Code Example:
<div class="language-python highlight"><pre><span></span><code><span id="__span-16-1"><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="c1"># Grouping with a 10-minute watermark and 10-minute tumbling window</span>
</span><span id="__span-16-2"><a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a><span class="n">final_df</span> <span class="o">=</span> <span class="n">words_df</span> \
</span><span id="__span-16-3"><a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a>    <span class="o">.</span><span class="n">withWatermark</span><span class="p">(</span><span class="s2">&quot;event_time&quot;</span><span class="p">,</span> <span class="s2">&quot;10 minutes&quot;</span><span class="p">)</span> \
</span><span id="__span-16-4"><a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a>    <span class="o">.</span><span class="n">groupBy</span><span class="p">(</span>
</span><span id="__span-16-5"><a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a>        <span class="n">window</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;event_time&quot;</span><span class="p">),</span> <span class="s2">&quot;10 minutes&quot;</span><span class="p">),</span> 
</span><span id="__span-16-6"><a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a>        <span class="n">col</span><span class="p">(</span><span class="s2">&quot;word&quot;</span><span class="p">)</span>
</span><span id="__span-16-7"><a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a>    <span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span> \
</span><span id="__span-16-8"><a id="__codelineno-16-8" name="__codelineno-16-8" href="#__codelineno-16-8"></a>    <span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;window.start&quot;</span><span class="p">,</span> <span class="s2">&quot;window.end&quot;</span><span class="p">,</span> <span class="s2">&quot;word&quot;</span><span class="p">,</span> <span class="s2">&quot;count&quot;</span><span class="p">)</span>
</span></code></pre></div></p>
<blockquote>
<p>--- <strong>Comparing Output Modes and Watermark Effects</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Update Mode</th>
<th style="text-align: left;">Complete Mode</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Behavior</td>
<td style="text-align: left;">Only outputs rows updated in the current batch.</td>
<td style="text-align: left;">Rewrites the entire result table to the sink.</td>
</tr>
<tr>
<td style="text-align: left;">Late Data</td>
<td style="text-align: left;">Respects watermarks. Discards data arriving after the cut-off.</td>
<td style="text-align: left;">Ignores watermarks. Keeps all historical data in memory to print the full table.</td>
</tr>
<tr>
<td style="text-align: left;">Memory</td>
<td style="text-align: left;">Efficient; discards old state based on watermark.</td>
<td style="text-align: left;">Higher risk of Out of Memory (OOM) errors in stateful processing.</td>
</tr>
</tbody>
</table>
<p>Practical Example of Late Data:
   Latest Event: 12:17.
   Watermark: 10 minutes (Cut-off time = 12:07).
   Scenario A (Late Data): An event at 11:04 arrives. Update mode discards it, while Complete mode creates a new window for it.
   Scenario B (Acceptable Delay): An event at 12:08 arrives. Because it is after the 12:07 cut-off, both modes update the counts for the 12:0012:10 window.</p>
<h2 id="spark-best-practises"><strong>Spark Best Practises</strong><a class="headerlink" href="#spark-best-practises" title="Permanent link">#</a></h2>
<blockquote>
<p>--- <strong>How to deal with data skewness</strong></p>
</blockquote>
<p>Handling data skewness is a common challenge in distributed computing frameworks like Apache Spark. </p>
<p>Here are some popular techniques to mitigate it:
- Salting: Salting involves adding a random component to a skewed key to create additional unique keys. After performing the operation (like a join), the extra key can be dropped to get back to the original data.</p>
<ul>
<li>
<p>Splitting skewed data: Identify the skewed keys and process them separately. For instance, you can filter out the skewed keys and perform a separate operation on them.</p>
</li>
<li>
<p>Increasing the number of partitions: Increasing the number of partitions can distribute the data more evenly. However, this might increase the overhead of managing more partitions.</p>
</li>
<li>
<p>Using reduceByKey instead of groupByKey: reduceByKey performs local aggregation before shuffling the data, which reduces the data transferred over the network.</p>
</li>
<li>
<p>Using Broadcast Variables: When joining a large DataFrame with a small DataFrame, you can use broadcast variables to send a copy of the small DataFrame to all nodes. This avoids shuffling of the large DataFrame.</p>
</li>
</ul>
<blockquote>
<p>--- <strong>Driver Failure</strong></p>
</blockquote>
<p>The driver program runs the main() function of the application and creates a SparkContext. If the driver node fails, the entire application will be terminated, as it's the driver program that declares transformations and actions on data and submits such requests to the cluster.</p>
<ul>
<li>
<p>Impact:</p>
</li>
<li>
<p>The driver node is a single point of failure for a Spark application.</p>
</li>
<li>
<p>If the driver program fails due to an exception in user code, the entire Spark application is terminated, and all executors are released.</p>
</li>
<li>
<p>Handling Driver Failure:</p>
</li>
<li>
<p>Driver failure is usually fatal, causing the termination of the application.</p>
</li>
<li>It's crucial to handle exceptions in your driver program to prevent such failures.</li>
<li>Also, monitor the health of the machine hosting the driver program to prevent failures due to machine errors.</li>
<li>In some cluster managers like Kubernetes, Spark supports mode like spark.driver.supervise to supervise and restart the driver on failure.</li>
</ul>
<blockquote>
<p>--- <strong>Executor Failure</strong></p>
</blockquote>
<p>Executors in Spark are responsible for executing the tasks. When an executor fails, the tasks that were running will fail.</p>
<ul>
<li>
<p>Impact:</p>
</li>
<li>
<p>Executors can fail for various reasons, such as machine errors or OOM errors in the user's application.</p>
</li>
<li>If an executor fails, the tasks that were running on it are lost.</li>
<li>
<p>The failure of an executor doesn't cause the failure of the Spark application, unless all executors fail.</p>
</li>
<li>
<p>Handling Executor Failure:</p>
</li>
<li>
<p>If an executor fails, Spark can reschedule the failed tasks on other executors.</p>
</li>
<li>There is a certain threshold for task failures. If the same task fails more than 4 times (default), the application will be terminated.</li>
<li>Make sure to tune the resources allocated for each executor, as an executor might fail due to insufficient resources.</li>
<li>For resilience, you can also opt to replicate the data across different executor nodes.</li>
</ul>
<blockquote>
<p>--- <strong>Spark Driver OOM Scenarios</strong></p>
</blockquote>
<ol>
<li>
<p>Large Collect Operations: If the data collected from executors using actions such as collect() or take() is too large to fit into the driver's memory, an OutOfMemoryError will occur. <br />
   Solution: Be cautious with actions that pull large volumes of data into the driver program. Use actions like take(n), first(), collect() carefully, and only when the returned data is manageable by the driver.</p>
</li>
<li>
<p>Large Broadcast Variables: If a broadcast variable is larger than the amount of free memory on the driver node, this will also cause an OOM error.
   Solution: Avoid broadcasting large variables. If possible, consider broadcasting a common subset of the data, or use Spark's built-in broadcast join if joining with a large DataFrame.</p>
</li>
<li>
<p>Improper Driver Memory Configuration: If spark.driver.memory is set to a high value, it can cause the driver to request more memory than what is available, leading to an OOM error.
   Solution: Set the spark.driver.memory config based on your application's need and ensure it doesn't exceed the physical memory limits.</p>
</li>
</ol>
<blockquote>
<p>--- <strong>Spark Executor OOM Scenarios</strong></p>
</blockquote>
<ol>
<li>
<p>Large Task Results: If the result of a single task is larger than the amount of free memory on the executor node, an OutOfMemoryError will occur.
   Solution: Avoid generating large task results. This is often due to a large map operation. Consider using reduceByKey or aggregateByKey instead of groupByKey when transforming data.</p>
</li>
<li>
<p>Large RDD or DataFrame operations: Certain operations on RDDs or DataFrames, like join, groupByKey, reduceByKey, can cause data to be shuffled around, leading to a large amount of data being held in memory at once, potentially causing an OOM error.
   Solution: Be cautious with operations that require shuffling large amounts of data. Use operations that reduce the volume of shuffled data, such as reduceByKey and aggregateByKey, instead of groupByKey.</p>
</li>
<li>
<p>Persistent RDDs/DataFrames: If you're persisting many RDDs/DataFrames in memory and there isn't enough memory to store them, this will also cause an OOM error.
   Solution: Unpersist unnecessary RDDs and DataFrames as soon as they are no longer needed. Tune the spark.memory.storageFraction to increase the amount of memory reserved for cached RDDs/DataFrames.</p>
</li>
<li>
<p>Improper Executor Memory Configuration: Similar to the driver, if spark.executor.memory is set to a high value, it can cause the executor to request more memory than what is available, leading to an OOM error.
   Solution: Set the spark.executor.memory config based on your application's need and ensure it doesn't exceed the physical memory limits of the executor nodes.</p>
</li>
</ol>
<blockquote>
<p>--- <strong>Code Level Optimization</strong></p>
</blockquote>
<ol>
<li>
<p>Use DataFrames/Datasets instead of RDDs: DataFrames and Datasets have optimized execution plans, leading to faster and more memory-efficient operations than RDDs. They also have more intuitive APIs for many operations.</p>
</li>
<li>
<p>Leverage Broadcasting: If you're performing an operation like a join between a large DataFrame and a small DataFrame, consider broadcasting the smaller DataFrame. Broadcasting sends the smaller DataFrame to all worker nodes, so they have a local copy and don't need to fetch the data across the network.</p>
</li>
<li>
<p>Avoid Shuffling: Operations like groupByKey cause shuffling, where data is transferred across the network, which can be slow. Operations like reduceByKey or aggregateByKey reduce the amount of data that needs to be shuffled, and can be faster.</p>
</li>
<li>
<p>Avoid Collecting Large Data: Be careful with operations like collect() that bring a large amount of data into the driver program, which could cause an out of memory error.</p>
</li>
<li>
<p>Repartitioning and Coalescing: Depending on your use case, you might want to increase or decrease the number of partitions. If you have too many small partitions, use coalesce to combine them. If you have too few large partitions, use repartition to split them.</p>
</li>
<li>
<p>Persist/Cache Wisely: Persist or cache the DataFrames or RDDs that you'll reuse. However, keep in mind that these operations consume memory, so use them judiciously.</p>
</li>
</ol>
<blockquote>
<p>--- <strong>Resource Configuration Optimization</strong></p>
</blockquote>
<ol>
<li>
<p>Tune Memory Parameters: Make sure to set spark.driver.memory, spark.executor.memory, spark.memory.fraction, and spark.memory.storageFraction based on the memory requirements of your application and the capacity of your hardware.</p>
</li>
<li>
<p>Control Parallelism: Use spark.default.parallelism and spark.sql.shuffle.partitions to control the number of tasks during operations like join, reduceByKey, etc. Too many tasks can cause a lot of overhead, but too few tasks might not fully utilize your cluster.</p>
</li>
<li>
<p>Dynamic Allocation: If your cluster manager supports it, use dynamic resource allocation, which allows Spark to dynamically adjust the resources your application occupies based on the workload. This means that if your application has stages that require lots of resources, they can be allocated dynamically.</p>
<div class="codehilite"><pre><span></span><code>spark.dynamicAllocation.enabled true 
spark.dynamicAllocation.initialExecutors 2 
spark.dynamicAllocation.minExecutors 1 
spark.dynamicAllocation.maxExecutors 20
spark.dynamicAllocation.schedulerBacklogTimeout 1m 
spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 2m 
spark.dynamicAllocation.executorIdleTimeout 2min
spark.dynamicAllocation.enabled is set to true to enable dynamic allocation.
spark.dynamicAllocation.initialExecutors is set to 2 to specify that initially, two executors will be allocated.
spark.dynamicAllocation.minExecutors and spark.dynamicAllocation.maxExecutors control the minimum and maximum number of executors, respectively.
spark.dynamicAllocation.schedulerBacklogTimeout and spark.dynamicAllocation.sustainedSchedulerBacklogTimeout control how long a backlog of tasks Spark will tolerate before adding more executors.
spark.dynamicAllocation.executorIdleTimeout controls how long an executor can be idle before Spark removes it.
</code></pre></div>

</li>
</ol>
<blockquote>
<p>--- <strong>Resource Configuration Optimization</strong></p>
</blockquote>
<ol>
<li>
<p>Tune Garbage Collection: Spark uses the JVM, so the garbage collector can significantly affect performance. You can use spark.executor.extraJavaOptions to pass options to the JVM to tune the garbage collection.</p>
</li>
<li>
<p>Use Appropriate Data Structures: Parquet and Avro are both columnar data formats that are great for analytical queries and schema evolution. If your data processing patterns match these, consider using these formats.</p>
</li>
</ol>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../../airflow/airflow/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Airflow">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Airflow
              </div>
            </div>
          </a>
        
        
          
          <a href="../../sql/Mongo/" class="md-footer__link md-footer__link--next" aria-label="Next: Mongo">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Mongo
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2026 Manishkumar Chetpalli

    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/manishkumarchetpalli/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/manish-chet" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["content.code.annotation", "content.code.copy", "content.tooltips", "content.tabs.link", "navigation.tabs", "navigation.tabs.sticky", "navigation.path", "navigation.top", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.instant.preview", "navigation.tracking", "navigation.path", "search.highlight", "search.suggest", "search.share", "navigation.footer", "content.action.feedback", "toc.follow", "toc.extend"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
    
  </body>
</html>