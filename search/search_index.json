{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome","text":"<p>Hello! I'm Manish. This site is my personal hub for sharing projects, insights, and lessons learned from my journey as a Big Data Engineer.</p> <p>Whether you're a beginner or a seasoned professional, there's something to learn from each project.</p> <p>Quote</p> <p>Once a new technology rolls over you, if you\u2019re not part of the steamroller, you\u2019re part of the road - Stewart Brand</p> <p>Feel free to explore my website to discover more about my work and interests.</p>"},{"location":"about/","title":"Hey there! I'm Manish Chetpalli","text":"<p>Welcome to my GitHub space \u2014 a curated collection of my work, ideas, and explorations in Big Data, Cloud Engineering, and open-source development.</p> <p>I\u2019m a Big Data Engineer with hands-on experience in Hadoop, Spark, Kafka, Airflow, Python, and distributed systems. I love building scalable data pipelines and solving real-world problems with elegant tech solutions.</p> <p>I'm always open to:</p> <ul> <li>Talking tech</li> <li>Sharing ideas or insights</li> <li>Collaborating on open-source or data engineering projects</li> </ul> <p>Feel free to reach out \u2014 let's build something amazing together!</p> <ul> <li>LinkedIn</li> <li>GitHub</li> <li>Email</li> </ul> <p>\u2728 \u201cBuild systems that work. Then build systems that last.\u201d  </p>"},{"location":"DataEngineering/DE/","title":"Overview","text":""},{"location":"DataEngineering/DE/#data-engineering-defined","title":"Data Engineering Defined","text":"<p>Despite its popularity, there is much confusion about data engineering. It has existed in some form since companies started working with data, coming into sharp focus with the rise of data science in the 2010s.</p> <p>One definition states that data engineering is a set of operations aimed at creating interfaces and mechanisms for the flow and access of information. Data engineers maintain data, ensuring it remains available and usable, and they set up and operate an organization's data infrastructure for analysis by data analysts and scientists.</p> <p>Data engineering is the development, implementation, and maintenance of systems and processes that take in raw data and produce high quality consistent information that supports downstream use cases, such as analysis and machine learning. Data engineering is the intersection of security, data management, DataOps, data architecture, orchestration, and software engineering.</p> <p>Historically, there were two types of data engineering: SQL-focused (relational databases, SQL processing) and Big Data-focused (Hadoop, Cassandra, Spark, programming languages like Java, Scala, Python).</p> <p>Note</p> <p>A data engineer gets data, stores it, and prepares it for consumption by data scientists, analysts.</p> <p>\u2014 Fundamentals of Data Engineering (O\u2019Reilly)</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DE/#the-data-engineering-lifecycle","title":"The Data Engineering Lifecycle","text":"<p>The five stages are: Generation, Storage, Ingestion, Transformation, and Serving.</p> <p></p> <p>Storage underpins other stages as it occurs throughout.</p> <p>Undercurrents are critical ideas that cut across the entire lifecycle: security, data management, DataOps, data architecture, orchestration, and software engineering.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DE/#evolution-of-the-field","title":"Evolution of the Field","text":"<p>Big Data Era (2000s and 2010s): Coincided with data explosion and cheap commodity hardware.</p> <p>Big data is defined as \"extremely large data sets that may be analyzed computationally to reveal patterns, trends, and associations\". It's also characterized by the three Vs: velocity, variety, and volume.</p> <p>Google's papers on Google File System (2003) and MapReduce (2004) were a \"big bang\" for data technologies.This inspired Apache Hadoop (2006) at Yahoo, leading to the birth of the big data engineer.Amazon Web Services (AWS) emerged around the same time, offering elastic computing (EC2), scalable storage (S3), and NoSQL databases (DynamoDB), creating a pay-as-you-go resource marketplace.</p> <p>The era saw the rise of tools like Hadoop, Apache Pig, Apache Hive, Apache Storm, Apache Spark, and a shift from GUI-based tools to code-first engineering.</p> <p>The transition from batch computing to event streaming ushered in \"real-time\" big data.</p> <p>2020s: Engineering for the Data Lifecycle: The role is rapidly evolving towards decentralized, modularized, managed, and highly abstracted tools.</p> <p>Data engineers are becoming data lifecycle engineers, focusing on higher-value aspects like security, data management, DataOps, data architecture, orchestration, and general data lifecycle management, rather than low-level framework details.There's a shift towards managing and governing data, making it easier to use and discover, and improving its quality.</p> <p>Data engineers are now concerned with privacy, anonymization, data garbage collection, and compliance with regulations like CCPA and GDPR.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DE/#relationship-with-data-science","title":"Relationship with Data Science","text":"<p>Data science often struggled with basic data problems (collection, cleansing, access, transformation, infrastructure) that data engineering aims to solve.</p> <p>Data scientists aren't typically trained for production-grade data systems. Data engineers build a solid foundation for data scientists to succeed, allowing them to focus on analytics, experimentation, and ML.</p> <p>Data engineering is of equal importance and visibility to data science, playing a vital role in its production success. The authors themselves moved from data science to data engineering due to this fundamental need.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DE/#skills-and-activities","title":"Skills and Activities","text":"<p> The skillset encompasses the undercurrents: security, data management, DataOps, data architecture, and software engineering. It requires understanding how to evaluate data tools and their fit across the lifecycle, how data is produced, and how it's consumed.</p> <p>Data engineers balance cost, agility, scalability, simplicity, reuse, and interoperability. Historically, data engineers managed monolithic technologies; now, the focus is on high-level abstractions or writing pipelines as code within orchestration frameworks.</p> <p>Business Responsibilities: Communicate with technical and non-technical people, scope and gather requirements, understand business impact, and continuously learn.</p> <p>Technical Responsibilities: Build architectures optimizing performance and cost, understand the data engineering lifecycle stages (generation, storage, ingestion, transformation, serving) and undercurrents, and possess production-grade software engineering chops.</p> <p>SQL is a powerful tool for complex analytics and data transformation problems, essential for high productivity. Data engineers should also develop expertise in composing SQL with frameworks like Spark and Flink, or using orchestration.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DE/#types-of-data-engineers","title":"Types of Data Engineers","text":"<p>Drawing from the \"type A\" (analysis) and \"type B\" (building) data scientists analogy:</p> <p>Type A Data Engineers: Focus on understanding and deriving insight from data.</p> <p>Type B Data Engineers: Share similar backgrounds but possess strong programming skills to build systems that make data science work in production.</p> <p>Internal-Facing vs. External-Facing Data Engineers: Serve various end users, with primary responsibilities being external, internal, or a blend.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DE/#data-engineers-work-with-key-technical-stakeholders","title":"Data Engineers Work With (Key Technical Stakeholders)","text":"<p>Data engineers are a hub between data producers (software engineers, data architects, DevOps/SREs) and data consumers (data analysts, data scientists, ML engineers).</p> <p></p> <ul> <li> <p>Upstream Stakeholders:</p> <p>Data Architects: Design application data layers that serve as source systems and interact across other lifecycle stages. Data engineers should understand architecture best practices.</p> <p>Software Engineers: Build and maintain source systems; data engineers need to understand these systems and collaborate to make data production-ready.</p> <p>DevOps/Site-Reliability Engineers (SREs): Ensure systems are reliable and available; data engineers collaborate on deployment, monitoring, and incident response.</p> </li> <li> <p>Downstream Stakeholders:</p> <p>Data Analysts: Consume data for reports, dashboards, and ad hoc analysis. Data engineers ensure data quality and provide necessary datasets.</p> <p>Data Scientists: Build models; data engineers provide the data automation and scale for data science to be efficient and production-ready.</p> <p>ML Engineers: Similar to data scientists, they build and deploy ML models, relying on data engineers for robust data pipelines.</p> </li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DE/#data-maturity-models","title":"Data Maturity Models","text":"<p>Stage 1: Starting with data</p> <p>Early stages, often small teams, data engineer acts as a generalist. Focus on defining the right data architecture, identifying and auditing data, and building a solid data foundation. Avoid jumping to ML without a foundation. Get quick wins (though they create technical debt), talk to people (avoid silos), and avoid undifferentiated heavy lifting.</p> <p>Stage 2: Scaling with data</p> <p>Company has some data, looking to expand usage. Focus on automating data flows, building systems for ML, and continuing to avoid undifferentiated heavy lifting. The main bottleneck is often the data engineering team, so focus on simple deployment/management. Shift to pragmatic leadership.</p> <p>Stage 3: Leading with data</p> <p>Data is a competitive advantage. Focus on automation for seamless data introduction, building custom tools for competitive advantage, \"enterprisey\" aspects like data management and DataOps, and deploying tools for data dissemination (catalogs, lineage). Efficient collaboration is key.</p> <p>Note</p> <p>\u201cData engineering is not about tools\u2014it's about building systems that let others derive value from data.\u201d</p>"},{"location":"DataEngineering/DEcycle/","title":"DE Lifecycle","text":"<p>The  data  engineering  lifecycle  comprises  stages  that  turn  raw  data  ingredients  into  a   useful end product, ready for consumption by analysts, data scientists, ML engineers,  and others.</p> <p></p> <p>The data engineering lifecycle is divided into five stages:</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEcycle/#generation","title":"Generation","text":"<p>The origin of raw data in the data engineering lifecycle.  </p> <p>Examples: Transactional databases (RDBMS), Application message queues (e.g., Kafka), IoT devices or sensor swarms, Web/mobile apps, spreadsheets.</p> <p>Tip</p> <p>Key engineering considerations:</p> <ul> <li>What type is it? (App DB, IoT, etc.)</li> <li>How is data stored? (Persistent or temporary)</li> <li>How fast is data generated? (Events/sec or GB/hr)</li> <li>Are there data quality issues? (Nulls, bad formats)</li> <li>Can duplicates or late-arriving data occur?</li> <li>What\u2019s the schema structure? Does it evolve?</li> <li>How often is data pulled? (Real-time, hourly?)</li> <li>What\u2019s the data update method? (Snapshot or CDC)</li> <li>Who owns/provides the data?</li> <li>Will reading from it affect app performance?</li> <li>Are there upstream dependencies?  </li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEcycle/#storage","title":"Storage","text":"<p>The place where data is kept during any stage of the lifecycle. Not just passive storage\u2014modern systems can process and query data too. Storage impacts ingestion, transformation, and serving stages.</p> <p></p> <p>Warning</p> <p>Why It\u2019s Complex</p> <ul> <li>Multiple storage types are often used together (e.g., cloud storage + data warehouses).</li> <li>Storage overlaps with other stages of the lifecycle (Kafka, S3, Snowflake).</li> <li>Some systems support storage + compute together (e.g., cloud data warehouses).</li> </ul> <p>Tip</p> <p>Key Considerations When Choosing Storage</p> <ul> <li>Performance: Are read/write speeds sufficient for your architecture? Will it bottleneck downstream jobs?</li> <li>Fit for Purpose: Are you misusing the storage (e.g., doing random writes in object storage)?</li> <li>Scalability: Can it handle future data volume, query load, and write throughput?</li> <li>SLA Support: Can it deliver data within required time frames for users and systems?</li> <li>Metadata &amp; Governance: Does it support schema evolution tracking, lineage, and data discovery?</li> <li>Capabilities: Pure storage (e.g., S3) vs. compute + storage (e.g., BigQuery, Snowflake)?</li> <li>Schema flexibility: Schema-less (S3), Flexible (Cassandra), Strict (Redshift, RDBMS)</li> <li>Compliance: Can it support data sovereignty and regulatory needs (e.g., GDPR, HIPAA)?</li> </ul> Type Description Storage Tip Hot Frequently accessed (e.g., per second) Store in fast-access systems (RAM, SSD) Warm Accessed occasionally (weekly/monthly) Moderate-speed, cost-efficient storage Cold Rarely accessed, kept for compliance Cheap storage, archival (e.g., Glacier) <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEcycle/#ingestion","title":"Ingestion","text":"<p>Ingestion is the process of gathering data from source systems. It\u2019s often a major bottleneck due to:</p> <ul> <li>Unreliable source systems</li> <li>Ingestion service failures</li> <li>Poor data quality or availability</li> </ul> <p></p> <p>Tip</p> <p>Key Considerations When Ingesting data</p> <ul> <li>Can storage systems handle real-time load?</li> <li>Do you need real-time or will micro-batch (e.g., every minute) suffice?</li> <li>Does real-time ingestion add value (e.g., live decisions)?</li> <li>Will streaming increase cost/complexity over batch?</li> <li>Are streaming tools managed or self-hosted?</li> <li>Does the ML model benefit from online predictions?</li> </ul> <p></p> Type Description Use Cases Batch Collects data in intervals or chunks Analytics, ML training, reports Streaming Processes data in real-time (low latency) Real-time dashboards, alerts <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEcycle/#transformation","title":"Transformation","text":"<p>The process of changing raw data into useful formats for analysis, reporting, or machine learning. This is where data starts creating business value.  </p> <p> Common early transformations: type casting, format standardization, and error removal. Later stages include: aggregations, normalization, featurization (for ML), and schema changes.</p> Type Description Example Use Cases Batch Processed in chunks Reporting, model training Streaming (in-flight) Transformed as it flows through a stream Real-time analytics, alerts <p></p> <p>Note</p> <p>Where Does Transformation Occur?</p> <ul> <li>In source systems (e.g., app adds timestamp before ingestion)</li> <li>During ingestion (e.g., enrich data in-stream)</li> <li>Post-ingestion in data warehouses/lakehouses</li> </ul> <p>Business Logic &amp; Data Modeling:</p> <ul> <li>Business logic makes data actionable and understandable.</li> <li>Example: Convert raw transactions to revenue using accounting logic.</li> <li>Consistency in applying business rules is critical.</li> </ul> <p>Featurization for ML:</p> <ul> <li>Extracting and engineering important features for model training.</li> <li>Combines domain knowledge and data science.</li> <li>Can be automated by data engineers in pipelines once defined.</li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEcycle/#serving-data","title":"Serving data","text":"<p>Making transformed data available for practical use: analytics, dashboards, machine learning, or operational systems. Data has no value if it isn\u2019t used or consumed.</p> <p>Use Cases for Data Serving</p> <p>Analytics</p> <p></p> <p>Business Intelligence (BI): Reporting on past and current trends using business logic.</p> <p>Logic-on-read is becoming more common (business rules applied during querying, not transformation).</p> <p>Self-Service Analytics: Empowers non-technical users to explore data. Requires high data quality and organizational maturity.</p> <p>Operational Analytics: Real-time dashboards (e.g., app health, live inventory). Consumed immediately to trigger action.</p> <p>Embedded Analytics: Delivered to customers via apps (e.g., SaaS dashboards). Requires robust access control and multitenancy to prevent data leaks.</p> <p></p> <p>Machine Learning</p> <p></p> <p>Feature Stores: Systems that store and manage features for ML models.  </p> <p>Data Engineers may Maintain Spark clusters, orchestrate pipelines, and monitor metadata. Collaborate closely with ML and analytics engineers.</p> <p>ML maturity depends on solid data foundations\u2014master analytics first.</p> <p>Reverse ETL</p> <p></p> <p>Sends data back to source systems (e.g., CRMs, marketing platforms). Example: Push customer segments from a warehouse to Google Ads.</p> <p>Challenges: Must maintain lineage, business logic, and security. May be replaced by event-driven architectures, but remains practical today.</p>"},{"location":"DataEngineering/DEundercurrent/","title":"DE UnderCurrents","text":"<p>Data engineering is evolving beyond just tools and technology. Undercurrents are foundational practices that support the entire data engineering lifecycle. </p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEundercurrent/#security-a-critical-undercurrent","title":"Security: A Critical Undercurrent","text":"<p>Only grant users/systems the minimum access necessary to perform their function. Avoid giving admin or superuser access unless strictly required. Prevents accidental damage and maintains a security-first mindset. Giving all users full admin access. Running commands with root privileges unnecessarily. Querying data with superuser roles when not needed.</p> <p>People and organizational behavior are the biggest vulnerabilities. Security breaches often stem from: Neglecting security protocols, Falling for phishing, Irresponsible behavior.</p> <p>Timely and Contextual Data Access: Data should be accessible: Only to the right people/systems, Only for the necessary time period. Data must be protected in transit and at rest using: Encryption, Tokenization, Data masking, Obfuscation, Access controls.</p> <p>Security is Continuous: Security is embedded across all phases of the data engineering lifecycle. Must be revisited and reinforced at every stage of data handling.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEundercurrent/#data-management-importance-of-metadata-in-data-engineering","title":"Data Management: Importance of Metadata in Data Engineering","text":"<p>Metadata is not just technical\u2014it's social. Airbnb emphasized human-centric metadata via tools like Dataportal to capture data ownership and usability context.</p> <p></p> <p>Types of Metadata</p> <ul> <li>Business Metadata: Describes how data is used in business (definitions, rules, ownership).</li> <li>Technical Metadata: Covers schema, lineage, pipeline workflows, etc.</li> <li>Operational Metadata: Includes logs, stats, job run data\u2014useful for debugging and monitoring.</li> <li>Reference Metadata: Lookup values like codes and classifications (e.g., ISO country codes).</li> </ul> <p></p> <p>Data Accountability: Assign a responsible person (not necessarily a data engineer) for specific data artifacts (tables, fields, logs). Supports data quality by enabling someone to coordinate governance.</p> <p>Data Quality Characteristics:</p> <ul> <li>Accuracy: Data should be correct, deduplicated, and valid.</li> <li>Completeness: All required fields are filled with valid data. </li> <li>Timeliness: Data should be available at the right time for its intended use.</li> </ul> <p>Real-World Complexity: Accuracy and completeness are challenged by factors like bots or offline data submissions (e.g., ad views in video apps). Data engineers must define and enforce acceptable latency standards.</p> <p>Master Data Management (MDM): Centralized \"golden records\" for business entities (customers, products, etc.). Combines business policy with tech (like APIs) to enforce consistency across systems and partners.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEundercurrent/#dataops","title":"DataOps","text":"<p>DataOps aims to improve the delivery, quality, and reliability of data products, just as DevOps does for software products. Data products, unlike software products, revolve around business logic, metrics, and decision-making processes. DataOps applies Agile principles, DevOps practices, and statistical process control (SPC) to the data engineering lifecycle to reduce time to value, improve data quality, and facilitate collaboration.</p> <p></p> <p>Key Aspects of DataOps:</p> <ul> <li>Cultural Practices: Data engineers need to foster a culture of communication, collaboration with the business, breaking silos, and continuous learning from both successes and failures.</li> <li>Automation: Automates processes like environment, code, and data version control, CI/CD, and configuration management. This ensures data workflows are reliable and consistent.</li> <li>Monitoring &amp; Observability: Ensures that data systems and transformations are constantly monitored, with alerting and logging in place to avoid errors or delays in reporting.</li> <li>Incident Response: Focuses on identifying and resolving issues rapidly, leveraging automation, monitoring, and observability tools, with a proactive, blameless communication culture.</li> </ul> <p>DataOps Lifecycle in an Organization:</p> <ul> <li>Low Maturity: A company may rely on cron jobs to schedule data transformation, which can lead to failures when jobs fail or take too long. Engineers often aren't aware of these failures until stakeholders report issues.</li> <li>Medium Maturity: Adoption of orchestration frameworks like Airflow helps automate dependencies and scheduling. However, challenges like broken DAGs may still occur, which necessitate automated DAG deployment and pre-deployment testing to prevent issues.</li> <li>High Maturity: Engineers continuously enhance automation, possibly introducing next-gen orchestration frameworks or frameworks for automatic DAG generation based on data lineage.</li> </ul> <p>Core Pillars of DataOps:</p> <p></p> <ul> <li>Automation: Ensures consistency and reliability in data product delivery by automating various aspects of the data lifecycle. This includes CI/CD and automating data quality checks, metadata integrity, and model drift.</li> <li>Observability and Monitoring: Critical to catch problems early, prevent data disasters, and keep stakeholders informed of system performance and data quality.</li> <li>Incident Response: Ensures fast and effective responses to failures or issues, using both proactive identification and retrospective resolution, supported by clear communication channels.</li> </ul> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEundercurrent/#data-architecture","title":"Data Architecture","text":"<p>Data engineers need to start by understanding the business requirements and use cases. These needs will inform the design and decisions about how to capture, store, transform, and serve data. The design of data systems must strike a balance between simplicity, cost, and operational efficiency. </p> <p>Data engineers must understand the trade-offs involved in choosing tools and technologies, whether for data ingestion, storage, transformation, or serving data. While data engineers and data architects often have distinct roles, collaboration is key. </p> <p>Data engineers should be able to implement the designs created by data architects and provide valuable feedback on those designs. With the rapid evolution of tools, technologies, and practices in the data space, data engineers must remain agile and continuously update their knowledge to maintain a relevant and effective data architecture.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEundercurrent/#orchestration","title":"Orchestration","text":"<p>Orchestration is the process of managing and coordinating the execution of multiple jobs in a way that optimizes efficiency and speed. This is typically achieved through an orchestration engine like Apache Airflow, which monitors job dependencies, schedules tasks, and ensures tasks run in the correct order.</p> <p>Not Just a Scheduler: Unlike simple schedulers (like cron), which only manage time-based scheduling, orchestration engines like Airflow manage complex task dependencies using Directed Acyclic Graphs (DAGs). DAGs define the order in which tasks should execute and can be scheduled for regular intervals.</p> <p>Key Features of Orchestration Systems:</p> <ul> <li>High Availability: Orchestration systems should stay online continuously, ensuring they can monitor and trigger jobs without manual intervention.</li> <li>Job Monitoring &amp; Alerts: They monitor job execution and send alerts when tasks don\u2019t complete as expected (e.g., jobs not finishing by the expected time).</li> <li>Job History &amp; Visualization: Orchestration systems often include visualization tools to track job progress, along with maintaining historical data to help with debugging and performance monitoring.</li> <li>Backfilling: If new tasks or DAGs are added, orchestration systems can backfill these tasks, ensuring that missing or delayed data can be processed.</li> <li>Complex Dependencies: Orchestration engines allow setting dependencies over time (e.g., ensuring a monthly reporting job doesn\u2019t run until all necessary ETL tasks are completed for that month).</li> </ul> <p>Evolution of Orchestration Tools:</p> <ul> <li>Early Tools: Traditional tools like Apache Oozie were primarily used in large enterprise environments, particularly with Hadoop. However, they were costly and not easily adaptable to different infrastructures.</li> <li>Airflow's Rise: Apache Airflow, introduced by Airbnb in 2014, revolutionized orchestration by offering an open-source, Python-based solution that is highly extensible and cloud-friendly. Airflow became widely adopted due to its flexibility and ability to manage complex workflows in modern, multi-cloud environments.</li> <li>Newer Tools: New orchestration tools like Prefect, Dagster, Argo, and Metaflow are emerging, focusing on improving portability, testability, and performance. Prefect and Dagster, in particular, aim to address issues related to portability and transitioning workflows from local development to production.</li> </ul> <p>Batch vs. Streaming Orchestration:</p> <ul> <li>Batch Orchestration: Most orchestration engines focus on batch processing, where tasks are executed based on scheduled time intervals or job dependencies.</li> <li>Streaming Orchestration: For real-time or continuous data processing, orchestration becomes more complex. While streaming DAGs are difficult to implement and maintain, next-generation streaming platforms like Pulsar are attempting to simplify the process.</li> </ul> <p>Orchestration plays a pivotal role in data engineering by enabling the coordination of data workflows, ensuring that tasks are executed in the correct order and monitoring their progress. Tools like Apache Airflow have made orchestration accessible to a wider range of organizations, and newer solutions continue to improve the scalability and portability of orchestration tasks across various environments.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"DataEngineering/DEundercurrent/#software-engineering","title":"Software Engineering","text":"<p>Software engineering plays a central role in data engineering, and its importance has only grown as the field has evolved. While modern frameworks like Spark, SQL-based cloud data warehouses, and dataframes have abstracted much of the complexity, core software engineering skills are still crucial.</p> <p></p> <p>Key Areas of Software Engineering for Data Engineers:</p> <p>Core Data Processing Code:</p> <ul> <li>Data engineers still need to write and optimize core data processing code, which is a significant part of the data lifecycle. Whether using tools like Spark, SQL, or Beam, proficiency in these frameworks is necessary.</li> <li>Writing efficient data transformations and processing pipelines requires a solid understanding of software engineering principles, such as modularity, maintainability, and performance optimization.</li> </ul> <p>Code Testing: - Data engineers need to apply proper testing methodologies to ensure that their code is correct and reliable. This includes unit testing, regression testing, integration testing, end-to-end testing, and smoke testing. - Testing is essential in all stages of the data pipeline to guarantee data quality and system reliability.</p> <p>Development of Open Source Frameworks:</p> <ul> <li>Many data engineers are involved in creating and contributing to open-source frameworks. These tools help address specific challenges within the data engineering lifecycle, whether related to data processing, orchestration, or monitoring.</li> <li>While there is a proliferation of open-source tools like Airflow, Prefect, Dagster, and Metaflow, data engineers must evaluate the best fit for their organization's needs and consider factors like cost, scalability, and ease of use.</li> </ul> <p>Streaming Data Processing:</p> <ul> <li>Streaming data processing is more complex than batch processing. Engineers face challenges with handling real-time joins, applying windowing techniques, and ensuring high throughput and low latency.</li> <li>Frameworks like Spark, Beam, Flink, and Pulsar are used for processing streaming data, and data engineers need to be familiar with them to effectively design real-time analytics and reporting systems.</li> </ul> <p>Infrastructure as Code (IaC):</p> <ul> <li>As data systems move to the cloud, Infrastructure as Code (IaC) becomes increasingly important. IaC allows engineers to automate the deployment and management of infrastructure using code, improving repeatability and version control.</li> <li>Cloud services like AWS, GCP, and Azure provide IaC frameworks, such as Terraform, CloudFormation, and Kubernetes, which help automate data system management.</li> </ul> <p>Pipelines as Code:</p> <ul> <li>Modern orchestration systems, such as Apache Airflow, allow engineers to define data pipelines as code. This approach provides flexibility, version control, and scalability in managing complex workflows across various stages of the data lifecycle.</li> <li>Data engineers need to be proficient in writing Python code to define tasks and dependencies, which the orchestration engine interprets and runs.</li> </ul> <p>General-Purpose Problem Solving:</p> <ul> <li>Despite using high-level tools, data engineers often encounter situations where custom code is needed. This could involve writing connectors for unsupported data sources or integrating new tools into existing pipelines.</li> <li>Proficiency in software engineering allows data engineers to solve these problems by understanding APIs, handling exceptions, and ensuring smooth integration of new components into the system.</li> </ul> <p>In essence, while the tools and abstractions for data engineering have advanced significantly, software engineering remains foundational. Data engineers must not only be skilled in using these tools but also in developing solutions to the unique challenges they encounter in the data engineering lifecycle.</p>"},{"location":"DataEngineering/DesigningGDA/","title":"DE Architecture & Design","text":"<p>Cloud data architecture, by design, handles the ingestion, transformation, and analysis of data that is too large or complex for traditional data architectures.</p> <p></p>"},{"location":"DataEngineering/DesigningGDA/#patterns-of-good-cloud-data-architecture","title":"Patterns of Good Cloud Data Architecture","text":"<p>Let's learn about 5 principles for cloud-native data architecture that are useful for designing and operating reliable, cost-effective and efficient systems in the cloud.</p> <p>Cloud offers incredible capabilities, but without deliberate design decisions, the architecture can become fragile, expensive, and challenging to maintain. Most cloud environments have not just one application but several technologies that need to be integrated.</p> <p>The overarching goal of cloud architecture is to connect the dots to provide customers with a valuable online platform.</p>"},{"location":"DataEngineering/DesigningGDA/#5-cloud-native-architecture-principles","title":"5 cloud-native architecture principles","text":"<p>This is essential for creating a good design:</p> <ul> <li> <p>Reliable: The system should continue to work correctly regardless of system faults or human errors.</p> </li> <li> <p>Efficient: The system must use computing resources efficiently to meet system requirements and maintain efficiency as the business grows.</p> </li> <li> <p>Maintainable: The world is fluid. Good data architecture should be able to respond to changes within the business and new technologies to unlock more possibilities in the future.</p> </li> <li> <p>Cost-optimized: The system should leverage various payment options to achieve cost efficiency.</p> </li> <li> <p>Secure: The system should be hardened to avoid insider attacks.</p> </li> </ul>"},{"location":"DataEngineering/DesigningGDA/#principle-1-have-an-automation-mindset","title":"Principle 1: Have an automation mindset","text":"<p>Automation has always been good practice for software systems. In traditional environments, automation refers to building, testing, and deploying software through continuous integration/continuous delivery (CI/CD) pipelines.</p> <p>A good cloud architecture takes a step ahead by automating the infrastructure as well as the internal components.</p> <p>The five common areas for automation are shown below:</p> <ul> <li>Software</li> </ul> <p>Software has been the most common area for automation regardless of the environment. Automation happens throughout the software's life cycle, from coding and deployment to maintenance and updates.</p> <ul> <li>Infrastructure</li> </ul> <p>In the cloud, we can apply the same engineering principles we use for applications to the entire environment. This implies the ability to create and manage infrastructure through code.</p> <p>Note</p> <p>Infrastructure as Code (IaC) is a process that enables us to manage infrastructure provisioning and configuration in the same way as we handle application code.</p> <p>Example</p> <p>we first provision a VM in the dev environment and then decide to create the same one in the production environment.</p> <p>Provisioning a server manually through a graphic interface often leads to mistakes.</p> <p>IaC means storing infrastructure configurations in a version-control environment and benefiting from CI/CD pipelines to ensure consistency across environments.</p> <ul> <li>Autoscaling</li> </ul> <p>The world is fluctuating, and a reliable system must handle the fluctuation in the load accordingly. Autoscaling helps the applications handle traffic increases and reduce costs when the demand is low without disrupting business operations.</p> <ul> <li>Recovery</li> </ul> <p>According to Google SRE philosophy, building a system with 100% availability is almost impossible and unnecessary. The team should, instead, embrace the risk and develop mechanisms to allow systems to recover from the failure quickly.</p> <p>Tip</p> <p>Automatic recovery works by monitoring workloads for key indicators and triggering operations when specific thresholds are reached.</p> <p>Example</p> <p>In the event of full memory or disk, the cloud will automatically request more resources and scale the system vertically, instead of just throwing an error and disrupting the system.</p> <ul> <li>Backup</li> </ul> <p>A backup strategy guarantees the business won't get interrupted during system failure, outage, data corruption, or natural disaster. Cloud backup operates by copying and storing data in a different physical location.</p>"},{"location":"DataEngineering/DesigningGDA/#principle-2-outsource-with-caution","title":"Principle 2: Outsource with caution","text":"<p>Most cloud providers offer different abstract levels of services, namely IaaS, PaaS, and SaaS. Their ever-growing features help us offload day-to-day management to the vendors. However, some organizations are concerned with giving providers access to their internal data for security reasons.</p> <p>Warning</p> <p>The decision of whether or not to use managed services comes down to operational overhead and security.</p> <p>Tip</p> <p>The best practice is to find a cloud provider with a high reputation, express our concerns, and find a solution together. Even if the provider can't solve the problem immediately, the discussion might open the door to future possibilities.</p>"},{"location":"DataEngineering/DesigningGDA/#principle-3-keep-an-eye-on-the-cost","title":"Principle 3: Keep an eye on the cost","text":"<p>Cost control isn\u2019t a prominent concern in traditional architecture because the assets and costs are pretty much fixed. However, in the cloud, the cost can be highly dynamic, and the team might surprisingly end up with a high bill.</p> <p>Warning</p> <p>Implementing cloud financial management is vital, and the organization must allocate time and resources to build knowledge around it and share the best practices with the teams.</p> <p>Fortunately, most cloud providers offer a centralized cost-monitoring tool that helps the team analyze and optimize the costs.</p> <p>A few quick wins on saving the cost:</p> <ul> <li> <p>Only pay what you need. Turn off stale servers and delete stale data.</p> </li> <li> <p>Enable table expiration on temporary data so they won't cost money after the expiration date.</p> </li> <li> <p>Maximize utilization. Implement efficient design to ensure high utilization of the underlying hardware.</p> </li> <li> <p>Query optimization. Learn different query optimization strategies such as incremental load, partitioning, and clustering.</p> </li> </ul>"},{"location":"DataEngineering/DesigningGDA/#principle-4-embrace-changes","title":"Principle 4: Embrace changes","text":"<p>The world is constantly evolving, and that's true for cloud architecture. As the business changes, the landscape of systems also needs to change. Good architecture doesn't stay in the existing state forever. Instead, they are very agile and can respond to business changes and adapt to them with the least effort.</p> <p>Example changes in cloud architecture, as below</p> <ol> <li>Migrate Database</li> <li>Switch Vendor</li> <li>Migrate from batch to realtime stream processing</li> <li>Upgrade Services</li> <li>Adapt to high volume or low volume.</li> </ol>"},{"location":"DataEngineering/DesigningGDA/#principle-5-do-not-neglect-security","title":"Principle 5: Do not neglect security","text":"<p>Last but not least, implementing a strong identity foundation becomes a huge responsibility of the data team.</p> <p>Tip</p> <p>Traditional architectures place a lot of faith in perimeter security, crudely a hardened network perimeter with \"trusted\" things inside and \"untrusted\" things outside. Unfortunately, this approach has always been vulnerable to insider attackers, as well as external threats such as spear phishing.</p> <p>In the cloud environment, all assets are connected to the outside world to some degree. Zero Trust architecture has been created to eliminate the risk from both outside and inside. Zero Trust is a strategy that secures an organization by eliminating implicit trust and validating every stage of digital interaction.</p> <p>Another important concept in terms of security is the shared responsibility model. It divides security into the security of the cloud and security in the cloud. Most cloud providers are responsible for the security of the cloud, and it's the user's responsibility to design a custom security model for their applications. Users are responsible for managing sensitive data, internal access to data and services, and ensuring GDPR compliance.</p>"},{"location":"DataEngineering/DesigningGDA/#lambda-architecture","title":"LAMBDA Architecture","text":"<p>In the \u201cold days\u201d (the early to mid-2010s), the popularity of working with streaming data exploded with the emergence of Kafka as a highly scalable message queue and frameworks such as Apache Storm and Samza for streaming/real-time analyt\u2010 ics. These technologies allowed companies to perform new types of analytics and modeling on large amounts of data, user aggregation and ranking, and product recommendations. Data engineers needed to figure out how to reconcile batch and streaming data into a single architecture. The Lambda architecture was one of the early popular responses to this problem. </p> <p>In a Lambda architecture, you have systems operating independently of each other\u2014batch, streaming, and serving. The source system is ideally immutable and append-only, sending data to two destinations for processing: stream and batch. In-stream processing intends to serve the data with the lowest possible latency in a \u201cspeed\u201d layer, usually a NoSQL database. In the batch layer, data is processed and transformed in a system such as a data warehouse, creating precomputed and aggre\u2010 gated views of the data. The serving layer provides a combined view by aggregating query results from the two layers</p> <p>Lambda architecture has its share of challenges and criticisms. Managing multiple systems with different codebases is as difficult as it sounds, creating error-prone systems with code and data that are extremely difficult to reconcile.</p> <p>How it works: The system will dispatch all incoming data to batch and streaming layers. The batch layer will maintain an append-only primary dataset and precompute the batch views The streaming layer will only handle the most recent data to achieve low latency. Both batch and stream views are served in the serving layer to be queried.The result of merging batch and real-time results can answer any incoming query.</p> <p>Challenges: Complexity and cost of running 2 parallel systems instead of 1. This approach often uses systems with different software ecosystems, making it challenging to replicate the business logic across the systems. It's also quite difficult to reconcile the outputs of 2 pipelines at the end.</p>"},{"location":"DataEngineering/DesigningGDA/#kappa-architecture","title":"KAPPA Architecture","text":"<p>As a response to the shortcomings of Lambda architecture, Jay Kreps proposed an alternative called Kappa architecture. The central thesis is this: why not just use a stream-processing platform as the backbone for all data handling\u2014inges\u2010 tion, storage, and serving? This facilitates a true event-based architecture. Real-time and batch processing can be applied seamlessly to the same data by reading the live event stream directly and replaying large chunks of data for batch processing. </p> <p>Kappa architecture Though the original Kappa architecture article came out in 2014, we haven\u2019t seen it widely adopted. There may be a couple of reasons for this. First, streaming itself is still a bit of a mystery for many companies; it\u2019s easy to talk about, but harder than expected to execute. Second, Kappa architecture turns out to be complicated and expensive in practice. While some streaming systems can scale to huge data volumes, they are complex and expensive; batch storage and processing remain much more efficient and cost-effective for enormous historical datasets.</p> <p>Advantages: In Kappa architecture, a streaming processing engine continuously processes real-time data and ingests it into long-term storage. When code changes occur, developers can recompute using the raw data stored in the event logs database.</p> <p>Challenges: Streaming remains a challenge for many companies due to its complexity and most likely high cost and maintainance. Managing duplicates and preserving order, for instance, can be more challenging than batch processing. data replay is often trickier than it may seem.</p>"},{"location":"DataEngineering/DesigningGDA/#data-lake","title":"Data Lake","text":"<p>A data lake is a popular data architecture comparable, to a data warehouse. It\u2019s a storage repository that holds a large amount of data, but unlike a data warehouse where data is structured, data in a data lake is in its raw format.</p> Topic Data Lake Data Warehouse Data Format Store unstructured, semi-structured and structured data in its raw format. Store only structured data after the transformation. Schema Schema-on-read: Schema is defined after data is stored. Schema-on-write: Schema is predefined prior to when data is stored. Usecase Data exploration: Unstructured data opens more possibilities for analysis and ML algorithms, A landing place before loading data into a data warehouse. Reporting: Reporting tools and dashboards prefer highly coherent data. Data Quality Data is in its raw format without cleaning, so data quality is not ensured. Data is highly curated, resulting in higher data quality. Cost Both storage and operational costs are lower. Storing data in the data warehouse is usually more expensive and time-consuming. <p>The following graph illustrates the key components of a data lake</p> <p></p> <ul> <li> <p>Ingestion layer: The ingestion layer collects raw data and loads them into the data lake. The raw data is not modified in this layer.</p> </li> <li> <p>Processing layer: Data lake uses object storage to store data. Object storage stores data with metadata tags and a unique identifier, making searching and accessing data easier. Due to the variety and high volume of data, a data lake usually provides tools for features like data catalog, authentication, data quality, etc.</p> </li> <li> <p>Insights layer: The insights layer is for clients to query the data from the data lake. Direct usage could be feeding the reporting tools, dashboards, or a data warehouse.</p> </li> </ul>"},{"location":"DataEngineering/DesigningGDA/#data-mesh","title":"Data Mesh","text":"<p>The term data mesh was coined by Zhamak Dehghani in 2019 and created the idea of domain-oriented decentralization for analytical data. Centrally managed architectures tend to create data bottlenecks and hold back analytics agility. On the other hand, completely decentralized architectures create silos and duplicates, making management across domains very difficult.</p> <p>The data mesh architecture proposes distributed data ownership, allowing teams to own the entire life cycle of their domains and deliver quicker analyses.</p> <p>The organization's IT team is responsible for the overall infrastructure, governance, and efficiency without owning any domain-related business.</p> <p>Adopting data mesh requires some pretty cultural and organizational changes.</p> <p>Currently, no template solutions for a data mesh, so many companies are still trying to figure out if it's a good fit for their organizations.</p> <p></p> <p>Each domain team is responsible for ingesting the operational data and building analytics models.</p> <p>The domain team agrees with the rest on global policies to safely and efficiently interact with the other domains within the mesh.</p> <p>A centralized data platform team builds infrastructures and tools for domain teams to build data products and perform analysis more effectively and quickly.</p> <ul> <li> <p>Principles</p> <ol> <li> <p>Domain ownership: Each domain team takes responsibility for the entire data life cycle.</p> </li> <li> <p>Data as a product: Treat provided data as a high-quality product, like APIs to other domains.</p> </li> <li> <p>Self-serve data platform: Build an effective data platform for domain teams to build data products quickly.</p> </li> <li> <p>Federated governance: Standardize data policies to create a healthy data ecosystem for domain interoperability.</p> </li> </ol> </li> </ul>"},{"location":"DataEngineering/fileformat/","title":"File Formats","text":""},{"location":"DataEngineering/fileformat/#how-data-is-stored-physically-on-disk","title":"How data is stored physically on disk","text":"<p>Row-based File Formats (e.g., CSV, traditional databases for OLTP):</p> <p>Data for an entire row is stored contiguously on disk.</p> <p>Advantage for OLTP: When you need all the details of a specific record (e.g., a customer's entire banking transaction), a row-based format allows you to access all its columns continuously. If you need to update multiple columns for a single transaction, the data is already together.</p> <p>Disadvantage for OLAP: If you only need a few columns (e.g., \"Title\" and \"Chart\") from a large dataset, a row-based system still has to read through all the interleaved data (including \"Date\") for every row. This leads to excessive I/O operations and slower performance because the system has to \"jump\" across the disk to pick out the desired columns from different rows.</p> <p>Column-based File Formats (e.g., Parquet for OLAP):</p> <p>Data for each column is stored contiguously on disk, independent of other columns.</p> <p>Advantage for OLAP: This format shines in \"read-many\" scenarios prevalent in Big Data analytics. If you only need \"Title\" and \"Chart\" columns, the system can go directly to the contiguous \"Title\" block and \"Chart\" block, skipping the \"Date\" column entirely. This significantly reduces I/O, leading to faster query performance and lower computational cost.</p> <p>Disadvantage for OLTP: If you need to retrieve or update an entire row, the system has to jump between different column blocks to gather all the data for that single row</p>"},{"location":"DataEngineering/fileformat/#parquet","title":"Parquet","text":"<p>Columnar storage format, available to any project in the Hadoop ecosystem. It's designed to bring efficient columnar storage of data compared to row-based like CSV or TSV files.</p> <p>It is columnar in nature and designed to bring efficient columnar storage of data. Provides efficient data compression and encoding schemes with enhanced performance to handle complex data in comparison to row-based files like CSV.</p> <p>Schema evolution is handled in the file metadata allowing compatible schema evolution. It supports all data types, including nested ones, and integrates well with flat data, semi-structured data, and nested data sources.</p> <p>Parquet is considered a de-facto standard for storing data nowadays</p> <p>Data compression - by applying various encoding and compression algorithms, Parquet file provides reduced memory consumption</p> <p>Columnar storage - this is of paramount importance in analytic workloads, where fast data read operation is the key requirement. But, more on that later in the article\u2026</p> <p>Language agnostic - as already mentioned previously, developers may use different programming languages to manipulate the data in the Parquet file</p> <p>Open-source format - meaning, you are not locked with a specific vendor</p> <p>Why is this additional structure super important?</p> <p>In OLAP scenarios, we are mainly concerned with two concepts: projection and predicate(s). Projection refers to a SELECT statement in SQL language \u2013 which columns are needed by the query. </p> <p>Predicate(s) refer to the WHERE clause in SQL language \u2013 which rows satisfy criteria defined in the query. In our case, we are interested in T-Shirts only, so the engine can completely skip scanning Row group 2, where all the values in the Product column equal socks!</p> <p>This means, every Parquet file contains \u201cdata about data\u201d \u2013 information such as minimum and maximum values in the specific column within the certain row group. Furthermore, every Parquet file contains a footer, which keeps the information about the format version, schema information, column metadata, and so on.</p> <p>While Parquet is primarily columnar, it actually uses a hybrid model to combine the efficiencies of both row and column storage. This hierarchical structure helps manage very large datasets.</p> <p></p> <p>Let\u2019s stop for a moment and understand above diagram, as this is exactly the structure of the Parquet file  Columns are still stored as separate units, but Parquet introduces additional structures, called Row group.</p> <p>The structure of a Parquet file can be visualized as a tree</p> <ol> <li> <p>File: The top-level entity.  It contains metadata about the entire file, such as the number of columns, total rows, and number of row groups.</p> </li> <li> <p>Row Group: This is a logical horizontal partition of the data within the file.  Instead of storing all data for a column as one huge block, Parquet breaks the data into smaller, manageable chunks called Row Groups.  By default, a row group stores around 128 MB of data.  For example, if you have 100 million records, a row group might contain 100,000 records.  Each row group also contains metadata, including the minimum and maximum values for each column within that specific row group. This metadata is crucial for optimization.</p> </li> <li> <p>Column (Column Chunk): Inside each row group, data is organized by column.  All the data for a specific column within that row group (e.g., all \"Title\" values for the first 100,000 records) is stored together contiguously.</p> </li> <li>Page: This is a further logical partition within a column chunk, where the actual data values are stored.  Each page contains its own metadata, which includes information like the maximum and minimum values present on that page.</li> </ol> <p>This hierarchical structure and the abundant metadata at different levels (file, row group, column, page) are what make Parquet highly efficient</p> <p>Metadata and its Role</p> <p>Parquet stores a rich set of metadata (data about data) internally. This metadata is a key factor in Parquet's performance advantages:</p> <ol> <li>File-level metadata: Includes information like the total number of columns, total rows, and the number of row groups.</li> <li>Row Group-level metadata: Crucially stores the minimum and maximum values for each column within that row group.</li> <li>Page-level metadata: Also contains statistics like minimum and maximum values for data within that specific page. Because of this extensive metadata, when a Parquet file is read, it doesn't need to be given additional parameters like schema information; it already contains all necessary details. This self-describing nature simplifies data processing.</li> </ol> <p>Encoding and Compression Techniques Parquet uses several intelligent encoding and compression techniques to reduce file size and improve query speed without losing information.</p> <p></p> <ol> <li> <p>Encoding: These techniques transform data into a more compact format before compression.</p> <p>Dictionary Encoding:</p> <p>Used for columns with many repeating values (low cardinality), like \"Destination Country Name\" where there might be millions of records but only ~200 distinct countries. Parquet identifies the distinct values in a column and creates a dictionary (a mapping) where each distinct value is assigned a small integer code (e.g., 0 for \"United States,\" 1 for \"France,\" etc.). The actual data in the column is then stored as these compact integer codes instead of the full strings. When reading, Parquet uses the dictionary to convert the codes back to the original values. This drastically reduces storage space.</p> <p>Run Length Encoding (RLE):</p> <p>Used for sequences of repeating values. Instead of storing \"AAAAABBCD,\" RLE would store \"A5B2C1D1\" (A appears 5 times, B 2 times, etc.). This makes the data much smaller, especially for columns with many consecutive identical values.</p> <p>Bit Packing:</p> <p>Optimizes storage at the bit level. If a column's values (after dictionary encoding) only range from 0 to 3, these values can be stored using just 2 bits per value (00, 01, 10, 11) instead of the standard 8 bits (1 byte) or more. This significantly reduces the byte size required to store each value.</p> </li> <li> <p>Compression: After encoding, Parquet applies compression algorithms to further reduce the file size.  Common compression codecs include Gzip, Snappy, and LZ4.  The choice of compression can impact performance. For example, Snappy is often much faster for reads than Gzip, even if Gzip provides slightly better compression ratios. The source states that a query running in 3000 seconds with Gzip might run in just 29 seconds with Snappy, making it 100 times faster</p> </li> </ol> <p>Optimization Techniques in Parquet</p> <p>The combination of columnar storage, hierarchical structure, rich metadata, and intelligent encoding/compression enables two powerful optimization techniques.</p> <ol> <li> <p>Predicate Pushdown (Filter Pushdown): This technique uses the row group-level metadata (min/max values for each column) to skip scanning entire row groups that cannot possibly satisfy a query's filter condition.</p> <p>Example: Consider a query SELECT * FROM table WHERE age &lt; 18.</p> <p>Parquet will first check the metadata of each row group.If a row group's metadata indicates that its age column has a minimum value of 22 and a maximum value of 35, Parquet immediately knows that this row group cannot contain any data where age &lt; 18.</p> <p>Therefore, the system discards that entire row group without reading any of its data from disk. This saves significant I/O, CPU utilization, time, and cost.</p> <p>This optimization also works with equality checks (e.g., WHERE age = 18) by checking if 18 exists in the row group's dictionary (if dictionary encoding is used) or falls within its min/max range.</p> </li> <li> <p>Projection Pruning: This technique capitalizes on Parquet's columnar storage by only reading the columns that are explicitly required by the query.Example: If a query is SELECT name, age FROM users, Parquet will only read the name and age column data from disk and completely skip reading any other columns like address, phone_number, etc.. Since columns are stored separately, this is highly efficient as it avoids bringing unnecessary data into memory, reducing I/O and processing load</p> </li> </ol>"},{"location":"airflow/airflow/","title":"Airflow","text":""},{"location":"airflow/airflow/#what-is-orchestration-in-bigdata","title":"What is Orchestration in BigData?","text":"<p>Orchestration in big data refers to the automated configuration, coordination, and management of complex big data systems and services. Like an orchestra conductor ensures each section of the orchestra plays its part at the right time and the right way to create harmonious music, orchestration in big data ensures that each component of a big data system interacts in the correct manner at the right time to execute complex, multi-step processes efficiently and reliably.</p> <p>--- Workflow management</p> <p>It defines, schedules, and manages workflows involving multiple tasks across disparate systems. These workflows can be simple linear sequences or complex directed acyclic graphs (DAGs) with branching and merging paths.</p> <p>--- Task scheduling</p> <p>Orchestration tools schedule tasks based on their dependencies. This ensures that tasks are executed in the correct order and that tasks that can run in parallel do so, increasing overall system efficiency.</p> <p>--- Failure handling</p> <p>Orchestration tools handle failures in the system, either by retrying failed tasks, skipping them, or alerting operators to the failure.</p> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"airflow/airflow/#need-of-workflow-management-while-designing-data-pipelines","title":"Need of Workflow management while Designing Data Pipelines","text":"<p>--- Ordering and Scheduling</p> <p>Data processing tasks often have dependencies, meaning one task needs to complete before another can begin. For example, a task that aggregates data may need to wait until the data has been extracted and cleaned. A workflow management system can keep track of these dependencies and ensure tasks are executed in the right order.</p> <p>--- Parallelization </p> <p>When tasks don't have dependencies, they can often run in parallel. This can significantly speed up data processing. Workflow management systems can manage parallel execution, maximizing the use of computational resources and reducing overall processing time.</p> <p>--- Error Handling</p> <p>If a task in a data pipeline fails, it can have a knock-on effect on other tasks. Workflow management systems can handle these situations, for instance by retrying failed tasks, skipping them, or stopping the pipeline and alerting operators.</p> <p>--- Visibility and Monitoring</p> <p>Workflow management systems often provide tools for monitoring the progress of data pipelines and visualizing their structure. This can make it easier to spot bottlenecks or failures, and to understand the flow of data through the pipeline.</p> <p>--- Resource Management</p> <p>Workflow management systems can allocate resources (like memory, CPU, etc.) depending on the requirements of different tasks. This helps in efficiently utilizing resources and ensures optimal performance of tasks.</p> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"airflow/airflow/#what-is-airflow","title":"What is AirFlow?","text":"<p>Apache Airflow is an open-source platform that programmatically allows you to author, schedule, and monitor workflows. It was originally developed by Airbnb in 2014 and later became a part of the Apache Software Foundation's project catalog.</p> <p>Airflow uses directed acyclic graphs (DAGs) to manage workflow orchestration. DAGs are a set of tasks with directional dependencies, where the tasks are the nodes in the graph, and the dependencies are the edges. In other words, each task in the workflow executes based on the completion status of its predecessors.</p> <p>--- Key features of Apache Airflow include</p> <ol> <li>Dynamic Pipeline Creation - Airflow allows you to create dynamic pipelines using Python. This provides flexibility and can be adapted to complex dependencies and operations.</li> <li>Easy Scheduling - Apache Airflow includes a scheduler to execute tasks at defined intervals. The scheduling syntax is quite flexible, allowing for complex scheduling.</li> <li>Robust Monitoring and Logging - Airflow provides detailed status and logging information about each task, facilitating debugging and monitoring. It also offers a user-friendly UI to monitor and manage the workflow.</li> <li>Scalability - Airflow can distribute tasks across a cluster of workers, meaning it can scale to handle large workloads.</li> <li>Extensibility - Airflow supports custom plugins and can integrate with several big data technologies. You can define your own operators and executors, extend the library, and even use the user interface API.</li> <li>Failure Management - In case of a task failure, Airflow sends alerts and allows for retries and catchup of past runs in a robust way</li> </ol> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"airflow/airflow/#airflow-architecture","title":"Airflow Architecture","text":"<p>--- Scheduler</p> <p>The scheduler is a critical component of Airflow. Its primary function is to continuously scan the DAGs (Directed Acyclic Graphs) directory to identify and schedule tasks based on their dependencies and specified time intervals. The scheduler is responsible for determining which tasks to execute and when. It interacts with the metadata database to store and retrieve task state and execution information.</p> <p>--- Metadata Database</p> <p>Airflow leverages a metadata database, such as PostgreSQL or MySQL, to store all the configuration details, task states, and execution metadata. The metadata database provides persistence and ensures that Airflow can recover from failures and resume tasks from their last known state. It also serves as a central repository for managing and monitoring task execution.</p> <p>--- Web Server</p> <p>The web server component provides a user interface for interacting with Airflow. It enables users to monitor task execution, view the status of DAGs, and access logs and other operational information. The web server communicates with the metadata database to fetch relevant information and presents it in a user-friendly manner. Users can trigger manual task runs, monitor task progress, and configure Airflow settings through the web server interface.</p> <p>--- Executors</p> <p>Airflow supports different executor types to execute tasks. The executor is responsible for allocating resources and running tasks on the specified worker nodes.</p> <p>Example</p> <p>the local executor executes tasks on the same node as the scheduler, while the Kubernetes executor executes tasks in containers.</p> <p>--- Worker Nodes</p> <p>Worker nodes are responsible for executing the tasks assigned to them by the executor. They retrieve the task details, dependencies, and code from the metadata database and execute the tasks accordingly. The number of worker nodes can be scaled up or down based on the workload and resource requirements.</p> <p>--- Message Queue</p> <p>Airflow relies on a message queue system, such as RabbitMQ, Apache Kafka, or Redis, to enable communication between the scheduler and the worker nodes. The scheduler places task execution requests in the message queue, and the worker nodes pick up these requests, execute the tasks, and update their status back to the metadata database. The message queue acts as a communication channel, ensuring reliable task distribution and coordination.</p> <p>--- The DAG Processor</p> <p>It processes DAG files and stores the metadata in the database. It scans the DAG folder every 30 seconds (default) to look for new or updated files. It uses DagFileProcessorManager to detect changes and DagFileProcessorProcess to parse the actual code. To speed up parsing, import statements and variables should be placed inside methods rather than at the top of the script.In newer versions, the DAG Processor is separated from the Scheduler to allow independent scaling and prevent CPU bottlenecks</p> <p>--- Triggerer</p> <p>Introduced to handle long-running tasks. Normally, long tasks block the Scheduler's task slots. The Triggerer allows a task to enter a \"Deferred\" state. It uses a small asynchronous Python code to \"watch\" the task while the Worker does the heavy lifting, freeing up the Scheduler's pool for other tasks</p> <p>--- Airflow 3.0: The New Client-Server Architecture</p> <p>The most significant architectural shift in Airflow 3.0 is the removal of direct database access for several components.</p> <ol> <li>Server Side - Only the Scheduler and the API Server can communicate directly with the Database.</li> <li>Client Side - The DAG Processor, Worker, and Triggerer are now \"clients.\" They cannot talk to the DB directly.</li> <li>Communication Flow-<ol> <li>The DAG Processor parses a DAG and sends the info to an Internal API Server.</li> <li>The API Server writes that info into the DB.</li> <li>The Scheduler reads from the DB and triggers the task.</li> <li>If a task is deferred, the Triggerer updates the status via the API Server to the DB.</li> </ol> </li> </ol> <p>--- Worker Execution Flow</p> <ol> <li>Task Submission - The Scheduler identifies a task that needs to run and hands it to the Executor. The Executor then submits a message containing the execution command to a Broker/Queue (typically Redis).</li> <li>Message Pulling - The Worker Process constantly monitors the Redis queue. When a message appears, the Worker pulls the message to begin the job.</li> <li> <p>Internal Hierarchy - Once a message is pulled, the execution follows an internal hierarchy within the worker machine to isolate the task.</p> <ol> <li>Worker Process: The main process that communicates with the queue.</li> <li>Worker Child Process: A sub-process spawned for a specific task.</li> <li>Local Task Process: An internal process dedicated to that specific instance.</li> <li>Task Runner: The final layer that executes the actual code (e.g., echo copying file).</li> </ol> </li> <li> <p>Result Reporting - After the Task Runner completes the work, it writes the task status (Success/Failure) to the Result Backend.</p> </li> <li>Database Update - The Scheduler reads the status from the Result Backend and updates the primary Postgres Metadata Database so the status can be reflected in the Airflow UI</li> </ol> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"airflow/airflow/#dags-and-tasks","title":"DAGs and Tasks","text":"<p>DAGs are at the core of Airflow\u2019s architecture. A DAG is a directed graph consisting of interconnected tasks. Each task represents a unit of work within the data pipeline. Tasks can have dependencies on other tasks, defining the order in which they should be executed. Airflow uses the DAG structure to determine task dependencies, schedule task execution, and track their progress.</p> <p>Each task within a DAG is associated with an operator, which defines the type of work to be performed. Airflow provides a rich set of built-in operators for common tasks like file operations, data processing, and database interactions. Additionally, custom operators can be created to cater to specific requirements. Tasks within a DAG can be triggered based on various events, such as time-based schedules, the completion of other tasks, or the availability of specific data.</p> <p>In Airflow, there's a task which perform 1 unique job across the DAG, the task is usually calling Airflow Operator.</p> <p>An operator is essentially a Python class that defines what a task should do and how it should be executed within the context of the DAG.</p> <ul> <li>Operators can perform a wide range of tasks, such as running a Bash script or Python script, executing SQL, sending an email, or transferring files between systems.</li> <li>Operators provide a high-level abstraction of the tasks, letting us focus on the logic and flow of the workflow without getting bogged down in the implementation details.</li> <li>Apache Airflow has several types of operators that allow you to perform different types of tasks. </li> </ul> <p>--- Why use Airflow/DAGs instead of Simple Scripts</p> <p>A common question is why one shouldn't just use a Python for loop or sequential method calls. </p> <ol> <li>Parallel Task Execution - If you need to fetch data from 50 different APIs, a sequential script might take 4 minutes (5 seconds each). Airflow can run these tasks in parallel, reducing the total time to just 5 seconds.</li> <li>Granular Re-runs (Error Handling) - In a sequential script, if the \"Clean\" step fails after a 2-hour \"Fetch\" step, you might have to restart everything. In an Airflow DAG, you can re-trigger only the failed task (the \"Clean\" step) without re-running the successful \"Fetch\" step, saving time and resources</li> </ol> <p>--- Create a DAG</p> <p>A DAG file starts with a <code>dag</code> object. We can create a <code>dag</code> object using a context manager or a decorator.</p> <pre><code>from airflow.decorators import dag\nfrom airflow import DAG\nimport pendulum\nfrom airflow.providers.standard.operators.bash import BashOperator \nfrom airflow.providers.standard.operators.python import PythonOperator\n\n# dag1 - using @dag decorator\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag1():\n    pass\n\nrandom_dag1()\n\n# dag2 - using context manager\nwith DAG(\n    dag_id=\"random_dag2\",\n    schedule=\"@daily\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n) as dag2:\n    pass\n\n# 1. Define a function for the PythonOperator\ndef print_context(**kwargs): [29]\n    print(kwargs)\n    print(\"Job completed\")\n\n# 2. Define the tasks\ncopy_file = BashOperator(\n    task_id=\"copy_file\",\n    bash_command=\"echo copying file\",\n    dag=dag\n)\n\ntask_2 = PythonOperator(\n    task_id=\"task_2\",\n    python_callable=print_context,\n    dag=dag\n)\n\n# 3. Set dependencies\n# Using the right-shift operator\ncopy_file &gt;&gt; task_2  \n# Alternatively: copy_file.set_downstream(task_2) \n</code></pre> <p>Either way, we need to define a few parameters to control how a DAG is supposed to run.</p> <p>Some of the most-used parameters are:</p> <ol> <li> <p>start_date - If it's a future date, it's the timestamp when the scheduler starts to run. If it's a past date, it's the timestamp from which the scheduler will attempt to backfill.</p> </li> <li> <p>catch_up - Whether to perform scheduler catch-up. If set to true, the scheduler will backfill runs from the start date.</p> </li> <li> <p>schedule - Scheduling rules. Currently, it accepts a cron string, time delta object, timetable, or list of dataset objects.</p> </li> <li> <p>tags - List of tags helping us search DAGs in the UI.</p> </li> </ol>"},{"location":"airflow/airflow/#task-level-operator","title":"Task level Operator","text":"<p>Tip</p> <p>While Airflow provides a wide variety of operators out of the box, we may still need to create custom operators to address our specific use cases.</p> <p>All operators are extended from <code>BaseOperator</code> and we need to override two methods: <code>__init__</code> and <code>execute</code>.</p> <p>The <code>execute</code> method is invoked when the runner calls the operator.</p> <p>The following example creates a custom operator <code>DataAnalysisOperator</code> that performs the requested type of analysis on an input file and saves the results to an output file.</p> <p>Warning</p> <p>It's advised not to put expensive operations in <code>__init__</code> because it will be instantiated once per scheduler cycle.</p> <pre><code>from airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass DataAnalysisOperator(BaseOperator):\n    @apply_defaults\n    def __init__(self, dataset_path, output_path, analysis_type, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dataset_path = dataset_path\n        self.output_path = output_path\n        self.analysis_type = analysis_type\n\n    def execute(self, context):\n        # Load the input dataset into a pandas DataFrame.\n        data = pd.read_csv(self.dataset_path)\n\n        # Perform the requested analysis.\n        if self.analysis_type == 'mean':\n            result = np.mean(data)\n        elif self.analysis_type == 'std':\n            result = np.std(data)\n        else:\n            raise ValueError(f\"Invalid analysis type '{self.analysis_type}'\")\n\n        # Write the result to a file.\n        with open(self.output_path, 'w') as f:\n            f.write(str(result))\n</code></pre> <p>The extensibility of the operator is one of many reasons why Airflow is so powerful and popular.</p> <p>Tip</p> <pre><code>BashOperator: Executes a bash command.\nPythonOperator: Calls a Python function.\nEmailOperator: Sends an email.\nSimpleHttpOperator: Sends an HTTP request.\nMySqlOperator, SqliteOperator, PostgresOperator, MsSqlOperator, OracleOperator, etc.: Executes a SQL command.\nDummyOperator: A placeholder operator that does nothing.\nSensor: Waits for a certain time, file, database row, S3 key, etc. There are many types of sensors, like HttpSensor, SqlSensor, S3KeySensor, TimeDeltaSensor, ExternalTaskSensor, etc.\nSSHOperator: Executes commands on a remote server using SSH.\nDockerOperator: Runs a Docker container.\nSparkSubmitOperator: Submits a Spark job.\nOperators in Airflow \nS3FileTransformOperator: Copies data from a source S3 location to a temporary location on the local filesystem, transforms the data, and then uploads it to a destination S3 location.\nS3ToRedshiftTransfer: Transfers data from S3 to Redshift.\nEmrAddStepsOperator: Adds steps to an existing EMR (Elastic Map Reduce) job flow.\nEmrCreateJobFlowOperator: Creates an EMR JobFlow, i.e., a cluster.\nAthenaOperator: Executes a query on AWS Athena.\nAwsGlueJobOperator: Runs an AWS Glue Job.\nS3DeleteObjectsOperator: Deletes objects from an S3 bucket.\nBigQueryOperator: Executes a BigQuery SQL query.\nBigQueryToBigQueryOperator: Copies data from one BigQuery table to another.\nDataprocClusterCreateOperator: This operator is used to create a new cluster of machines on GCP's Dataproc service.\nDataProcPySparkOperator: This operator is used to submit PySpark jobs to a running Dataproc cluster.\nDataProcSparkOperator: This operator is used to submit Spark jobs written in Scala or Java to a running Dataproc cluster.\nDataprocClusterDeleteOperator: This operator is used to delete an existing cluster.\n</code></pre>"},{"location":"airflow/airflow/#sensor-operator","title":"Sensor Operator","text":"<p>A special type of operator is called a sensor operator. It's designed to wait until something happens and then succeed so their downstream tasks can run. The DAG has a few sensors that are dependent on external files, time, etc.</p> <p>Common sensor types are:</p> <ul> <li> <p>TimeSensor: Wait for a certain amount of time to pass before executing a task.</p> </li> <li> <p>FileSensor: Wait for a file to be in a location before executing a task.</p> </li> <li> <p>HttpSensor: Wait for a web server to become available or return an expected result before executing a task.</p> </li> <li> <p>ExternalTaskSensor: Wait for an external task in another DAG to complete before executing a task.</p> </li> </ul> <p>We can create a custom sensor operator by extending the <code>BaseSensorOperator</code> class and overriding two methods: <code>__init__</code> and <code>poke</code>.</p> <p>The <code>poke</code> method performs the necessary checks to determine whether the condition has been satisfied. If so, return <code>True</code> to indicate that the sensor has succeeded. Otherwise, return <code>False</code> to continue checking in the next interval.</p> <p>Here is an example of a custom sensor operator that pulls an endpoint until the response matches with <code>expected_text</code>.</p> <pre><code>from airflow.sensors.base_sensor_operator import BaseSensorOperator\nfrom airflow.utils.decorators import apply_defaults\nimport requests\n\nclass EndpointSensorOperator(BaseSensorOperator):\n\n    @apply_defaults\n    def __init__(self, url, expected_text, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.url = url\n        self.expected_text = expected_text\n\n    def poke(self, context):\n        response = requests.get(self.url)\n        if response.status_code != 200:\n            return False\n        return self.expected_text in response.text\n</code></pre>"},{"location":"airflow/airflow/#poke-mode-vs-reschedule-mode","title":"Poke mode vs. Reschedule mode","text":"<p>There are two types of modes in a sensor:</p> <ul> <li> <p>poke: the sensor repeatedly calls its <code>poke</code> method at the specified <code>poke_interval</code>, checks the condition, and reports it back to Airflow.   As a consequence, the sensor takes up the worker slot for the entire execution time.</p> </li> <li> <p>reschedule: Airflow is responsible for scheduling the sensor to run at the specified <code>poke_interval</code>.   But if the condition is not met yet, the sensor will release the worker slot to other tasks between two runs.</p> </li> </ul> <p>Tip</p> <p>In general, poke mode is more appropriate for sensors that require short run-time and <code>poke_interval</code> is less than five minutes.</p> <p>Reschedule mode is better for sensors that expect long run-time (e.g., waiting for data to be delivered by an external party) because it is less resource-intensive and frees up workers for other tasks.</p>"},{"location":"airflow/airflow/#backfilling","title":"Backfilling","text":"<p>Backfilling is an important concept in data processing. It refers to the process of populating or updating historical data in the system to ensure that the data is complete and up-to-date.</p> <p>This is typically required in two use cases:</p> <ul> <li> <p>Implement a new data pipeline: If the pipeline uses an incremental load, backfilling is needed to populate historical data that falls outside the reloading window.</p> </li> <li> <p>Modify an existing data pipeline: When fixing a SQL bug or adding a new column, we also want to backfill the table to update the historical data.</p> </li> </ul> <p>Warning</p> <p>When backfilling the table, we must ensure that the new changes are compatible with the existing data; otherwise, the table needs to be recreated from scratch.</p> <p>Sometimes, the backfilling job can consume significant resources due to the high volume of historical data.</p> <p>It's also worth checking any possible downstream failure before executing the backfilling job.</p> <p>Airflow provides the backfilling process in its cli command.</p> <pre><code>airflow backfill [dag name] -s [start date] -e [end date]\n</code></pre> <p>To create DAGs, we just need basic knowledge of Python. However, to create efficient and scalable DAGs, it's essential to master Airflow's specific features and nuances.</p>"},{"location":"airflow/airflow/#create-a-task-object","title":"Create a task object","text":"<p>A DAG object is composed of a series of dependent tasks. A task can be an operator, a sensor, or a custom Python function decorated with <code>@task</code>. Then, we will use <code>&gt;&gt;</code> or the opposite way, which denotes the dependencies between tasks.</p> <pre><code># dag3\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag3():\n\n    s1 = TimeDeltaSensor(task_id=\"time_sensor\", delta=timedelta(seconds=2))\n    o1 = BashOperator(task_id=\"bash_operator\", bash_command=\"echo run a bash script\")\n    @task\n    def python_operator() -&gt; None:\n        logging.info(\"run a python function\")\n    o2 = python_operator()\n\n    s1 &gt;&gt; o1 &gt;&gt; o2\n\nrandom_dag3()\n</code></pre> <p>A task has a well-defined life cycle, including 14 states, as shown in the following graph:</p> <p></p>"},{"location":"airflow/airflow/#pass-data-between-tasks","title":"Pass data between tasks","text":"<p>One of the best design practices is to split a heavy task into smaller tasks for easy debugging and quick recovery.</p> <p>Example</p> <p>we first make an API request and use the response as the input for the second API request. To do so, we need to pass a small amount of data between tasks.</p> <p>XComs (cross-communications) is a method for passing data between Airflow tasks. Data is defined as a key-value pair, and the value must be serializable.</p> <ul> <li> <p>The <code>xcom_push()</code> method pushes data to the Airflow metadata database and is made available for other tasks.</p> </li> <li> <p>The <code>xcom_pull()</code> method retrieves data from the database using the key.</p> </li> </ul> <p></p> <p>Every time a task returns a value, the value is automatically pushed to XComs.</p> <p>We can find them in the Airflow UI <code>Admin</code> -&gt; <code>XComs</code>. If the task is created using <code>@task</code>, we can retrieve XComs by using the object created from the upstream task.</p> <p>In the following example, the operator <code>o2</code> uses the traditional syntax, and the operator <code>o3</code> uses the simple syntax:</p> <pre><code>@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag4():\n\n    @task\n    def python_operator() -&gt; None:\n        logging.info(\"run a python function\")\n        return str(datetime.now()) # return value automatically stores in XCOMs\n    o1 = python_operator()\n    o2 = BashOperator(task_id=\"bash_operator1\", bash_command='echo \"{{ ti.xcom_pull(task_ids=\"python_operator\") }}\"') # traditional way to retrieve XCOM value\n    o3 = BashOperator(task_id=\"bash_operator2\", bash_command=f'echo {o1}') # make use of @task feature\n\n    o1 &gt;&gt; o2 &gt;&gt; o3\n\nrandom_dag4()\n</code></pre> <p>Warning</p> <p>Although nothing stops us from passing data between tasks, the general advice is to not pass heavy data objects, such as pandas DataFrame and SQL query results because doing so may impact task performance.</p>"},{"location":"airflow/airflow/#use-jinja-templates","title":"Use Jinja templates","text":"<p>Info</p> <p>Jinja is a templating language used by many Python libraries, such as Flask and Airflow, to generate dynamic content.</p> <p>It allows us to embed variables within the text and then have those variables replaced with actual values during runtime.</p> <p>Airflow's Jinja templating engine provides built-in functions that we can use between double curly braces, and the expression will be evaluated at runtime.</p> <p>Users can also create their own macros using user_defined_macros and the macro can be a variable as well as a function.</p> <pre><code>def days_to_now(starting_date):\n    return (datetime.now() - starting_date).days\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"],\n    user_defined_macros={\n        \"starting_date\": datetime(2015, 5, 1),\n        \"days_to_now\": days_to_now,\n    })\ndef random_dag5():\n\n    o1 = BashOperator(task_id=\"bash_operator1\", bash_command=\"echo Today is {{ execution_date.format('dddd') }}\")\n    o2 = BashOperator(task_id=\"bash_operator2\", bash_command=\"echo Days since {{ starting_date }} is {{ days_to_now(starting_date) }}\")\n\n    o1 &gt;&gt; o2\n\nrandom_dag5()\n</code></pre> <p>Note</p> <p>The full list of built-in variables, macros and filters in Airflow can be found in the Airflow Documentation</p> <p>Another feature around templating is the <code>template_searchpath</code> parameter in the DAG definition.</p> <p>It's a list of folders where Jinja will look for templates.</p> <p>Example</p> <p>A common use case is invoking an SQL file in a database operator such as BigQueryInsertJobOperator. Instead of hardcoding the SQL query, we can refer to the SQL file, and the content will be automatically rendered during runtime.</p> <pre><code>@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"],\n    template_searchpath=[\"/usercode/dags/sql\"])\ndef random_dag6():\n\n    BigQueryInsertJobOperator(\n        task_id=\"insert_query_job\",\n        configuration={\n            \"query\": {\n                \"query\": \"{% include 'sample.sql' %}\",\n                \"useLegacySql\": False,\n            }\n        }\n    )\n\nrandom_dag6()\n</code></pre>"},{"location":"airflow/airflow/#manage-cross-dag-dependencies","title":"Manage cross-DAG dependencies","text":"<p>In principle, every DAG is an independent workflow. However, sometimes, it's necessary to create dependencies between DAGs.</p> <p>Example</p> <p>a DAG performs an ETL job that produces a table sales. The sales table is the source of two downstream DAGs, where one generates revenue reports, and the other one uses it to train a machine learning model.</p> <p>There are several ways to implement cross-DAG dependencies in Airflow.</p> <ul> <li><code>TriggerDagOperator</code> is an operator that triggers a downstream DAG from any point in the DAG. It's similar to a push mechanism where the producer decides when to notify the consumers.</li> </ul> <pre><code>from airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag7():\n\n    TriggerDagRunOperator(\n        task_id=\"trigger_dagrun\",\n        trigger_dag_id=\"random_dag1\",\n        conf={},\n    )\n\nrandom_dag7()\n</code></pre> <ul> <li><code>ExternalTaskSensor</code> is a sensor operator for downstream DAGs to pull states of the upstream DAG, similar to a pull mechanism. The downstream DAG will wait until the task is completed in the upstream DAG.</li> </ul> <pre><code>from airflow.sensors.external_task import ExternalTaskSensor\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag8():\n\n    ExternalTaskSensor(\n        task_id=\"external_sensor\",\n        external_dag_id=\"random_dag3\",\n        external_task_id=\"python_operator\",\n        allowed_states=[\"success\"],\n        failed_states=[\"failed\", \"skipped\"],\n    )\n\nrandom_dag8()\n</code></pre> <p>Another method introduced in <code>version 2.4</code> uses datasets to create data-driven dependencies between DAGs.</p> <p>An Airflow dataset is a logical grouping of data updated by upstream tasks. The upstream task defines the output dataset via <code>outlets</code> parameter. The completion of the task means the successful update of the dataset.</p> <p>In downstream DAGs, instead of using a time-based schedule, the DAG refers to the corresponding dataset produced by the upstreams. Therefore, the downstream DAG will be triggered in a data-driven manner rather than a scheduled-based manner.</p> <pre><code>dag1_dataset = Dataset(\"s3://dag1/output_1.txt\", extra={\"hi\": \"bye\"})\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag9_producer():\n    BashOperator(outlets=[dag1_dataset], task_id=\"producer\", bash_command=\"sleep 5\")\n\nrandom_dag9_producer()\n\n@dag(\n    schedule=[dag1_dataset],\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag9_consumer():\n    BashOperator(task_id=\"consumer\", bash_command=\"sleep 5\")\n\nrandom_dag9_consumer()\n</code></pre>"},{"location":"airflow/airflow/#best-practices","title":"Best practices","text":"<p>When working with Airflow, there are several best practices to keep in mind that help ensure our pipelines run smoothly and efficiently.</p>"},{"location":"airflow/airflow/#idempotency","title":"Idempotency","text":"<p>Idempotency is a fundamental concept for data pipelines. In the context of Airflow, idempotency means running the same DAG Run multiple times has the same effect as running it only once. When a DAG is designed to be idempotent, it can be executed repeatedly without causing unexpected changes to the pipeline's output.</p> <p>This is especially necessary when a DAG Run might be rerun due to failures or errors in the processing.</p> <p>Example</p> <p>An example to make DAG idempotent is to use templates such as variable <code>{{ execution_date }}</code>.</p> <p>It's associated with the expected scheduled time of each run, and the date won't be changed even if we rerun the DAG Run a few hours later.</p>"},{"location":"airflow/airflow/#avoid-top-level-code-in-the-dag-file","title":"Avoid top-level code in the DAG file","text":"<p>By default, Airflow reads the dag folder every 30 seconds, including the top-level code that is outside of DAG context.</p> <p>Because of this, having expensive top-level code, such as making requests to external APIs, can cause performance issues because they are called every 30 seconds rather than only when DAG is scheduled.</p> <p>The general advice is to limit the amount of top-level code in the DAG file and move it within the DAG context or operators.</p> <p>This can help reduce unnecessary overheads and allow Airflow to focus on executing the right things.</p> <p>The following example shows both good and bad ways of making an API request:</p> <pre><code># Bad example - requests will be made every 30 seconds instead of everyday at 4:30am\nres = requests.get(\"https://api.sampleapis.com/coffee/hot\")\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag7():\n\n    @task\n    def python_operator() -&gt; None:\n        logging.info(f\"API result {res}\")\n    python_operator()\n\nrandom_dag7()\n\n# Good example\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag7():\n\n    @task\n    def python_operator() -&gt; None:\n        res = requests.get(\"https://api.sampleapis.com/coffee/hot\") # move API request within DAG context\n        logging.info(f\"API result {res}\")\n    python_operator()\n\nrandom_dag7()\n</code></pre>"},{"location":"airflow/airflow/#how-to-execute-tasks-parallelly","title":"How to execute tasks parallelly?","text":"<pre><code>```bash\nstart_task = DummyOperator(task_id='start_task', dag=dag)\nparallel_task_1 = DummyOperator(task_id='parallel_task_1', dag=dag) \nparallel_task_2 = DummyOperator(task_id='parallel_task_2', dag=dag) \nparallel_task_3 = DummyOperator(task_id='parallel_task_3', dag=dag) \nend_task = DummyOperator(task_id='end_task', dag=dag)\n# Setting up the dependencies start_task &gt;&gt; [parallel_task_1, parallel_task_2, parallel_task_3] &gt;&gt; end_task\n```\n</code></pre> <p>In this example, the three parallel tasks (parallel_task_1, parallel_task_2, parallel_task_3) are specified as a list in the dependency chain. The start_task runs first. Once it completes, all three parallel tasks begin. When all three of them complete, the end_task starts.</p>"},{"location":"airflow/airflow/#how-to-overwrite-depends_on_past-property","title":"How to overwrite depends_on_past property?","text":"<p>The depends_on_past attribute in the default_args dictionary of a DAG applies globally to all tasks in the DAG when set. However, if you want to override this behavior for specific tasks, you can specify the depends_on_past attribute directly on those tasks.</p> <p>For specific tasks where you want to override this behavior, set depends_on_past directly on the task:     <pre><code>task_with_custom_dep = DummyOperator(     \n    task_id='task_with_custom_dep',\n    depends_on_past=False, \n    dag=dag\n    )\n</code></pre></p>"},{"location":"airflow/airflowiq/","title":"AIRFLOW_IQ","text":"<p>1. What is Airflow?</p> <p>Airflow is an open-source tool for programmatically authoring, scheduling, and monitoring data pipelines. Apache Airflow is an open source data orchestration tool that allows data practitioners to define data pipelines programmatically with the help of Python. Airflow is most commonly used by data engineering teams to integrate their data ecosystem and extract, transform, and load data.</p> <p>2. What issues does Airflow resolve?</p> <p>Crons are an old technique of task scheduling. Scalable Cron requires external assistance to log, track, and manage tasks. The Airflow UI is used to track and monitor the workflow's execution. Creating and maintaining a relationship between tasks in cron is a challenge, whereas it is as simple as writing Python code in Airflow. Cron jobs are not reproducible until they are configured externally. Airflow maintains an audit trail of all tasks completed.</p> <p>3. Explain how workflow is designed in Airflow?</p> <p>A directed acyclic graph (DAG) is used to design an Airflow workflow. That is to say, when creating a workflow, consider how it can be divided into tasks that can be completed independently. The tasks can then be combined into a graph to form a logical whole. The overall logic of your workflow is based on the shape of the graph. An Airflow DAG can have multiple branches, and you can choose which ones to follow and which to skip during workflow execution. Airflow Pipeline DAG Airflow could be completely stopped, and able to run workflows would then resume through restarting the last unfinished task. It is important to remember that airflow operators can be run more than once when designing airflow operators. Each task should be idempotent, or capable of being performed multiple times without causing unintended consequences.</p> <p>4. Explain Airflow Architecture and its components?</p> <p>Airflow has six main components:</p> <ul> <li>The web server for serving and updating the Airflow user interface.</li> <li>The metadata database for storing all metadata (e.g., users, tasks) related to your Airflow instance.</li> <li>The scheduler for monitoring and scheduling your pipelines.</li> <li>The executor for defining how and on which system tasks are executed.</li> <li>The queue for holding tasks that are ready to be executed.</li> <li>The worker(s) for executing instructions defined in a task.</li> </ul> <p>Airflow runs DAGs in six different steps:</p> <ol> <li>The scheduler constantly scans the DAGs directory for new files. The default time is every 5 minutes.</li> <li>After the scheduler detects a new DAG, the DAG is processed and serialized into the metadata database.</li> <li>The scheduler scans for DAGs that are ready to run in the metadata database. The default time is every 5 seconds.</li> <li>Once a DAG is ready to run, its tasks are put into the executor's queue.</li> <li>Once a worker is available, it will retrieve a task to execute from the queue.</li> <li>The worker will then execute the task.</li> </ol> <p>5. What are the types of Executors in Airflow?</p> <p>The executors are the components that actually execute the tasks, while the Scheduler orchestrates them. Airflow has different types of executors, including SequentialExecutor, LocalExecutor, CeleryExecutor and KubernetesExecutor. People generally choose the executor which is best for their use case.</p> <ul> <li>SequentialExecutor: Only one task is executed at a time by SequentialExecutor. The scheduler and the workers both use the same machine.</li> <li>LocalExecutor: LocalExecutor is the same as the Sequential Executor, except it can run multiple tasks at a time.</li> <li>CeleryExecutor: Celery is a Python framework for running distributed asynchronous tasks. As a result, CeleryExecutor has long been a part of Airflow, even before Kubernetes. CeleryExecutors has a fixed number of workers on standby to take on tasks when they become available.</li> <li>KubernetesExecutor: Each task is run by KubernetesExecutor in its own Kubernetes pod. It, unlike Celery, spins up worker pods on demand, allowing for the most efficient use of resources.</li> </ul> <p>6. What are the pros and cons of SequentialExecutor?</p> <ul> <li>Pros: It's simple and straightforward to set up. It's a good way to test DAGs while they're being developed.</li> <li>Cons: It isn't scalable. It is not possible to perform many tasks at the same time. Unsuitable for use in production.</li> </ul> <p>7. What are the pros and cons of LocalExecutor?</p> <ul> <li>Pros: Able to perform multiple tasks. Can be used to run DAGs during development.</li> <li>Cons: The product isn't scalable. There is only one point of failure. Unsuitable for use in production.</li> </ul> <p>8. What are the pros and cons of CeleryExecutor?</p> <ul> <li>Pros: It allows for scalability. Celery is responsible for managing the workers. Celery creates a new one in the case of a failure.</li> <li>Cons: Celery requires RabbitMQ/Redis for task queuing, which is redundant with what Airflow already supports. The setup is also complicated due to the above-mentioned dependencies.</li> </ul> <p>9. What are the pros and cons of KubernetesExecutor?</p> <ul> <li>Pros: It combines the benefits of CeleryExecutor and LocalExecutor in terms of scalability and simplicity. Fine-grained control over task-allocation resources. At the task level, the amount of CPU/memory needed can be configured.</li> <li>Cons: Airflow is newer to Kubernetes, and the documentation is complicated.</li> </ul> <p>10. How to define a workflow in Airflow?</p> <p>Python files are used to define workflows. DAG (Directed Acyclic Graph) The DAG Python class in Airflow allows you to generate a Directed Acyclic Graph, which is a representation of the workflow.</p> <p><pre><code>from airflow.models import DAG\nfrom airflow.utils.dates import days_ago\nargs = { 'start_date': days_ago(0) }\ndag = DAG(\n    dag_id='bash_operator_example',\n    default_args=args,\n    schedule_interval='* * * * *',\n)\n</code></pre> You can use the start date to launch a task on a specific date. The schedule interval specifies how often each workflow is scheduled to run. '* * * * *' indicates that the tasks must run every minute.</p> <p>11. How do you make the module available to airflow if you're using Docker Compose?</p> <p>If we are using Docker Compose, then we will need to use a custom image with our own additional dependencies in order to make the module available to Airflow. Refer to the following Airflow Documentation for reasons why we need it and how to do it.</p> <p>12. How to schedule DAG in Airflow?</p> <p>DAGs could be scheduled by passing a timedelta or a cron expression (or one of the @ presets), which works well enough for DAGs that need to run on a regular basis, but there are many more use cases that are presently difficult to express \"natively\" in Airflow, or that require some complicated workarounds.</p> <p>13. What is XComs In Airflow?</p> <p>XCom (short for cross-communication) are messages that allow data to be sent between tasks. The key, value, timestamp, and task/DAG id are all defined.</p> <p>14. What is xcom_pull in XCom Airflow?</p> <p>The xcom push and xcom pull methods on Task Instances are used to explicitly \"push\" and \"pull\" XComs to and from their storage. Whereas if do xcom push parameter is set to True (as it is by default), many operators and @task functions will auto-push their results into an XCom key named return value. If no key is supplied to xcom pull, it will use this key by default, allowing you to write code like this:</p> <pre><code>value = task_instance.xcom_pull(task_ids='pushing_task')\n</code></pre> <p>15. What is Jinja templates?</p> <p>Jinja is a templating engine that is quick, expressive, and extendable. The template has special placeholders that allow you to write code that looks like Python syntax. After that, data is passed to the template in order to render the final document.</p> <p>16. How to use Airflow XComs in Jinja templates?</p> <p>We can use XComs in Jinja templates as given below:</p> <pre><code>SELECT * FROM {{ task_instance.xcom_pull(task_ids='foo', key='table_name') }}\n</code></pre> <p>17. How does Apache Airflow act as a Solution?</p> <ul> <li>Failures: This tool assists in retrying in case there is a failure.</li> <li>Monitoring: It helps in checking if the status has been succeeded or failed.</li> <li>Dependency: There are two different types of dependencies, such as:<ul> <li>Data Dependencies that assist in upstreaming the data</li> <li>Execution Dependencies that assist in deploying all the new changes</li> </ul> </li> <li>Scalability: It helps centralize the scheduler</li> <li>Deployment: It is useful in deploying changes with ease</li> <li>Processing Historical Data: It is effective in backfilling historical data</li> </ul> <p>18. How would you design an Airflow DAG to process a large dataset?</p> <p>When designing an Airflow DAG to process a large dataset, there are several key considerations to keep in mind.</p> <ul> <li>The DAG should be designed to be modular and scalable. This means that the DAG should be broken down into smaller tasks that can be run in parallel, allowing for efficient processing of the data. Additionally, the DAG should be designed to be able to scale up or down depending on the size of the dataset.</li> <li>The DAG should be designed to be fault-tolerant. This means that the DAG should be designed to handle errors gracefully and be able to recover from them. This can be done by using Airflow's retry and catchup features, as well as by using Airflow's XCom feature to pass data between tasks.</li> <li>The DAG should be designed to be efficient. This means that the DAG should be designed to minimize the amount of data that needs to be processed and to minimize the amount of time it takes to process the data. This can be done by using Airflow's features such as branching, pooling, and scheduling.</li> <li>The DAG should be designed to be secure. This means that the DAG should be designed to protect the data from unauthorized access and to ensure that only authorized users can access the data. This can be done by using Airflow's authentication and authorization features.</li> </ul> <p>By following these guidelines, an Airflow DAG can be designed to efficiently and securely process a large dataset.</p> <p>19. What strategies have you used to optimize Airflow performance?</p> <p>When optimizing Airflow performance, I typically focus on three main areas:</p> <ul> <li>Utilizing the right hardware: Airflow is a distributed system, so it's important to ensure that the hardware you're using is up to the task. This means having enough memory, CPU, and disk space to handle the workload. Additionally, I make sure to use the latest version of Airflow, as this can help improve performance.</li> <li>Optimizing the DAGs: I make sure to optimize the DAGs by using the best practices for Airflow. This includes using the right operators, setting the right concurrency levels, and using the right execution dates. Additionally, I make sure to use the right parameters for the tasks, such as setting the right retry limits and timeouts.</li> <li>Utilizing the right tools: I make sure to use the right tools to monitor and analyze the performance of Airflow. This includes using the Airflow UI, the Airflow CLI, and the Airflow Profiler. Additionally, I make sure to use the right metrics to measure performance, such as task duration, task throughput, and task latency.</li> </ul> <p>By focusing on these three areas, I am able to optimize Airflow performance and ensure that the system is running as efficiently as possible.</p> <p>20. How do you debug an Airflow DAG when it fails?</p> <p>When debugging an Airflow DAG that has failed, the first step is to check the Airflow UI for the failed task. The UI will provide information about the task, such as the start and end time, the duration of the task, and the error message. This information can help to identify the cause of the failure.</p> <p>The next step is to check the Airflow logs for the failed task. The logs will provide more detailed information about the task, such as the exact command that was executed, the environment variables, and the stack trace. This information can help to pinpoint the exact cause of the failure.</p> <p>The third step is to check the code for the failed task. This can help to identify any errors in the code that may have caused the failure.</p> <p>Finally, if the cause of the failure is still not clear, it may be necessary to set up a debugging environment to step through the code and identify the exact cause of the failure. This can be done by setting up a local Airflow instance and running the DAG in debug mode. This will allow the developer to step through the code and identify the exact cause of the failure.</p> <p>21. What is the difference between a Directed Acyclic Graph (DAG) and a workflow in Airflow?</p> <p>A Directed Acyclic Graph (DAG) is a graph structure that consists of nodes and edges, where the edges represent the direction of the flow of data between the nodes. A DAG is acyclic, meaning that there are no loops or cycles in the graph. A DAG is used to represent the flow of data between tasks in a workflow.</p> <p>Airflow is a platform for programmatically authoring, scheduling, and monitoring workflows. Airflow uses DAGs to define workflows as a collection of tasks. A workflow in Airflow is a DAG that is composed of tasks that are organized in a way that reflects their relationships and dependencies. The tasks in a workflow are connected by edges that represent the flow of data between them.</p> <p>The main difference between a DAG and a workflow in Airflow is that a DAG is a graph structure that is used to represent the flow of data between tasks, while a workflow in Airflow is a DAG that is composed of tasks that are organized in a way that reflects their relationships and dependencies.</p> <p>22. How do you handle data dependencies in Airflow?</p> <p>Data dependencies in Airflow are managed using the concept of Operators. Operators are the building blocks of an Airflow workflow and are used to define tasks that need to be executed. Each Operator is responsible for a specific task and can be configured to handle data dependencies.</p> <p>For example, the PythonOperator can be used to define a task that runs a Python script. This script can be configured to read data from a source, process it, and write the results to a destination. The PythonOperator can also be configured to wait for a certain set of data to be available before executing the task.</p> <p>The TriggerRule parameter of an Operator can also be used to define data dependencies. This parameter can be used to specify the conditions that must be met before the task is executed. For example, a task can be configured to run only when a certain file is present in a certain directory.</p> <p>Finally, the ExternalTaskSensor Operator can be used to wait for the completion of a task in another DAG before executing a task. This is useful when a task in one DAG depends on the completion of a task in another DAG.</p> <p>23. How do you ensure data integrity when using Airflow?</p> <p>Data integrity is an important consideration when using Airflow. To ensure data integrity when using Airflow, I would recommend the following best practices:</p> <ul> <li>Use Airflow's built-in logging and monitoring features to track data changes and detect any anomalies. This will help you identify any potential issues with data integrity.</li> <li>Use Airflow's built-in data validation features to ensure that data is accurate and complete. This will help you ensure that data is consistent and reliable.</li> <li>Use Airflow's built-in scheduling and task management features to ensure that data is processed in a timely manner. This will help you ensure that data is up-to-date and accurate.</li> <li>Use Airflow's built-in security features to protect data from unauthorized access. This will help you ensure that data is secure and protected.</li> <li>Use Airflow's built-in data backup and recovery features to ensure that data is recoverable in the event of a system failure. This will help you ensure that data is not lost in the event of a system failure.</li> </ul> <p>By following these best practices, you can ensure that data integrity is maintained when using Airflow.</p> <p>24. How do you handle data security when using Airflow?</p> <p>When using Airflow, data security is of utmost importance. To ensure data security, I take the following steps:</p> <ul> <li>I use secure authentication methods such as OAuth2 and Kerberos to authenticate users and restrict access to the Airflow environment.</li> <li>I use encryption for data in transit and at rest. This includes encrypting data stored in databases, files, and other storage systems.</li> <li>I use secure protocols such as HTTPS and SFTP to transfer data between systems.</li> <li>I use role-based access control (RBAC) to restrict access to sensitive data and resources.</li> <li>I use logging and monitoring tools to detect and respond to security incidents.</li> <li>I use vulnerability scanning tools to identify and address potential security issues.</li> <li>I use secure coding practices to ensure that the code is secure and free from vulnerabilities.</li> <li>I use secure configuration management to ensure that the Airflow environment is configured securely.</li> <li>I use secure deployment processes to ensure that the Airflow environment is deployed securely.</li> <li>I use secure backup and disaster recovery processes to ensure that data is backed up and can be recovered in the event of a disaster.</li> </ul> <p>25. How do you ensure scalability when using Airflow?</p> <p>When using Airflow, scalability can be achieved by following a few best practices.</p> <ul> <li>First, it is important to ensure that the Airflow DAGs are designed in a way that allows them to be easily scaled up or down. This can be done by using modular components that can be reused and scaled independently. Additionally, it is important to use Airflow's built-in features such as the ability to set up multiple workers and the ability to set up multiple DAGs. This allows for the DAGs to be scaled up or down as needed.</li> <li>Second, it is important to use Airflow's built-in features to ensure that the DAGs are running efficiently. This includes using Airflow's scheduling capabilities to ensure that tasks are running at the right time and using Airflow's logging capabilities to ensure that tasks are running correctly. Additionally, it is important to use Airflow's built-in features to ensure that tasks are running in the most efficient way possible. This includes using Airflow's task retry capabilities to ensure that tasks are retried if they fail and using Airflow's task concurrency capabilities to ensure that tasks are running in parallel.</li> <li>Finally, it is important to use Airflow's built-in features to ensure that the DAGs are running securely. This includes using Airflow's authentication and authorization capabilities to ensure that only authorized users can access the DAGs and using Airflow's encryption capabilities to ensure that the data is secure.</li> </ul> <p>By following these best practices, scalability can be achieved when using Airflow.</p> <p>26. What are Variables (Variable Class) in Apache Airflow?</p> <p>Variables are a general way to store and retrieve content or settings as a simple key-value pair within Airflow. Variables in Airflow can be listed, created, updated, and deleted from the UI. Technically, Variables are Airflow's runtime configuration concept.</p> <p>27. Why don't we use Variables instead of Airflow XComs, and how are they different?</p> <p>An XCom is identified by a \"key,\" \"dag id,\" and the \"task id\" it had been called from. These work just like variables but are alive for a short time while the communication is being done within a DAG. In contrast, the variables are global and can be used throughout the execution for configurations or value sharing.</p> <p>There might be multiple instances when multiple tasks have multiple task dependencies; defining a variable for each instance and deleting them at quick successions would not be suitable for any process's time and space complexity.</p> <p>28. What are the states a Task can be in? Define an ideal task flow.</p> <p>Just like the state of a DAG (directed acyclic graph) being running is called a \"DAG run\", the tasks within that dag can have several tasks instances. they can be:</p> <ul> <li>none: the task is defined, but the dependencies are not met.</li> <li>scheduled: the task dependencies are met, has got assigned a scheduled interval, and are ready for a run.</li> <li>queued: the task is assigned to an executor, waiting to be picked up by a worker.</li> <li>running: the task is running on a worker.</li> <li>success: the task has finished running, and got no errors.</li> <li>shutdown: the task got interrupted externally to shut down while it was running.</li> <li>restarting: the task got interrupted externally to restart while it was running.</li> <li>failed: the task encountered an error.</li> <li>skipped: the task got skipped during a dag run due to branching (another topic for airflow interview, will cover branching some reads later)</li> <li>upstream_failed: An upstream task failed (the task on which this task had dependencies).</li> <li>up_for_retry: the task had failed but is ongoing retry attempts.</li> <li>up_for_reschedule: the task is waiting for its dependencies to be met (It is called the \"Sensor\" mode).</li> <li>deferred: the task has been postponed.</li> <li>removed: the task has been taken out from the DAG while it was running.</li> </ul> <p>Ideally, the expected order of tasks should be : none -&gt; scheduled -&gt; queued -&gt; running -&gt; success.</p> <p>30. What is the role of Airflow Operators?</p> <p>There are three main types of operators: - Action: Perform a specific action such as running code or a bash command. - Transfer: Perform transfer operations that move data between two systems. - Sensor: Wait for a specific condition to be met (e.g., waiting for a file to be present) before running the next task</p> <p>31. What is Branching in Directed Acyclic Graphs (DAGs)?</p> <p>Branching tells the DAG to run all dependent tasks, but you can choose which Task to move onto based on a condition. A task_id (or list of task_ids) is given to the \"BranchPythonOperator\", the task_ids are followed, and all other paths are skipped. It can also be \"None\" to ignore all downstream tasks.</p> <p>Even if tasks \"branch_a\" and \"join\" both are directly downstream to the branching operator, \"join\" will be executed for sure if \"branch_a\" will get executed, even if \"join\" is ruled out of the branching condition.</p> <p>32. What are ways to Control Airflow Workflow?</p> <p>By default, a DAG will only run an airflow task when all its Task dependencies are finished and successful. However, there are several ways to modify this:</p> <ul> <li>Branching (BranchPythonOperator): We can apply multiple branches or conditional limits to what path the flow should go after this task.</li> <li>Latest Only (LatestOnlyOperator): This task will only run if the date the DAG is running is on the current data. It will help in cases when you have a few tasks which you don't want to run while backfilling historical data.</li> <li>Depends on Past (depends_on_past = true; arg): Will only run if this task run succeeded in the previous DAG run.</li> <li>Trigger rules (\"trigger_rule\"; arg): By default, a DAG will only run an airflow task when all of its previous tasks have succeeded, but trigger rules can help us alter those conditions. Like \"trigger_rule = always\" to run it anyways, irrespective of if the previous tasks succeeded or not, OR \"trigger_rule = all_success\" to run it only when all of its previous jobs succeed.</li> </ul> <p>33. Explain the External task Sensor?</p> <p>An External task Sensor is used to sense the completion status of a DAG_A from DAG_B or vice-versa. If two tasks are in the same Airflow DAG we can simply add the line of dependencies between the two tasks. But Since these two are completely different DAGs, we cannot do this.</p> <p>We can Define an ExternalTaskSensor in DAG_B if we want DAG_B to wait for the completion of DAG_A for a specific execution date.</p> <p>There are six parameters to an External Task Sensor: - external_dag_id: The DAG Id of the DAG, which contains the task which needs to be sensed. - external_task_id: The Task Id of the task to be monitored. If set to default(None), the external task sensor waits for the entire DAG to complete. - allowed_states: The task state at which it needs to be sensed. The default is \"success.\" - execution_delta: Time difference with the previous execution, which is needed to be sensed; the default is the same execution_date as the current DAG. - execution_date_fn: It's a callback function that returns the desired execution dates to the query.</p> <p>34. What is TaskFlow API? and how is it helpful?</p> <p>We have read about Airflow XComs (cross-communication) and how it helps to transfer data/messages between tasks and fulfill data dependencies. There are two basic commands of XComs which are \"xcompull\" used to pull a list of return values from one or multiple tasks and \"xcom_push\" used for pushing a value to the Airflow XComs.</p> <p>Now, Imagine you have ten tasks, and all of them have 5-6 data dependencies on other tasks; writing an xcom_pull and x_push for passing values between tasks can get tedious.</p> <p>So TaskFlow API is an abstraction of the whole process of maintaining task relations and helps in making it easier to author DAGs without extra code, So you get a natural flow to define tasks and dependencies.</p> <p>_Note: TaskFlow API was introduced in the later version of Airflow, i.e., Airflow 2.0. So can be of minor concern in airflow interview questions.</p> <p>35. How are Connections used in Apache Airflow?</p> <p>Apache Airflow is often used to pull and push data into other APIs or systems via hooks that are responsible for the connection. But since hooks are the intermediate part of the communication between the external system and our dag task, we can not use them to contain any personal information like authorization credentials, etc. Now let us assume the external system here is referred to as a MySQL database. We do need credentials to access MySQL, right? So where does the \"Hook\" get the credentials from?</p> <p>That's the role of \"Connection\" in Airflow.</p> <p>Airflow has a Connection concept for storing credentials that are used to talk to external systems. A Connection is a set of parameters - such as login username, password, and hostname - along with the system type it connects to and a unique id called the \"conn_id\".</p> <p>If the connections are stored in the metadata database, metadata database airflow supports the use of \"Fernet\" (an encryption technique) to encrypt the password and other sensitive data.</p> <p>Connections can be created in multiple ways: - Creating them directly from the airflow UI. - Using Environment Variables. - Using Airflow's REST API. - Setting it up in the airflows configuration file itself \"airflow.cfg\". - Using airflow CLI (Command Line Interface)</p> <p>36. Explain Dynamic DAGs.</p> <p>Dynamic-directed acyclic graphs are nothing but a way to create multiple DAGs without defining each of them explicitly. This is one of the major qualities of apache airflow, which makes it a supreme \"workflow orchestration tool\".</p> <p>Let us say you have ten different tables to modify every day in your MySQL database, so you create ten DAG's to upload the respective data to their respective databases. Now think if the table names change, would you go to each dag and change the table names? Or make new dags for them? Certainly not, because sometimes there can be hundreds of tables.</p> <p>37. How to control the parallelism or concurrency of tasks in Apache Airflow configuration?</p> <p>Concurrency is the number of tasks allowed to run simultaneously. This can be set directly in the airflow configurations for all dags in the Airflow, or it can be set per DAG level. Below are a few ways to handle it: - In config :     - parallelism: maximum number of tasks that can run concurrently per scheduler across all dags.     - max_active_tasks_per_dag: maximum number of tasks that can be scheduled at once.     - max_active_runs_per_dag: . the maximum number of running tasks at once. - DAG level (as an argument to an Individual DAG) :     - concurrency: maximum number of tasks that can run concurrently in this dag.     - max_active_runs: maximum number of active runs for this DAG. The scheduler will not create new DAG runs once the limit hits.</p> <p>38. What are Macros in Airflow?</p> <p>Macros are functions used as variables. In Airflow, you can access macros via the \"macros\" library. There are pre-defined macros in Airflow that can help in calculating the time difference between two dates or more! But we can also define macros by ourselves to be used by other macros as well, like we can use a macro to dynamically generate the file path for a file. Some of the examples of pre-defined and most-used macros are: - Airflow.macros.datetimediff_for_humans(dt, _since=None): Returns difference between two datetimes, or one and now. (Since = None refers to \"now\")** - airflow.macros.dsadd(_ds, numberof__days) : Add or subtract n number of days from a YYYY-MM-DD(ds), will subtract if number_of_days is negative.</p> <p>39. List the types of Trigger rules.</p> <ul> <li>all_success: the task gets triggered when all upstream tasks have succeeded.</li> <li>all_failed: the task gets triggered if all of its parent tasks have failed.</li> <li>all_done: the task gets triggered once all upstream tasks are done with their execution irrespective of their state, success, or failure.</li> <li>one_failed: the task gets triggered if any one of the upstream tasks gets failed.</li> <li>one_success: the task gets triggered if any one of the upstream tasks gets succeeds.</li> <li>none_failed: the task gets triggered if all upstream tasks have finished successfully or been skipped.</li> <li>none_skipped: the task gets triggered if no upstream tasks are skipped, irrespective of if they succeeded or failed.</li> </ul> <p>40. What are SLAs?</p> <p>SLA stands for Service Level Agreement; this is a time by which a task or a DAG should have succeeded. If an SLA is missed, an email alert is sent out as per the system configuration, and a note is made in the log. To view the SLA misses, we can access it in the web UI.</p> <p>It can be set at a task level using the \"timedelta\" object as an argument to the Operator, as sla = timedelta(seconds=30).</p> <p>41. What is Data Lineage?</p> <p>Many times, we may encounter an error while processing data. To determine the root cause of this error, we may need to track the path of the data transformation and find where the error occurred. If we have a complex data system then it would be challenging to investigate its root. Lineage allows us to track the origins of data, what happened to it, and how did it move over time, such as in S3, HDFS, MySQL or Hive, etc. It becomes very useful when we have multiple data tasks reading and writing into storage. We need to define the input and the output data sources for each task, and a graph is created in Apache Atlas, which depicts the relationships between various data sources.</p> <p>42. What if your Apache Airflow DAG failed for the last ten days, and now you want to backfill those last ten days' data, but you don't need to run all the tasks of the dag to backfill the data?</p> <p>We can use the Latest Only (LatestOnlyOperator) for such a case. While defining a task, we can set the latest_only to True for those tasks, which we do not need to use for backfilling the previous ten days' data.</p> <p>43. What will happen if you set 'catchup=False' in the dag and 'latest_only = True' for some of the dag tasks?</p> <p>Since in the dag definition, we have set catchup to False, the dag will only run for the current date, irrespective of whether latest_only is set to True or False in any one or all the tasks of the dag. 'catchup = False' will just ensure you do not need to set latest_only to True for all the tasks.</p> <p>44. How would you handle a task which has no dependencies on any other tasks?</p> <p>We can set \"trigger_rules = 'always'\" in a task, which will make sure the task will run irrespective of if the previous tasks have succeeded or not.</p> <p>45. How can you use a set or a subset of parameters in some of the dags tasks without explicitly defining them in each task?</p> <p>We can use the \"params\" argument. It is a dictionary of DAG-level parameters that are made accessible in jinja templates. These \"params\" can be used at the task level. We can pass \"params\" as a parameter to our dag as a dictionary of parameters such as {\"param1\": \"value1\", \"param2\": \"value2\"}. And these can be used as \"echo {{params.param1}}\" in a bash operator.</p> <p>46. What Executor will you use to test multiple jobs at a low scale?</p> <p>Local Executor is ideal for testing multiple jobs in parallel for performing tasks for a smallscale production environment. The Local Executor runs the tasks on the same node as the scheduler but on different processors. There are other executors as well who use this style while distributing the work. Like, Kubernetes Executor would also use Local Executor within each pod to run the task.</p> <p>47. If we want to exchange large amounts of data, what is the solution to the limitation of XComs?</p> <p>Since Airflow is an orchestrator tool and not a data processing framework, if we want to process large gigabytes of data with Airflow, we use Spark (which is an open-source distributed system for large-scale data processing) along with the Airflow DAGs because of all the optimizations that It brings to the table.</p> <p>48. What would you do if you wanted to create multiple dags with similar functionalities but with different arguments?</p> <p>We can use the concept of Dynamic DAGs generation. We can define a create_dag method which can take a fixed number of arguments, but the arguments will be dynamic. The dynamic arguments can be passed to the create_dag method through Variables, Connections, Config Files, or just passing a hard-coded value to the method.</p> <p>49. Is there any way to restrict the number of variables to be used in your directed acyclic graph, and why would we need to do that?</p> <p>Airflow Variables are stored in the Metadata Database, so any call to a variable would mean a connection to the database. Since our DAG files are parsed every X seconds, using a large number of variables in our DAG might end up saturating the number of allowed connections to our database. To tackle that, we can just use a single Airflow variable as a JSON, as an Airflow variable can contain JSON values such as {\"var1\": \"value1\", \"var2\": \"value2\"}.</p> <p>50. How can you use a set or a subset of parameters in some of the dags tasks without explicitly defining them in each task?</p> <p>We can use the \"params\" argument. It is a dictionary of DAG-level parameters that are made accessible in jinja templates. These \"params\" can be used at the task level. We can pass \"params\" as a parameter to our dag as a dictionary of parameters such as {\"param1\": \"value1\", \"param2\": \"value2\"}. And these can be used as \"echo {{params.param1}}\" in a bash operator.</p>"},{"location":"airflow/dataorchestration/","title":"Data Orchestration","text":"<p>To support business continuity, data models need to be regularly refreshed. In the past, engineers used the cron tool in Linux systems to schedule ELT jobs.</p> <p>However, as data volume and system complexity increase exponentially, creating cron jobs becomes a bottleneck and eventually hits the limitation of scalability and maintainability.</p> <p>To be more specific, the problems are:</p> <ul> <li>Dependencies between jobs: In a large-scale data team, it's expected to have many dependencies between data models.</li> </ul> <p>Example</p> <p>updating the revenue table should be done only after the sales table has been updated.</p> <p>In more complicated scenarios, a table can have multiple upstream dependencies, each with a different schedule. Managing all these dependencies manually is time-consuming and error-prone.</p> <ul> <li> <p>Performance: If not managed well, cron jobs can consume a significant amount of system resources such as CPU, memory, and disk space. With the ever-increasing volume of data, performance can quickly become an issue.</p> </li> <li> <p>Engineering efforts: To maintain the quality of dozens of cron jobs or apps and process a variety of data formats, data engineers have to spend a significant amount of time writing low-level code rather than creating new data pipelines.</p> </li> <li> <p>Data silos: Scattered cron jobs can easily lead to data silos, resulting in duplicated efforts, conflicting data, and inconsistent data quality. Enforcing data governance policies can also be difficult, leading to potential security issues.</p> </li> </ul> <p>The emergence of data orchestration marks a significant step in the evolution of modern data stacks.</p> <p>Data orchestration is an automated process that combines and organizes data from multiple sources, making it ready for use by data consumers. Orchestrators ease the workload of data engineering teams by providing prebuilt solutions for scheduling, monitoring, and infrastructure setup.</p>"},{"location":"airflow/dataorchestration/#tasks-of-data-orchestration","title":"Tasks of data orchestration","text":"<p>The actual tasks of data orchestration can vary from system to system, but essentially it consists of 3 parts as described:</p> <ul> <li>Data collection</li> <li>Data unification</li> <li>Data activation</li> </ul> <p>Note</p> <p>it's worth noting that data orchestration is not a database engine.</p> <p>It's a platform that schedules different jobs to run at the right time, in the right order and in the right way.</p> <p>It's common for a company to use multiple data orchestration platforms. This is because each platform may perform different tasks and target different users.</p> <p>Example</p> <p>For instance, tools like Airflow and Prefect are used for creating and managing complex workflows and are, therefore, mostly used by data engineers.</p> <p>On the other hand, **dbt* is focused on the data unification stage and is heavily adopted by data analysts and data scientists.</p> <p>Note</p> <p>One of the challenges of using multiple data orchestration platforms is managing the dependency and lineage between the platforms, if any exist.</p> <p>Fortunately, tool integration is becoming increasingly common.</p>"},{"location":"airflow/dataorchestration/#data-collection","title":"Data collection","text":"<p>Within an organization, data may come from multiple sources and in various formats. Adapting every single format from every single source can be time-consuming. Data orchestration automates the process of collecting data from disparate sources with little to no human effort.</p> <p>Example</p> <p>Many data orchestration platforms have easy integration with various tools such as Google Sheets, CSV files, BigQuery, Zendesk support, which speeds up the onboarding process of a new data source.</p> <p>This is expandable to other complex data format, such as Parquet, Avro, ORC, or delta format.</p> <p>Data orchestration provides the additional advantage of reducing data silos. The platform can swiftly access data from anywhere, whether it's from legacy systems, data warehouses, or the cloud.</p> <p>This prevents data from being trapped in a single location and makes it easily accessible.</p>"},{"location":"airflow/dataorchestration/#data-unification","title":"Data unification","text":"<p>Data inevitably needs to be unified and converted into a standard format.</p> <p>There are three types of platforms in this category, and all the platforms allow users to schedule jobs.</p> <ul> <li>Platforms that don't interfere with the data transformation logic.   Users can use any method to transform data, such as SQL, Python, or Bash scripts.</li> </ul> <p>Example</p> <p>Airflow is only responsible for submitting the data transformation job created by the user to a data warehouse and waiting for the result. It doesn't care how the user implements the job.</p> <ul> <li> <p>Platforms that manage the data transformation logic.   Platforms like dbt define their own way of transforming data and managing model dependency within the tool. They guide users to use optimized methods to transform data.</p> </li> <li> <p>Platforms that manage the data transformation logic through UI.   Certain low-code or no-code tools only require users to define transformation logic through user-friendly UI. These tools open the doors to non-engineers, but their functionalities may be limited.</p> </li> </ul>"},{"location":"airflow/dataorchestration/#data-activation","title":"Data activation","text":"<p>Business users who use the dashboards can define Service Level Agreements (SLA), meaning that the data must be ready before a certain time.</p> <p>Note</p> <p>If not, the orchestrator will notify the stakeholders through one of the communication channels.</p> <p>In addition to the above steps, orchestration platforms monitor the data life cycle in real time through a UI, where users can manually intervene with the process if needed.</p> <p>Another critical feature is dependency management. Before going into specific platforms, it's important to understand an important concept in data orchestration: directed acyclic graph (DAG).</p>"},{"location":"airflow/dataorchestration/#directed-acyclic-graph-dag","title":"Directed Acyclic Graph (DAG)","text":"<p>DAG is the bedrock of managing dependencies between different tasks.</p> <p>A DAG is a graphical representation of a series of tasks in the data pipeline. By its name, we can tell that it is a graph, but with two conditions:</p> <ul> <li> <p>Directed: Every edge in the graph points in one direction.</p> </li> <li> <p>Acyclic: The graph doesn't have directed cycles.</p> </li> </ul> <p>Considered the DAG below. In this DAG, data can go from A to B, but never from B to A. Nodes from which an edge extends are upstream nodes, while nodes at the receiving end of the edge are downstream nodes. In a DAG, nodes can have multiple upstream and downstream nodes.</p> <p></p> <p>Danger</p> <p>In addition, nodes can never inform themselves because this could create an infinite loop.</p> <p>For example, data cannot go from E to A. Otherwise, A becomes the downstream of E as well as the upstream.</p>"},{"location":"airflow/dataorchestration/#advantages","title":"Advantages","text":"<p>DAG is a fundamental concept in data orchestration for a few reasons:</p> <ul> <li> <p>It ensures that there is no infinite loop in the data pipeline. The scenario where the pipeline could run forever will not happen.</p> </li> <li> <p>It ensures the consistent execution order of tasks. Tasks will be executed in the same order every day.</p> </li> <li> <p>The graph helps us to visualize the dependencies in a user-friendly way. We can break down a complex task into smaller subtasks, making it easier to debug and maintain.</p> </li> <li> <p>It opens the possibility of parallelism. Independent tasks can be executed in parallel. For example, nodes B and C can run simultaneously.</p> </li> </ul> <p>Info</p> <p>In general, data orchestration connects the dots and enables data to flow in a consistent and manageable way.</p> <p>With a solid orchestration platform in place, organizations can quickly and efficiently scale their data volumes and deliver high-quality data to stakeholders for better decision-making.</p>"},{"location":"cloud/databricks/","title":"Overview","text":""},{"location":"cloud/databricks/#challenges-in-a-normal-day-to-day-data-platform","title":"Challenges in a Normal Day-to-Day Data Platform","text":"<p>--- Too Many Tools and Integration Issues</p> <p>Organizations often use a multitude of separate tools for different tasks, such as data warehousing, ETL (Extract, Transform, Load), running Spark jobs, saving data to data lakes, orchestration, AI/ML solutions, and BI (Business Intelligence) reporting.</p> <p>Each of these tools needs to integrate properly with one another to function effectively. For instance, if a BI dashboard doesn't integrate with a data warehousing solution, proper results cannot be obtained.</p> <p>Furthermore, governance (handling security, lineage, and metadata) must work across all these tools; otherwise, data leaks and security issues can arise. The complexity and challenges increase significantly when dealing with numerous individual tools.</p> <p>--- Proprietary Solutions or Vendor Lock-in</p> <p>Many data warehousing solutions are proprietary, meaning they require data to be stored in their specific, encoded format. This creates a vendor lock-in, preventing direct communication with or extraction of data without using the vendor's data engine. If the vendor's solution is not used, accessing and reading the data becomes impossible.</p> <p>Databricks addresses this by providing open-source solutions. Data can reside in an organization's data cloud platform in open-source formats like Parquet, CSV, or Avro.</p> <p>On top of this, Databricks uses an open-source data engine called Delta Lake, which communicates with the data. This allows users the freedom to switch vendors if desired, as their data remains accessible in an open format within their data lake, ensuring no vendor lock-in.</p> <p>--- Data Silos</p> <p>Traditional platforms often have separate data lakes (used for AI/ML solutions and ETL jobs) and data warehousing solutions (used by BI tools). This leads to duplicate copies of data. Data is often moved from the data lake to populate a separate data warehouse, resulting in the same data existing in different places, possibly with different owners.</p> <p>Maintaining multiple copies by different owners is a significant challenge. Databricks tackles this by merging the data lake and data warehouse into a Data Lakehouse</p> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/databricks/#the-data-lakehouse","title":"The Data Lakehouse","text":"<p>A datalakehouse is new, open data management architecture that combines the flexibity, cost efficiency and scale of datalakes with data management and ACID transactions of Data Warehouses, enabling business intelligence and machine learning on all data.</p> <p>--- Databricks Data Intelligence Platform</p> <p>The Databricks Data Intelligence Platform is defined as Data Lakehouse plus Generative AI. Generative AI provides the platform with its power for natural language and allows enterprises to gain insights from their enterprise data. Therefore, Databricks is called a data intelligence platform because it combines the benefits of a Data Lakehouse with Generative AI capabilities.</p> <p>--- Delta Lakehouse features</p> <ol> <li>Handles All Types of Data</li> <li>Cheap Cloud Object Storage</li> <li>Uses Open File Format</li> <li>Support for All Workloads</li> <li>Direct BI Integration</li> <li>ACID Support &amp; Version History</li> <li>Improved Performance</li> <li>Simple Architecture</li> </ol> <p>--- Disadvantages for datawarehouse</p> <ol> <li>Increased Data Volume &amp; Variety</li> <li>Longer Time to Ingest New Data</li> <li>Proprietary Data Formats /Vendor Lock-in</li> <li>Scalability Issues</li> <li>High Storage Costs</li> <li>Limited Support for Advanced Analytics</li> </ol> <p>--- Disadvantages for datalake</p> <ol> <li>No Support for ACID Transactions(Atomicity, Consistency, Isolation, and Durability)</li> <li>Partial Data Loads</li> <li>Inconsistent Reads</li> <li>GDPR Challenges(general data protection regulation)</li> <li>Complicated Data Corrections</li> <li>No Rollback Capability</li> <li>Poor BI Performance and Support</li> <li>Complex to set-up</li> <li>No Data Versioning</li> <li>Streaming vs. Batch Processing</li> </ol> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/databricks/#databricks-lakehouse-platform-architecture","title":"Databricks Lakehouse Platform Architecture","text":"<p>The high-level architecture of Databricks consists of two main parts:</p> <p>--- Control Plane</p> <p>The control plane is managed by Databricks and resides within the Databricks cloud account. Its primary purpose is to manage Databricks' backend services. It also handles information like notebook configurations, cluster configurations, job information, and logs required to manage the data plane. The main purpose of the control plane is to orchestrate and provide configurations necessary to run jobs, clusters, and code.</p> <p>--- Data/Compute Plane(Serverless)</p> <p>The data plane resides within the customer's cloud account. Client data always resides in the customer's cloud account within the data plane, never at the control plane. Clusters created to process this data are also created and run within the customer's cloud account (data plane). These clusters are managed by configurations from the control plane.</p> <p>Processed data is saved back to the client's cloud account only. There is no data movement to the control plane; configurations and access are managed by the control plane, but data remains in the data plane. If a cluster needs to connect to external data sources (e.g., MySQL, a different data lake), it will connect and process that data within the data plane.</p> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/databricks/#databricks-workspace-components","title":"Databricks Workspace Components","text":"<p>Databricks is composed of several main components that work together to provide a comprehensive data analytics and AI platform.</p> <p>--- Apache Spark</p> <p>At its core, Databricks is built on Apache Spark, a powerful open-source, distributed computing system that provides fast data processing and analytics capabilities. Databricks enhances Spark with optimized performance and additional features.</p> <p>--- Databricks Workspace</p> <p>This is a collaborative environment where data scientists, data engineers, and analysts can work together. It includes interactive notebooks (supporting languages like Python, Scala, SQL, and R), dashboards, and APIs for collaborative development and data exploration.</p> <p>--- Databricks Runtime</p> <p>A performance-optimized version of Apache Spark with enhancements for reliability and performance, including optimizations for cloud environments and additional data sources.</p> <p>--- Delta Lake</p> <p>An open-source storage layer that brings reliability to Data Lakes. Delta Lake provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing.</p> <p>--- Workflow</p> <p>Databricks Workflows simplify job orchestration, allowing you to create, schedule, and manage data pipelines using a no-code or low-code interface. They support tasks like ETL, machine learning, and batch or streaming workflows, ensuring seamless integration with Databricks Jobs and other tools.</p> <p>--- Databricks SQL</p> <p>A feature for running SQL queries on your data lakes. It provides a simple way for analysts and data scientists to query big data using SQL, visualize results, and share insights.</p> <p>--- SQL Warehouses</p> <p>Databricks SQL Warehouse is a scalable, cloud-native data warehouse that supports high-performance SQL queries on your data lake. It enables analytics and BI reporting with integrated tools like Power BI and Tableau, ensuring fast, cost-efficient query execution with fully managed infrastructure.</p> <p>--- Catalog</p> <p>Databricks Catalog provides centralized governance, organizing your data and metadata.</p> <p>--- Data Integration Tools</p> <p>These allow for easy integration with various data sources, enabling users to import data from different storage systems.</p> <p>--- Databricks compute configuration</p> <ol> <li> <p>Severless</p> <p>reduce cluster start time, increase productivity ,expected lower cost due to reduced ideal time and autoscaling, maintainaing cluster for administration.</p> </li> <li> <p>Classical compute (self managed)</p> <p>all purpose cluster -&gt; created manually,persistent,suitable for interactive analytical workloads,shared among many users, expensive to run</p> <p>job cluster -&gt; created by jobs, terminated at end of job, suitable for automated workloads, isolated just for the job, cheaper to run</p> </li> </ol> <p>Standard Access Mode clusters do not support Scala UDFs</p> <p>--- Databricks Cluster configuration</p> <ol> <li> <p>Node type </p> <p>Single node - not suitable for large scale workloads, incompatible with shared usage</p> <p>Multi node - shared compute is needed 2. Access mode  </p> <p>dedicated/single user - only single user access</p> <p>standard/shared - multiple user access (used for production)</p> <p>no isolation shared - multiple user access</p> </li> <li> <p>Datbricks runtime  </p> <p>databricks runtime - photon(vectorized query enginer which accelerates apache spark workloads)</p> <p>databricks runtime ML</p> </li> <li> <p>Auto termination</p> <p>Terminates the cluster after X minutes of inactivity</p> <p>Default value for Single Node and Standard clusters is 120 minutes</p> <p>Users can specify a value between 10 and 43200 mins as the duration</p> </li> <li> <p>Autoscaling</p> <p>User specifies the min and max worker nodes</p> <p>Auto scales between min and max based on the workload</p> <p>Users can opt for spot instances(unused VM or spare capacity in the cloud) for the work</p> </li> <li> <p>Cluster vm type/size</p> <p>Memory Optimized - suitable for memory intensive like ML</p> <p>Storage Optimized - high disk throuput IO</p> <p>Compute Optimized - ideal for structured streaming where peak time processing are criticial and distributed analytics</p> <p>General Purpose - enterprised grade application for analytical workloads in memory caching</p> <p>GPU Accelerated - deep learning models</p> </li> <li> <p>Cluster policy</p> <p>restricted</p> <p>unrestricted</p> </li> </ol> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/databricks/#unity-catalog","title":"Unity Catalog","text":"<p>Unity Catalog is a unified governance solution for managing and securing your data assets in Databricks.</p> <p>--- Key features of Unity Catalog</p> <p>Databricks Catalog provides centralized governance, organizing your data and metadata.</p> <p>Unity Catalog\u2019s security model is based on standard ANSI SQL and allows administrators to grant permissions in their existing data lake using familiar syntax, at the level of catalogs, schemas (also called databases), tables, and views.</p> <p>Unity Catalog automatically captures user-level audit logs that record access to your data. Unity Catalog also captures lineage data that tracks how data assets are created and used across all languages.</p> <p>Unity Catalog lets you easily access and query your account\u2019s operational data, including audit logs, billable usage, and lineage</p> <p>--- The Unity Catalog object model</p> <p>In Unity Catalog, all metadata is registered in a metastore. The hierarchy of database objects in any Unity Catalog metastore is divided into three levels, represented as a three-level namespace (catalog.schema.table-etc) when you reference tables, views, volumes, models, and functions.</p> <p>Metastores: The metastore is the top-level container for metadata in Unity Catalog. It registers metadata about data and AI assets and the permissions that govern access to them. For a workspace to use Unity Catalog, it must have a Unity Catalog metastore attached.</p> <p>DBRuntime requires  - 11.3 onwards to work with unity catalog</p> <p>No isolation shared doesnt support unity catalog</p> <p>Object hierarchy in the metastore: In a Unity Catalog metastore, the three-level database object hierarchy consists of catalogs that contain schemas, which in turn contain data and AI objects, like tables and models.</p> <ol> <li> <p>Level one</p> <p>Catalogs are used to organize your data assets and are typically used as the top level in your data isolation scheme. Catalogs often mirror organizational units or software development lifecycle scopes.</p> <p>Non-data securable objects, such as storage credentials and external locations, are used to manage your data governance model in Unity Catalog. These also live directly under the metastore.</p> </li> <li> <p>Level two</p> <p>Schemas (also known as databases) contain tables, views, volumes, AI models, and functions. Schemas organize data and AI assets into logical categories that are more granular than catalogs. Typically a schema represents a single use case, project, or team sandbox.</p> </li> <li> <p>Level three</p> <p>Volumes is essentially a mapping between a cloud storage directory and Databricks\u2019 Unity Catalog. It allows you to securely manage and access data stored in cloud object storage directly within Databricks. Tables are collections of data organized by rows and columns. Views are saved queries against one or more tables. Functions are units of saved logic that return a scalar value or set of rows. Models are AI models packaged with MLflow and registered in Unity Catalog as functions.</p> </li> </ol>"},{"location":"cloud/databricks/#auto-loader","title":"Auto Loader","text":"<p>A new structured streaming source designed for large-scale, efficient data ingestion. Incrementally and efficiently processes new data files as they arrive in the cloud storage</p> <p>Supports Azure Data Lake Storage, Amazon S3, Google Cloud Storage, Databricks File System. Supports JSON, CSV, XML, PARQUET, AVRO, ORC, TEXT, and BINARYFILE file formats</p> <p>--- Auto Loader features</p> <ol> <li>Efficient File Detection using Cloud Services</li> <li>Scalability Improvements</li> <li>Schema Evolution &amp; Resiliency</li> <li>Recommended in Lakeflow Declarative Pipelines</li> </ol>"},{"location":"cloud/databricks/#databricks-asset-bundles","title":"Databricks Asset Bundles","text":"<p>Databricks Asset Bundles are a tool to facilitate the adoption of software engineering best practices, including source control, code review, testing, and continuous integration and delivery (CI/CD), for your data and AI projects.</p> <p>Collection of:     Code (notebooks, Python, SQL)     Environment settings (dev, test, prod)     Configurations for Databricks resources (jobs, pipelines, clusters, MLflow, etc.)</p> <p>Represented in YAML</p> <p>Deployed through CI/CD tools like GitHub Actions, Azure DevOps</p> <p>Provides a standard, repeatable way to deliver Databricks projects</p> <p>--- Structure of bundle</p> <ol> <li>bundle \u2192 project name &amp; metadata</li> <li>resources \u2192 jobs, pipelines, clusters, MLflow, etc.</li> <li>targets \u2192 environment specific settings (dev, test, prod)</li> <li>variables \u2192 define reusable values (like parameters)</li> <li>include \u2192 pull in configs from other files (for modular bundles)</li> <li>run_as \u2192 specify the identity (user or service principal) to run jobs</li> <li>artifacts \u2192 reference libraries, wheels, or other external assets</li> <li>sync \u2192 control what gets synced between local and workspace</li> </ol> <p>--- Sample YAML</p> <pre><code># Databricks Asset Bundle for dab_demo_project\nbundle:\n  name: dab_demo_project\n\nresources:\n  jobs:\n    dab_demo_project_job:\n      name: dab_demo_project_job\n      tasks:\n        - task_key: dab_demo_notebook\n          notebook_task:\n            notebook_path: src/dab_demo_notebook.ipynb\n          job_cluster_key: job_cluster\n\n      job_clusters:\n        - job_cluster_key: job_cluster\n          new_cluster:\n            spark_version: 15.4.x-scala2.12\n            node_type_id: Standard_D3_v2\n            data_security_mode: SINGLE_USER\n            num_workers: 1\n\ntargets:\n  dev:\n    default: true\n    mode: development\n    workspace:\n      host: https://adb-12345.14.azuredatabricks.net\n\n  prod:\n    workspace:\n      host: https://adb-67890.14.azuredatabricks.net\n</code></pre> <p>--- Databricks CLI commands</p> <ol> <li>databricks bundle init Initializes a new Databricks bundle project in your working directory.</li> </ol> <p>databricks bundle init Prompts you for a template (Python, SQL, MLflow, etc.).</p> <p>Creates a starter databricks.yml file and project folder structure.</p> <ol> <li>databricks bundle validate Validates the configuration of your bundle before deployment.</li> </ol> <p>databricks bundle validate Ensures your databricks.yml file is correctly structured.</p> <p>Catches missing or invalid fields early.</p> <p>Always a good practice to run before deploying.</p> <ol> <li>databricks bundle deploy Deploys your bundle to the Databricks workspace.</li> </ol> <p>databricks bundle deploy -t dev -t specifies the target environment (e.g. dev, test, prod).</p> <p>Uploads source code and provisions jobs, clusters, pipelines, etc.</p> <p>Creates a hidden .bundle folder in the workspace with your files.</p> <ol> <li>databricks bundle run Runs a defined workflow (job) from your bundle.</li> </ol> <p>databricks bundle run my_job Executes a specific job defined in the bundle.</p> <p>Useful for testing deployments.</p> <ol> <li>databricks bundle destroy Removes all deployed resources from the specified target environment.</li> </ol> <p>databricks bundle destroy -t dev Cleans up jobs, clusters, and other resources created by the bundle.</p> <p>Helpful when resetting environments or avoiding conflicts in shared workspaces.</p>"},{"location":"cloud/databricks/#delta-lake","title":"Delta Lake","text":"<p>Delta Lake is the optimized storage layer that provides the foundation for tables in a Lakehouse on Databricks. Delta Lake is open source software that extends Parquet data files with a file-based transaction log for ACID transactions and scalable metadata handling.</p> <p>It allows you to use a single copy of data for both batch and streaming operations and providing incremental processing at scale.</p> <p>--- Delta Lake features</p> <ol> <li>ACID Transactions</li> <li>Scalable Metadata</li> <li>Time Travel</li> <li>Simple Solution Architecture</li> <li>Support for DML Operations</li> <li>Better Performance</li> <li>Open Source</li> </ol> <p>--- ACID transaction</p> <ol> <li>Transaction Logs are written at the end of the transaction</li> <li>Readers will always read the transaction logs first to identify the list of data files to read</li> </ol> <p>--- Delta Lake compaction</p> <p>Process of combining many smaller files into few larger files to improve performance and</p> <ol> <li>optimize storage.</li> <li>Faster Reads</li> <li>Efficient Storage</li> </ol> <p>Compaction types:-     File Compaction (OPTIMIZE Command)     Z-Order Compaction (ZORDER BY Clause)     Liquid Clustering (Preview Feature)</p> <p>--- Liquid Clustering</p> <p>Liquid Clustering is an innovative data layout technique in Delta Lake that automatically manages how data is organized to improve performance and cost efficiency.</p> <p>Motivation for LC:-     Hard to design right partitioning &amp; z-ordering strategy     Leads to data skew and concurrency conflicts     Performance drops as data grows and workloads change     Re-partitioning required rewriting the entire data</p> <p>Data is divided into clusters based on the clustering keys. As new data arrives, databricks monitors and rebalances these clusters automatically.</p> <p>No manual re-organization or z-ordering required Changing the Clustering columns doesn\u2019t require re-writing  entire data</p> <p>Benefits:-     Faster Queries     Automatic Optimization     Adaptive Data Layout     Lower maintenance cost     Simpler Operations</p>"},{"location":"cloud/databricks/#delta-live-tables","title":"Delta Live Tables","text":"<p>Delta Live Tables is a declarative ETL framework for building reliable, maintainable, and testable data processing pipelines.</p> <p>You define the transformations to perform on your data and Delta Live. Tables manages task orchestration, cluster management, monitoring,data quality, and error handling.</p> <p>It handles both streaming and batch workloads with minimal manual intervention.</p>"},{"location":"cloud/overview/","title":"Overview","text":"Service Azure AWS GCP CI/CD Azure DevOps, GitHub Enterprise AWS Code Build, AWS Code Deploy, AWS Code Pipeline Google Cloud Build, Google Cloud Deploy Data warehouse Azure Synapse Analytics Amazon Redshift BigQuery Data Integration Azure Data Factory AWS Glue, Amazon Data Pipeline Google Cloud Data Fusion Messaging Azure Service Bus, Azure Event Hubs AWS Kinesis, Amazon SNS, Amazon SQS Google Pub/Sub Workflow orchestration Azure Data Factory Amazon Data Pipeline, AWS Glue, Apache Airflow Cloud Composer Document data Azure Cosmos DB Amazon DocumentDB Firestore NoSQL - Key/Value Azure Cosmos DB Amazon DynamoDB Cloud Bigtable RDBMS Azure SQL Database Amazon Aurora, Amazon RDS Cloud SQL Storage Transfer Azure Data Factory, Azure Storage Mover AWS Storage Gateway, AWS Data Sync Storage Transfer Service Network connectivity Azure Virtual Private Network AWS Virtual Private Network Cloud VPN Audit logging Azure Audit Logs AWS CloudTrail Cloud Audit Logs Key management Azure Key Vault AWS KMS Cloud KMS Identity Azure Identity Management AWS IAM Google Cloud IAM Storage Service Azure Blob Storage Amazon S3 Google Cloud Storage"},{"location":"cloud/unitycatalog/","title":"Unitycatalog","text":""},{"location":"cloud/unitycatalog/#data-governance","title":"Data Governance","text":"<p>Data governance is a strategic approach to managing data within an organization, ensuring that data is accurate, secure, and used responsibly. It involves the development and enforcement of policies and procedures to control data across various stages of its lifecycle\u2014from ingestion and storage to processing and sharing. </p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/unitycatalog/#five-pillars-of-data-governance","title":"Five Pillars of Data Governance","text":"<p>Unity Catalog provides five major capabilities that enable robust data governance across Databricks workspaces:</p> <ol> <li> <p>Access Control: This feature allows administrators to control precisely who can see which data and objects (tables, schemas, catalogs). This ensures that sensitive data (e.g., 10 highly sensitive tables out of 100) are only visible to authorized senior personnel, such as senior data engineers or analysts.</p> </li> <li> <p>Auditing: UC allows administrators to track and review all activities within the catalog, including which users are running queries, what queries are being executed, and which queries are using the maximum compute resources. This provides necessary transparency.</p> </li> <li> <p>Lineage (Diagram/Example): This feature tracks the data flow, showing the source (upstream) and consumption (downstream) of data objects.</p> <p>Example</p> <p>If a Gold layer table is reporting incorrect numbers, lineage allows the engineer to quickly track back the dependencies: identifying that Gold Table A uses Silver Table X, which in turn uses Bronze Table Y, quickly identifying the source of the issue. This drastically reduces investigation time from days to hours.</p> </li> <li> <p>Quality Monitoring: Users can create monitoring dashboards (similar to KPIs) to track the behavior and performance of their queries and data quality metrics.</p> </li> <li> <p>Data Discovery: This feature acts as a central repository for metadata, allowing users to easily find crucial information about tables, such as data types, table type (managed or external), and storage location, eliminating the need to manually remember details for hundreds of tables.</p> </li> </ol> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/unitycatalog/#what-is-unity-catalog","title":"What is Unity Catalog","text":"<p>In the context of data, governance means ensuring your data is secure, available, and accurate. Unity Catalog is the feature in Databricks that provides this capability.</p> <p>Unity Catalog offers a \"unified\" governance model, which means you \"define it once and secure it everywhere\". This is a key benefit, making it a popular feature in Databricks.</p> <p>Without Unity Catalog, each Databricks workspace must be managed individually, including its users, metastore, and compute resources.</p> <p>With Unity Catalog, you get centralized governance, allowing you to manage all your workspaces from a single, account-level location. This simplifies administration, as you can manage users and metastores at the account level and then assign them to specific workspaces.</p> <p>Features</p> <p>Unity Catalog is a centralized, open-source solution that provides security, auditing, data lineage, and data discovery capabilities. Because it's open source, the code is publicly available on GitHub.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/unitycatalog/#architecture","title":"Architecture","text":"<p>Unity Catalog utilizes a four-layered hierarchy, which simplifies governance and organization across Databricks workspaces.</p> <ol> <li> <p>Metastore (Level 0): The top-level container for metadata. Also known as repository to store metadata. Can only create 1 metastore per region.</p> </li> <li> <p>Catalog (Level 1): Equivalent to a traditional database.</p> </li> <li> <p>Schema (Level 2): Also known as databases, schemas contain the data objects. A common industry practice is organizing schemas by Medallion layers (Bronze, Silver, Gold).</p> </li> <li> <p>Data Objects (Level 3): These are the resources stored within the schema, including Tables, Views, Volumes, and Functions.</p> </li> </ol> <p>The Metastore is the central repository where metadata about data and AI assets (models, permissions) are registered.</p> <p>Note</p> <p>\u2022 Requirement: For any Databricks workspace to use Unity Catalog, a Unity Catalog Metastore must be attached.</p> <p>\u2022 Region Constraint: Only one Metastore can be created per region where you have workspaces.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/unitycatalog/#comparison-databricks-architecture-pre-uc-vs-post-uc","title":"Comparison: Databricks Architecture (Pre-UC vs. Post-UC)","text":"<p>Unity Catalog fundamentally revolutionized Databricks architecture by addressing limitations inherent in the previous approach (often Hive Metastore).</p> <p></p> Feature Pre-Unity Catalog (Hive Metastore) Post-Unity Catalog (UC Metastore) Metastore Count Multiple Metastores required, typically one default Metastore per workspace. One global Metastore manages all associated workspaces in that region. Data Storage (Managed Tables) Managed data was distributed across multiple storage accounts, requiring the management of \"N\" number of ADLS Gen 2/S3 accounts. Managed data is stored in a single, unified storage account linked to the Metastore. Data Sharing Managed tables/metadata could not be easily shared directly across different workspaces. Managed tables can be shared across all attached Databricks workspaces. Governance Lacked centralized governance, lineage, and auditing capabilities provided by UC. Centralized data governance, lineage tracking, and comprehensive auditing are standard. <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/unitycatalog/#key-architectural-changes","title":"Key Architectural Changes","text":"<p>There are three main components to the changes implemented by Unity Catalog:</p> <ol> <li> <p>Centralized user and group management: Unity Catalog utilizes the account console for managing users and groups, which can then be assigned to multiple workspaces. This approach means that user and group definitions are consistent across all workspaces assigned to Unity Catalog.</p> </li> <li> <p>Separation of metastore management: Unlike the workspace-specific metastore used previously. Unity Catalog\u2019s metastores are managed centrally per cloud region through the account console. A single Unity Catalog metastore can serve multiple workspaces, allowing them to share the same underlying storage. This consolidation simplifies data management, improves data accessibility, and reduces data duplication, as multiple workspaces can access the same data without needing to replicate it across different environments.</p> </li> <li> <p>Centralized access control: Access controls within Unity Catalog are controlled centrally and apply across all workspaces. This ensures that defined policies and permissions are enforced consistently across the organization, thereby enhancing overall security</p> </li> </ol> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/unitycatalog/#the-three-level-namespace","title":"The Three-Level Namespace","text":"<p>A key concept introduced with Unity Catalog is the three-level namespace for accessing data.</p> <p>To access a table or view, you must specify its full path using the format: catalog_name.schema_name.table_name.</p> <p>Example</p> <p>If you have a table named sales under a schema named bronze in a catalog named dev, you would access it using <code>dev.bronze.sales</code>. This structure ensures data is securely accessed based on its specific catalog and schema location.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/unitycatalog/#uc-security-model","title":"UC security Model","text":"<p> -------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/unitycatalog/#managed-storage-location-hierarchy","title":"Managed Storage Location Hierarchy","text":"<p>Unity Catalog determines where the underlying data for managed tables is stored based on a principle of precedence: the lowest level in the hierarchy that has a defined storage location takes priority.</p> <p></p> <p>Note</p> <p>If storage is defined at Object/Table level (External Tables), that location is used.</p> <p>If not defined at the object level, UC checks the Schema level.</p> <p>If not defined at the schema level, UC checks the Catalog level.</p> <p>If not defined at the catalog level, the default Metastore managed location is used.</p> <p>If both the Metastore and Catalog have associated storage accounts, the Catalog's location gets priority because it is \"more closer to the object\".</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/unitycatalog/#managed-tables-vs-external-tables","title":"Managed Tables vs. External Tables","text":"Feature Managed Table External Table Metadata Storage Metastore. Metastore. Data Storage Managed Data Lake (linked to Metastore). External Data Lake (owned by the user). Data Ownership Managed by Databricks. Owned by the user. DROP Command Drops both the metadata (table definition) AND the underlying data files. Drops only the metadata/table definition. The underlying data files remain intact in the external data lake. Optimization Databricks automatically handles optimization (liquid clustering, partitions), offering high performance. User must manage optimization and performance externally. Recovery Supports the <code>UNDROP TABLE</code> command, allowing dropped data to be recovered within 7 days. Not applicable, as data files are not deleted by the <code>DROP</code> command. <p>Note</p> <p>Managed tables are increasingly preferred due to performance gains and the introduction of the <code>UNDROP</code> feature, mitigating the risk of accidental data deletion.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/unitycatalog/#volumes-governing-non-tabular-data","title":"Volumes: Governing Non-Tabular Data","text":"<p>Volumes are Unity Catalog objects, similar to tables and views, but they provide a governance layer over non-tabular data sets (files).</p> <p>Volumes allow users to store, access, and govern files in any format (structured, semi-structured, unstructured, e.g., CSV, PDF) directly through the Unity Catalog namespace.</p> <p>Example</p> <p>Instead of reading a CSV file using the complex cloud path, you reference it using the UC Volume path format: <code>volumes/&lt;catalog_name&gt;/&lt;schema_name&gt;/&lt;volume_name&gt;/&lt;file.csv&gt;</code>.</p> <p>Both external volumes (linked to external containers, like the raw data container) and managed volumes (stored within the UC managed location) can be created.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"cloud/unitycatalog/#advanced-data-security-functions-and-data-masking","title":"Advanced Data Security: Functions and Data Masking","text":"<p>Unity Catalog enables advanced data security policies, such as data masking, using Functions\u2014the fourth major object type housed within a schema.</p> <p>Data masking involves hiding or obscuring sensitive data (like salary or personal identifiers) from unauthorized users while allowing specific, authorized teams (like HR or owners) to see the real values.</p> <p>Workflow for Data Masking:</p> <ol> <li>Identify Target: Identify the sensitive column (e.g., <code>salary</code>) in the target table (e.g., <code>employee</code>).</li> <li>Create Group: Ensure the authorized users are part of a specific group (e.g., <code>Admins</code> or <code>HR_Team</code>).</li> <li>Create UC Function: Define a SQL function within the Unity Catalog hierarchy (e.g., <code>external_catalog.external_schema.masking</code>) that dictates the masking logic.        Logic Example: The function uses the built-in <code>is_account_group_member('&lt;group_name&gt;')</code> function inside a <code>CASE</code> statement. If the current user is a member of the authorized group, the function returns the real input value (salary); otherwise, it returns a masked value (e.g., ``).</li> <li>Apply Mask: Use the <code>ALTER TABLE</code> command to apply the masking function to the target column:     <pre><code>ALTER TABLE &lt;table_name&gt; ALTER COLUMN salary SET MASK &lt;function_name&gt;;\n</code></pre></li> <li>Result: When an unauthorized user queries the table, they see the masked values. When an authorized user (who is a member of the defined group) queries the table, they see the real, unmasked data.</li> </ol>"},{"location":"datawarehousing/overview/","title":"Data Warehousing","text":"<p>Data warehousing is the process of collecting, integrating, storing and managing data from multiple sources in a central repository. It enables organizations to organize large volumes of current and historical data for efficient querying, analysis and reporting.</p> <p>Note</p> <p>The main goal of data warehousing is to support decision-making by providing clean, consistent and timely access to data. It ensures fast data retrieval even when working with massive datasets.</p> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"datawarehousing/overview/#data-warehouse-architecture","title":"Data Warehouse Architecture","text":"<p>--- Need for Data Warehousing</p> <ol> <li> <p>Handling Large Data Volumes - Traditional databases store limited data (MBs to GBs), while data warehouses are built to handle huge datasets (up to TBs), making it easier to store and analyze long-term historical data.</p> </li> <li> <p>Enhanced Analytics - Databases handle transactions; data warehouses are optimized for complex analysis and historical insights.</p> </li> <li> <p>Centralized Data Storage - A data warehouse combines data from multiple sources, giving a single, unified view for better decision-making.</p> </li> <li> <p>Trend Analysis - By storing historical data, a data warehouse allows businesses to analyze trends over time, enabling them to make strategic decisions based on past performance and predict future outcomes.</p> </li> <li> <p>Business Intelligence Support - Data warehouses work with BI tools to give quick access to insights, helping in data-driven decisions and improving efficiency. Components of Data Warehouse</p> </li> <li> <p>Data Sources - These are the various operational systems, databases and external data feeds that provide raw data to be stored in the warehouse. ETL (Extract, Transform, Load) Process - The ETL process is responsible for extracting data from different sources, transforming it into a suitable format and loading it into the data warehouse.</p> </li> <li> <p>Data Warehouse Database - This is the central repository where cleaned and transformed data is stored. It is typically organized in a multidimensional format for efficient querying and reporting.</p> </li> <li> <p>Metadata - Metadata describes the structure, source and usage of data within the warehouse, making it easier for users and systems to understand and work with the data.</p> </li> <li> <p>Data Marts - These are smaller, more focused data repositories derived from the data warehouse, designed to meet the needs of specific business departments or functions.</p> </li> <li> <p>OLAP (Online Analytical Processing) Tools - OLAP tools allow users to analyze data in multiple dimensions, providing deeper insights and supporting complex analytical queries.</p> </li> <li> <p>End-User Access Tools - These are reporting and analysis tools, such as dashboards or Business Intelligence (BI) tools, that enable business users to query the data warehouse and generate reports.</p> </li> </ol> <p>--- Types of Data Warehouses</p> <p>Enterprise Data Warehouse (EDW) - A centralized warehouse that stores data from across the organization for analysis and reporting.</p> <p>Operational Data Store (ODS) - Stores real-time operational data used for day-to-day operations, not for deep analytics.</p> <p>Data Mart - A subset of a data warehouse, focusing on a specific business area or department.</p> <p>Cloud Data Warehouse - A data warehouse hosted in the cloud, offering scalability and flexibility.</p> <p>Big Data Warehouse - Designed to store vast amounts of unstructured and structured data for big data analysis.</p> <p>Virtual Data Warehouse - Provides access to data from multiple sources without physically storing it.</p> <p>Hybrid Data Warehouse - Combines on-premises and cloud-based storage to offer flexibility.</p> <p>Real-time Data Warehouse - Designed to handle real-time data streaming and analysis for immediate insights.</p> <p>--- DBMS vs DataWarehouse</p> Aspect Database (DBMS) Data Warehouse Purpose Designed for operational / transactional processing (OLTP) Designed for analytical processing (OLAP) Type of Data Stores current and up-to-date data Stores historical data over long periods Data Usage Used for day-to-day operations Used for trend analysis, reporting, and decision-making Transaction Handling Each operation is an indivisible transaction Optimized for complex queries and aggregations Data Scope Usually application-specific Integrated at organization level Data Sources Typically sourced from a single application Combines data from multiple databases and sources Data Structure Highly normalized to reduce redundancy Often denormalized (star/snowflake schemas) Time Dimension Mostly current state data Maintains time-variant data Query Type Simple inserts, updates, deletes, and selects Complex analytical queries (GROUP BY, JOINs, aggregates) Performance Focus Fast read/write transactions Fast read-heavy analytical queries Cost Relatively inexpensive to construct and maintain Can be expensive due to storage, ETL, and infrastructure Users End users, applications Data analysts, BI tools, decision makers Examples Student records in a school database Analyzing best-performing schools across a city <p>--- We can only store Structured Data in Data Warehouse?</p> <p>Traditionally, data warehouses were designed to handle structured data - that is, data that fits neatly into a table with rows and columns, like you would find in a relational database or an Excel spreadsheet. This structured data could be analyzed using SQL or similar querying languages.</p> <p>However, with the advent of Big Data, the nature of data has evolved. Today's organizations also need to handle semi-structured data (like JSON or XML files) and unstructured data (like text documents, images, videos, etc.) to extract insights. Consequently, the concept of data warehousing has also evolved.</p> <p>Modern data warehouses have extended their capabilities and can now accommodate, process, and analyze semi-structured and even unstructured data. For example, solutions like Google's BigQuery, Amazon's Redshift, and Snowflake allow users to query non-relational data types including nested and repeated data.</p> <p>However, while it's possible to store and query unstructured and semi-structured data in a modern data warehouse, often it's not the most efficient or cost-effective way to handle such data. For many use cases, it may be more appropriate to use other types of data storage and processing systems, like data lakes or NoSQL databases, which are specifically designed to handle these types of data. These systems can then work in conjunction with a data warehouse as part of a broader data architecture.</p> <p>Note</p> <p>It's important to note that implementing a data warehouse is not a trivial task. It involves data cleaning, data integration, and data transformation tasks that can be complex and time-consuming. Therefore, the decision to create a data warehouse should take into consideration the specific needs of the organization, the availability of resources, and the potential return on investment.</p> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"datawarehousing/overview/#oltp","title":"OLTP","text":"<p>OLTP, or Online Transaction Processing, is a class of software applications capable of supporting transaction-oriented programs. </p> <p>--- Here are some key points about OLTP</p> <ol> <li>Transactional Operations - OLTP systems handle a large number of short, atomic transactions. These transactions are typically CRUD operations - Create, Read, Update, Delete.</li> <li>Concurrency - Due to the high number of transactions, OLTP systems use multi-concurrency control techniques to prevent conflicts and ensure data consistency.</li> <li>Data Integrity - The systems are designed to ensure absolute data integrity through ACID (Atomicity, Consistency, Isolation, Durability) properties.</li> <li>Real-Time Processing - OLTP applications are designed for real-time transactional processing and quick response times.</li> <li>High Availability - Given the critical nature of many OLTP systems, they are designed for high availability and fault tolerance.</li> <li>Operational vs. Analytical - OLTP systems are focused on operational data and current transactional activity, as opposed to OLAP (Online Analytical Processing) systems, which are optimized for complex, analytical queries and historical and aggregated data reporting.</li> <li>Performance Metrics - The performance of OLTP systems is usually measured in transactions per second (TPS).</li> <li>Database Design - OLTP systems often use a relational database design with an extensive index to deliver rapid responses to SQL queries.</li> </ol> <p>--- OLTP Systems</p> <ol> <li>ERP Systems - Many ERP (Enterprise Resource Planning) systems, such as SAP ERP, Oracle ERP, Microsoft Dynamics, etc., are considered OLTP systems.</li> <li>Banking Systems - Online banking systems and ATM (Automated Teller Machines) software are examples of OLTP systems.</li> <li>Airline Reservation Systems - Software for reserving and selling tickets for airlines.</li> <li>E-commerce Platforms - Systems like Amazon and eBay, which handle numerous online transactions daily.</li> <li>Telecommunication Network Systems - They handle real-time transactions like call data records.</li> <li>Retail POS Systems - Point of Sale systems in retail stores.</li> <li>Customer Relationship Management Systems - CRM systems like Salesforce and HubSpot, where real-time data updates are crucial.</li> <li>Online Service Applications - Many apps, such as ride-sharing apps like Uber and Lyft, food delivery apps like DoorDash, and Airbnb, use OLTP systems for real-time transactions.</li> </ol> <p>--- Popular Databases for OLTP</p> <ol> <li>Oracle Database - A popular relational database management system from Oracle Corporation.</li> <li>MySQL - An open-source relational database management system owned by Oracle Corporation.</li> <li>Microsoft SQL Server - A relational database management system developed by Microsoft.</li> <li>PostgreSQL - A powerful, open-source object-relational database system.</li> <li>IBM DB2 - A family of database server products developed by IBM.</li> <li>MariaDB - An open-source relational database management system, forked from MySQL.</li> <li>SAP HANA - An in-memory, column-oriented, relational database management system developed by SAP.</li> <li>Amazon Aurora - A relational database service from Amazon Web Services (AWS), compatible with MySQL and PostgreSQL.</li> <li>Google Cloud Spanner - A scalable, enterprise-grade, globally-distributed, and strongly consistent database service built for the cloud specifically to combine the benefits of relational database structure with non-relational horizontal scale.</li> <li>CockroachDB - An open-source distributed SQL database designed for global online transaction processing (OLTP).</li> </ol> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"datawarehousing/overview/#olap","title":"OLAP","text":"<p>OLAP, or Online Analytical Processing, is a category of software tools that provide analysis of data for business decisions. It is characterized by relatively low volume of transactions but complex queries involving aggregations, which need to be executed relatively quickly.</p> <p>--- Here are the key characteristics of OLAP systems</p> <ol> <li>Multi-Dimensional Analysis - OLAP tools allow users to analyze data along multiple dimensions, which is generally more intuitive for users. For example, a business person might want to analyze sales by product, by region, and by time period.</li> <li>Speedy Query Performance - Despite the size and complexity of the data sets, OLAP tools provide rapid results to queries due to their use of multidimensional data cubes and precalculation.</li> <li>Aggregation and Computation - OLAP systems are optimized to perform complex calculations and data aggregations, like sums, averages, ratios, ranks, etc., on the fly.</li> <li>Support for Complex Queries - OLAP tools support complex queries and enable users to perform 'what-if' type analysis.</li> <li>Data Discovery - They support data discovery by providing flexible, ad-hoc querying and multi-dimensional analysis.</li> <li>Read-Optimized - Unlike OLTP, which is write-optimized, OLAP is optimized for a high volume of read operations with fewer write operations.</li> </ol> <p>--- OLAP Systems</p> <p>There are several databases and systems that are designed to work as OLAP systems, providing multi-dimensional analysis of data. </p> <ol> <li>Microsoft Analysis Services - Also known as SSAS (SQL Server Analysis Services), it provides data mining and multi-dimensional analysis.</li> <li>SAP BW (Business Warehouse) - SAP's data warehousing solution that includes OLAP capabilities.</li> <li>Amazon Redshift - A fully managed, petabyte-scale data warehouse service in the cloud from Amazon Web Services.</li> <li>Google BigQuery - A fully managed, serverless data warehouse that enables super-fast SQL queries and interactive analysis of massive datasets.</li> <li>Snowflake - A cloud-based data warehousing platform that supports multi-dimensional analysis of large volumes of data.</li> </ol> <p>--- OLTP vs OLAP</p> Aspect OLTP (Online Transaction Processing) OLAP (Online Analytical Processing) Main Function Manages day-to-day transactions of an organization Supports complex analysis and decision making Database Design Normalized schema, optimized for INSERTs and UPDATEs Denormalized schema, optimized for analysis and reporting Data Type Detailed, current operational data Summarized, consolidated, and historical data Transactions Short and fast updates and queries Long, complex queries involving aggregations Number of Records Accessed Accesses individual records (e.g., single row) Accesses large volumes of data, often entire tables Performance Metrics Measured in transactions per second (TPS) Measured in query response time Users Front-line workers (clerks, operators, IT staff) Managers, Business Analysts, Decision Makers Database Size Smaller (stores current operational data) Larger (stores historical data over time) Nature of Queries Simple, predefined queries Complex, ad-hoc queries Consistency Requirement High concurrency control and transaction consistency Lower consistency needs, data loaded in batches Examples Online banking, airline ticket booking, order processing Sales trend analysis, financial reporting, BI dashboards <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"datawarehousing/overview/#normalized-data","title":"Normalized data","text":"<p>Data Normalization is a process in database design that organizes data to minimize redundancy and improve data integrity. It involves structuring data in accordance with a series of so-called normal forms, each with an increasing level of strictness.</p> <p></p> <p>--- The primary advantages of normalization are</p> <ol> <li>Minimize Redundancy - By ensuring that each piece of data is stored in only one place, normalization reduces the redundancy of data within the database.</li> <li>Data Consistency - Normalization improves data consistency as each data item is stored in only one place. Any update needs to be performed only in one place, reducing the chances of inconsistent data.</li> <li>Efficient Use of Storage - Normalized databases are more compact and often require less storage space, especially for large databases.</li> <li>Simplified Data Management - Normalization simplifies the management and updating of data as it ensures that a piece of information exists in one place only.</li> </ol> <p>Normalized Data Systems, such as OLTP (Online Transaction Processing) systems, use normalized databases. These systems require high data integrity and support a large number of short online transactions (INSERT, UPDATE, DELETE). Examples include databases for managing sales transactions, customer relationship management (CRM) systems, airline reservation systems, and banking systems. These systems rely on data normalization to ensure data consistency and efficiency in processing a high volume of transactions.</p> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"datawarehousing/overview/#denormalized-data","title":"Denormalized Data","text":"<p>Data Denormalization is the process of combining data from several normalized tables into one, for the purpose of improving read performance, simplifying queries, or preparing the data for specific analytical requirements. This is the opposite of normalization, where data is separated into multiple tables to minimize redundancy and improve data integrity.</p> <p>Denormalization may lead to data redundancy and anomalies (like insert, update, or delete anomalies), but it reduces the need for complex joins and can therefore enhance the performance of read-heavy applications.</p> <p>Denormalized Data Systems, such as OLAP (Online Analytical Processing) systems or data warehousing systems, use denormalized databases. These systems are designed for reporting and data analysis, and they support complex queries against large amounts of data. The data in these systems is typically read-only or infrequently updated, and the emphasis is on retrieval speed rather than transaction speed.</p> <p></p> <p>--- Here are some key characteristics of denormalized data systems</p> <ol> <li>Data Redundancy - Since data is combined into fewer tables, some data can be repeated or redundant.</li> <li>Read Optimization - Denormalized databases can retrieve data faster because they often need fewer joins to answer a query.</li> <li>Complexity - By reducing the number of tables, the database structure becomes simpler, and complex queries can be more easily written.</li> <li>Data Integrity - In denormalized databases, maintaining data integrity can be more challenging due to potential redundancy.</li> <li>Usage - Denormalized data systems are commonly used for data analysis, data mining, data warehousing, and any system where reading data is more important than updating or inserting data.</li> </ol> <p>Note</p> <p>It's important to note that denormalization doesn't mean that normalization principles are entirely abandoned. Rather, it's a strategic optimization for specific read-heavy use cases.</p> <p>--- Normalized vs Denormalized</p> Aspect Normalized Data Denormalized Data Structure Data is spread across multiple tables to eliminate redundancy and dependency Data is combined into fewer tables to speed up data retrieval Redundancy Redundancy is minimized; each data item is stored in only one place Redundancy may be introduced to improve read performance Performance Writing data (INSERT, UPDATE, DELETE) is usually faster; reading can be slower due to multiple joins Reading data is typically faster due to fewer joins; writing can be slower because of redundancy Data Integrity Higher, as redundancy is eliminated Lower, as redundancy can lead to inconsistencies Use Case Optimized for transactional systems like OLTP, where write operations are common Optimized for analytical and reporting systems like OLAP, where read operations are common <p>--- Which is better depends on your use case</p> <ol> <li>Choose Normalized Data when - You are dealing with transactional systems (like OLTP), where write operations (insert, update, delete) are common and the data integrity is crucial. This includes applications like online retail websites, banking systems, CRM, etc.</li> <li>Choose Denormalized Data when - You are dealing with analytical systems (like OLAP), where read operations and speed of data retrieval are more important than write operations. This is common in reporting, data mining, and decision support systems.</li> </ol> <p>It's important to remember that this is not an either/or situation. Often, in a single system, a part of the database might be normalized (to support transactional operations) and another part might be denormalized (to support analytical operations). It's all about choosing the right tool for the right job based on your specific needs.</p> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"datawarehousing/overview/#datalake","title":"DataLake","text":"<p>Data Lake is a storage repository, usually on a large scale, that holds a vast amount of raw data in its original, native format until it's needed. Unlike a traditional data warehouse, which stores data in a structured and often processed format, a data lake retains all data\u2014structured, semi-structured, and unstructured\u2014and does not require the schema to be defined before storing the data.</p> <p></p> <p>--- Here are some reasons why we need data lakes</p> <ol> <li>Data Variety - Can store any type of data (structured, semi-structured, unstructured), accommodating the wide variety of data formats in big data.</li> <li>Scalability - Can handle massive volumes of data and quickly scale as data grows.</li> <li>Cost-Effective - Utilizes inexpensive, commodity hardware for storing large amounts of data.</li> <li>Data Exploration - Allows for exploratory data analysis, which is essential for data discovery and advanced analytics.</li> <li>Data Integration - Acts as a central hub for all organizational data, creating a unified data view.</li> <li>Real-time Processing - Supports both batch and real-time data processing, suitable for real-time analytics.</li> <li>Schema on Read - Doesn't require data modeling before storing, offering flexibility in data usage.</li> </ol> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"datawarehousing/overview/#datamart","title":"DataMart","text":"<p>A Data Mart is a subset of a data warehouse that is focused on a specific area of business, such as sales, marketing, finance, or HR.  Essentially, a data mart is a condensed version of a data warehouse that is tailored to fit the needs of a specific business unit or team.</p> <p></p> <p>--- Here are the reasons why we need data marts</p> <ol> <li>Focused Business Insights - Since data marts focus on a specific area of business, they can provide more detailed and relevant insights for a specific department or team.</li> <li>Improved Performance - With less data to process, queries run faster in data marts, improving response times for users.</li> <li>Ease of Use - Data marts are simpler and more intuitive for end users because they only contain data relevant to a specific business area. Users don't need to sift through unrelated data, which makes their analysis faster and more efficient.</li> <li>Autonomy - By having their own data mart, a business unit can maintain control over their data, making decisions on data handling, and access based on their unique needs and priorities.</li> <li>Less Risk - With data marts, you can start small and grow over time. This approach reduces the risk and cost compared to implementing a full data warehouse at once.</li> <li>Increased Security - It's easier to secure data because data marts only hold a limited set of data. Therefore, even if a breach occurs, the amount of exposed data would be less compared to a full data warehouse.</li> </ol> <p>Despite these advantages, it's essential to ensure that data marts are well-coordinated within an organization to avoid issues like data duplication or data inconsistencies. Sometimes, an organization may choose a hybrid approach, having a central data warehouse for the entire organization, and then creating data marts for specific departments or functions.</p> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"datawarehousing/overview/#data-modeling","title":"Data Modeling","text":"<p>Data Modelling, on the other hand, is a specific part of the data warehouse design process. It involves creating a conceptual model of how data should be structured in the database.</p> <p>--- Difference between Data Warehouse Design &amp; Data Modelling</p> <p>Data Warehouse Design and Data Modelling are interrelated concepts in the field of data management, but they focus on different aspects. Here's a brief comparison -</p> <p>Data Warehouse Design refers to the process of creating a blueprint for the data warehouse system. It includes the decision-making and planning for -</p> <ol> <li>Data sources - Identifying the sources from where data will be pulled.</li> <li>Data transformation - Defining how data will be cleaned, transformed, and integrated.</li> <li>Data storage - Choosing the structure in which data will be stored, such as deciding between a normalized or denormalized schema, or choosing between different architectures like a Star Schema, Snowflake Schema, etc.</li> <li>Data retrieval - Planning for how data will be queried and retrieved, such as creating indexes, views, or OLAP cubes.</li> <li>Security and backup - Defining how the data will be secured, how privacy will be maintained, and how data will be backed up.</li> </ol> <p>--- FACT table</p> <p>A Fact Table is a central component of a star schema or a snowflake schema in a dimensional modelling. It is the main table in a dimensional model, which holds the quantitative or measurable data that a business wishes to analyze.</p> <p>Fact tables typically contain two types of columns -</p> <ol> <li>Fact - These are the measurable, quantitative data points that are relevant to the business. They are usually numerical values that can be aggregated (like summed, averaged, etc.). Examples of facts could be 'Sales Amount', 'Quantity Sold', 'Profit', etc.</li> <li>Foreign Keys - These are the keys that connect the fact table to its associated dimension tables. These keys enable you to categorize your facts and look at them from different perspectives.</li> </ol> <p></p> <p>Example</p> <p>For instance, in a retail business scenario, a fact table might have a structure like this -</p> <p>In this case, 'Sales' and 'Quantity Sold' are the facts. 'Date Key', 'Product Key', and 'Store Key' are the foreign keys linking the fact table to the dimension tables for 'Date', 'Product', and 'Store', respectively. This design allows you to analyze your sales and quantities from different dimensions like time, product type, and location.</p> <p>--- Dimension Table</p> <p>The main purpose of a dimension table is to provide context and descriptive attributes for facts stored in the Fact Table in the same dimensional data model. This allows users to understand and interpret the quantitative values in the fact table.</p> <p>Characteristics of dimension tables include -</p> <ol> <li>Descriptive - Dimension tables store descriptive or textual data, often referred to as \"attribute data.\" For example, the Product dimension table might include details like product name, product type, product category, etc.</li> <li>Granularity - The granularity of a dimension table is at the level of the individual instance. For instance, each row in a Product dimension table would represent a single product.</li> <li>Primary Key - Each dimension table has its own primary key, which is used to connect the dimension table with the fact table(s).</li> <li>Hierarchical - Dimensions often include a hierarchy of categories. For instance, a \"Time\" dimension might include fields for Year, Quarter, Month, and Day.</li> <li>Stability - While fact table data changes frequently, dimension table data is relatively stable. However, dimension tables can change in response to business changes, such as adding a new product or a new sales region.</li> </ol> <p></p> <p>Example</p> <p>In this example, 'Store_ID' would be the primary key that connects this dimension table to the fact table, and the other columns ('Store_Name', 'Store_Type', 'Location') provide descriptive details about each store.</p> <p>Dimension tables help in performing meaningful analysis of the fact data. They allow the business users to look at the data from various perspectives and help in slicing and dicing the fact data.</p> <p>------------------------------------------------------------------------------------------------------------</p> <p>--- Star Schema</p> <p>The Star Schema is a simple yet powerful database architecture used in data warehousing and business intelligence reporting. The star schema gets its name from the physical model's resemblance to a star shape with a fact table in the middle surrounded by dimension tables.</p> <p></p> <ol> <li>Fact Table - At the center of the star schema is the fact table. This table contains the quantitative or measurable data (the \"facts\") that the business wants to analyze. For example, in a retail business, the fact table might store transaction data like units sold and total sales. Fact tables often contain a large number of rows, reflecting the detailed level of data they store.</li> <li>Dimension Tables - Surrounding the fact table are dimension tables. These tables contain descriptive data that provide context to the facts. For example, in the retail business scenario, the dimension tables might include information on products, customers, stores, and dates. Each row in a dimension table represents a unique instance of that dimension, like a specific product or a specific store.</li> <li>Schema Layout - In a star schema, each dimension table is directly connected to the fact table via a foreign key relationship. The fact table includes a foreign key for each dimension table that it's linked to. These foreign keys enable the joining of the fact table with dimension tables.</li> <li>Simplicity - The star schema is denormalized, which means it does not strictly enforce certain data integrity rules that are required in a normalized database design. This denormalization simplifies the database design and makes it easier to write and run queries.</li> <li>Performance - The star schema is designed for efficient data retrieval and is the standard schema for a reason. The simple, predictable queries, reduced number of tables, and minimized joins make it ideal for handling complex business queries and aggregations.</li> </ol> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"datawarehousing/overview/#snowflake-schema","title":"Snowflake Schema","text":"<p>The Snowflake Schema is a type of database schema that is used in data warehousing. It's a variant of the star schema, but with additional levels of complexity due to further normalization of the dimension tables. It's called a \"snowflake\" schema because its diagram resembles a snowflake with spokelike arms branching out from a center.</p> <ol> <li>Fact Table - Similar to the star schema, the snowflake schema also consists of a central fact table which contains the business data (facts) that are being analyzed. This could include things like sales amounts, quantities, and so forth.</li> <li>Normalized Dimension Tables - In the snowflake schema, the dimension tables are normalized. This means that the data in the dimension tables is split or segregated into additional tables. For example, instead of having a single \"Product\" table that includes all product-related information, you might have a main \"Product\" table linked to separate tables for \"Product Category\" and \"Product Manufacturer\". This normalization reduces data redundancy.</li> <li>Schema Layout - In a snowflake schema, the fact table is at the center with multiple branching dimension tables, which can have other tables branching off of them. This multi-level relationship between the fact table and dimension tables gives the schema its snowflake-like appearance.</li> <li>Query Complexity - Due to the additional level of normalization, queries in snowflake schemas can become more complex as they may involve more tables and joins. This might lead to longer query times, but it can also lead to more efficient storage.</li> <li>Storage Efficiency - One of the main advantages of the snowflake schema is the reduction in storage required due to the normalization of dimension tables. This can also lead to improved data integrity as it helps to eliminate redundancy and inconsistencies.</li> </ol> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"datawarehousing/overview/#galaxy-schema","title":"Galaxy Schema","text":"<p>A Galaxy Schema, also known as a Fact Constellation Schema, is a complex type of schema used in data warehouse environments. It extends the concepts of the Star and Snowflake schemas by allowing multiple fact tables to share dimension tables.</p> <ol> <li>Multiple Fact Tables - Unlike the star and snowflake schemas, which have a single fact table, a galaxy schema can have multiple fact tables. Each fact table in a galaxy schema represents a different business process or event.</li> <li>Shared Dimension Tables - The dimension tables in a galaxy schema are often shared among fact tables. This makes it possible to analyze facts from different fact tables across shared dimensions.</li> <li>Schema Layout - The galaxy schema looks like multiple star or snowflake schemas combined, where the fact tables form the center of each star or snowflake, and the dimension tables branch out from there. The shared dimensions form the intersecting points between these multiple stars or snowflakes.</li> <li>Complexity - Galaxy schemas are typically more complex than star or snowflake schemas. They involve more tables, more relationships, and more complex queries. However, they also offer more analytical capabilities due to the ability to compare and analyze multiple facts across shared dimensions.</li> <li>Flexibility - One of the main advantages of the galaxy schema is its flexibility. It allows for complex analyses and queries across multiple business processes or events. This can be particularly useful in large organizations with complex data needs.</li> </ol> <p>------------------------------------------------------------------------------------------------------------</p>"},{"location":"datawarehousing/overview/#scd","title":"SCD","text":"<p>SCD stands for Slowly Changing Dimensions, which are a common concept in Data Warehousing, Business Intelligence, and data modeling. These dimensions are the aspects of the business that can change over time, but not at a high frequency, hence \"slowly changing.\"</p> <p>--- SCD Type 1 (Overwrite)</p> <p>In this type, when changes occur in attribute values, the old values are overwritten with new ones. No history is kept in this scenario, only the current state is stored. This approach is typically used for minor or correctional changes where tracking the history is not important.</p> <p>Example</p> <p>Let's take an example of a customer dimension where the customer's address is an attribute. If a customer changes their address, in a Type 1 SCD, the new address would simply replace the old address in the customer dimension table.</p> <p>--- SCD Type 2 (Add a new row)</p> <p>This type is used when it is necessary to maintain a full history of data changes. When changes occur, instead of updating the existing records, a new row is added with the new values, and the original record is marked as inactive or retained with a flag indicating that it is the old version.</p> <p>Example</p> <p>In the same customer dimension example, if the customer changes their address and we are using a Type 2 SCD, a new row would be added for the customer with the new address, and the old address row would be marked as inactive or have a flag indicating it's an old version. This way, we have a complete history of all addresses the customer has had over time.</p> <p>--- SCD Type 3 (Add a new column)</p> <p>In this type, when changes occur, a new column is added to the table to track the changes. This is generally used when we are only interested in maintaining the current value and the immediate previous value.</p> <p>Example</p> <p>In the customer dimension example, if the customer changes their address and we are using a Type 3 SCD, a new column would be added (e.g., \"Previous_Address\") to store the old address, and the \"Address\" column would be updated with the new address. This method allows us to see the current and previous address but doesn't keep a full history of all addresses.</p> <p>---SCD Type 4 (Using History Table)</p> <p>This method involves the use of a separate history table to track the changes. When a change happens, the current table (dimension table) gets updated, and the old record gets pushed into the history table. This method keeps the dimension table lightweight but provides full historical context in the separate table.</p> <p>Example</p> <p>If a customer changes their address in a Type 4 SCD setup, the customer's current address in the main customer table would be updated with the new address, and the old address would be stored in a separate customer address history table.</p> <p>--- SCD Type 6 (Combination of Type 1, 2, and 3) </p> <p>Also known as a hybrid method, this combines elements of Type 1, Type 2, and Type 3 SCDs. This method usually holds one or more current attributes (Type 1), one or more historical attributes (Type 2), and may include a \"previous value\" column for specific attributes (Type 3).</p> <p>Example</p> <p>If a customer changes their address in a Type 6 SCD setup, a new row would be added with the new address (like Type 2), and the old row may be kept as it is or updated in a Type 1 style for certain attributes. Additionally, there might be a column in the new row capturing the previous address (like Type 3).</p> <p>Please note that Type 4 and Type 6 are less commonly used due to their complexity, but they do provide additional options for handling changes over time in a data warehouse environment. The decision on which type to use depends on the specific needs of your business and the resources available for managing the data warehouse.</p> <p>--- Slowly Changing Dimensions (SCD) \u2013 Raw Examples</p> <p>Sample Source Data</p> <p>Customer C001 changes address on 2024-06-01</p> customer_id name address C001 Rahul Mumbai <p>---  SCD Type 1 \u2013 Overwrite (No History)</p> <p>Dimension Table (customer_dim)</p> customer_id name address C001 Rahul Pune <p>Old value (Mumbai) is overwritten  No history maintained</p> <p>---  SCD Type 2 \u2013 Add New Row (Full History)</p> <p>Dimension Table (customer_dim)</p> surrogate_key customer_id name address start_date end_date is_active 101 C001 Rahul Mumbai 2022-01-01 2024-05-31 N 102 C001 Rahul Pune 2024-06-01 9999-12-31 Y <p>Full history maintained  Time-based analysis possible</p> <p>---  SCD Type 3 \u2013 Add New Column (Limited History)</p> <p>Dimension Table (customer_dim)</p> customer_id name current_address previous_address C001 Rahul Pune Mumbai <p>Only current + previous value  No complete historical tracking</p> <p>---  SCD Type 4 \u2013 History Table</p> <p>--- Current Dimension Table (customer_dim)</p> customer_id name address C001 Rahul Pune <p>--- History Table (customer_address_history)</p> customer_id address change_date C001 Mumbai 2024-05-31 <p>Dimension table remains small  Full history stored separately</p> <p>---  SCD Type 6 \u2013 Hybrid (Type 1 + 2 + 3)</p> <p>Dimension Table (customer_dim)</p> surrogate_key customer_id name current_address previous_address start_date end_date is_active 201 C001 Rahul Mumbai NULL 2022-01-01 2024-05-31 N 202 C001 Rahul Pune Mumbai 2024-06-01 9999-12-31 Y <p>Type 2 \u2192 New row  Type 3 \u2192 Previous address column  Type 1 \u2192 Non-historical attributes can be overwritten</p> <p>---  Summary Table</p> SCD Type History Technique Type 1 No Overwrite Type 2 Full New rows Type 3 Limited New column Type 4 Full History table Type 6 Full + Quick Access Hybrid"},{"location":"datawarehousing/overview/#types-of-databases","title":"Types of Databases","text":""},{"location":"delta/deltalake/","title":"Delta Lake","text":""},{"location":"delta/deltalake/#data-lake-problems-and-the-delta-lake-solution","title":"Data Lake Problems and the Delta Lake Solution","text":"<p>Data lakes offered flexible data storage, allowing ingestion of structured, semi-structured, and unstructured data like audio, video, images, or logs, and storing this high volume data cheaply and scalably on services like S3 or ADLS.</p> <p>However, data lakes presented severe challenges that Delta Lake was designed to solve, including a lack of ACID support, lack of support for update, merge, and delete operations, and issues related to data reliability and quality due to the absence of schema enforcement.</p> <p>Traditional updates or deletes required reading all the data into memory, applying the changes, and writing it back; if the system failed during this process, it resulted in inconsistent or corrupt data. Delta Lake addresses these issues and provides additional features such as time travel, unified batch and streaming capabilities, schema evolution and enforcement, and audit history</p>"},{"location":"delta/deltalake/#delta-log-internals-and-scaling","title":"Delta Log Internals and Scaling","text":"<p>The Delta Log acts as the transaction layer on top of Parquet files, recording transactions atomically in JSON files (e.g., 0.json, 1.json). The current state of the table is calculated by \"summing\" all transaction logs, applying additions and removals to determine the set of active Parquet files.</p> <p></p> <p>Delta Lake computes the latest state of a table by reading the Delta Log and performing a summation of all recorded transactions. This approach ensures data consistency and provides features like time travel.</p> <p>--- Core Components of a Delta Table</p> <p>A Delta Lake table is composed of two main elements</p> <ol> <li>Data Files (Parquet): The actual data is stored in immutable Parquet files.</li> <li>Transaction Layer (Delta Log): A transaction layer, called the Delta Log, resides on top of the data files. This log records every operation performed on the table.</li> </ol> <p>--- The Delta Log and Transaction Recording</p> <p>Deltalog jason file containes - {commitinfo,metadata,protocol,operationperformed}</p> <p>Each transaction or operation (such as an insert, update, or delete) on a Delta table is treated as an atomic unit of work.</p> <p>Recording Transactions: Transactions are recorded as sequential JSON files (e.g., 0.json, 1.json, 2.json) within the hidden _delta_log folder.</p> <p>Operation Details: Each JSON file contains details about the operation performed (e.g., create or replace table, write, update, delete) and tracks which Parquet files were added or logically removed. A transaction groups one or more operations together. The commit info section contains details about the operation and the user who performed it. The add section notes a new Parquet file that was added to the table state. The remove section logically marks a Parquet file as no longer part of the current table state (a soft delete).</p> <p>Computing the Latest State via Summation</p> <p></p> <p>When a user executes a query (e.g., SELECT * FROM table), Delta Lake processes the Delta Log to construct the latest valid view of the data:</p> <ol> <li> <p>Applying Transactions in Order: Delta Lake starts from the beginning of the transaction history and sequentially applies each transaction record (JSON file).</p> </li> <li> <p>Summation Principle: The system performs a \"summation\" of the transactions. It tracks all file additions (+) and removals (-). Files that are added and subsequently removed in later transactions cancel each other out.</p> </li> <li> <p>Handling Updates and Deletes: When an update or delete occurs, the original Parquet file containing the modified data is logically marked as REMOVE, and a new file (or files) containing the revised records is ADDed. For small changes, especially when deletion vectors are enabled, Delta Lake records the changes in the deletion vector file instead of rewriting the entire Parquet file immediately (Merge on Read approach). However, the log still records the necessary file additions and removals to reflect the logical state change.</p> </li> <li> <p>Final State Construction: The final, latest state of the table is computed by reading only the Parquet files that remain after all logical removals have been processed in the summation.</p> </li> </ol> <p>Scaling the Delta Log (Compacted JSONs and Checkpoints)</p> <p>For tables with millions of transactions, reading every JSON file in the Delta Log to compute the latest state would be computationally expensive and slow. </p> <p>To optimize this, Delta Lake uses two key scaling mechanisms:</p> <ol> <li> <p>Compacted JSONs: Compacted JSON files are used to reduce the overhead associated with reading many small JSON transaction files. Delta Lake periodically creates a compacted JSON file (e.g., a .compacted.json file). This compacted file picks up and dumps all the information (transactions and details) contained within a set of smaller JSON log files into one larger file, avoiding the need to open and close many individual files. If a transaction is committed, Delta can read the already compacted file plus any subsequent uncompacted files to compute the latest state. It avoids having to read all the historical individual JSON files.</p> </li> <li> <p>Checkpoint Parquet Files: Checkpoint files provide a mechanism to drastically reduce the number of files Delta must scan to determine the latest state of a table, offering a \"tremendous amount of optimization\". After a predetermined number of JSON files are created (e.g., 36 files in the Databricks environment used in the example), a checkpoint.parquet file is generated. This checkpoint file condenses all the information from the very first transaction (version zero) up to the point of the checkpoint.</p> </li> </ol> <p>To compute the latest state of the table, Delta only needs to find and read the latest checkpoint.parquet file and then apply any subsequent JSON files (or compacted JSONs) that have been created since that checkpoint. This allows Delta to skip reading potentially hundreds or thousands of older historical JSON log files. The gap at which a checkpoint file is created varies; it is 36 transactions in the Databricks version shown, but often 10 in the open-source version. This difference is due to Databricks implementing performance optimizations to relax this gap.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#pessimistic-concurrency-control","title":"Pessimistic concurrency control","text":"<p>Core Assumption</p> <p>PCC gets its name because the DBMS operates under the assumption that conflicts are bound to happen and are likely to occur. A conflict occurs when two or more transactions attempt to modify the same data simultaneously, resulting in an invalid state. It is a strategy used by database management systems (DBMS) to ensure that transactions maintain the correctness and consistency of the database.</p> <p>Mechanism: Exclusive Locking</p> <p>To prevent these conflicts, PCC uses a locking mechanism:</p> <p>\u2022 When a transaction attempts to make a change to a database, an exclusive lock is placed on that data.</p> <p>\u2022 This lock ensures that no other transaction can come in and make changes while the first transaction is running.</p> <p></p> <p>Example Walkthrough (T1 and T2)</p> <p>Consider an account with a starting balance of $1,000, and two simultaneous transactions: T1 (withdraw $100) and T2 (deposit $400).</p> <ol> <li>T1 Starts: Transaction T1 begins first.</li> <li>Lock Acquired: The moment T1 starts, it acquires an exclusive lock on the relevant row (the account balance).</li> <li>T2 Waits: Because T1 holds the lock, T2 cannot proceed and must wait until T1 completes.</li> <li>T1 Processing: T1 reads the balance ($1,000), subtracts $100, and computes the new balance as $900.</li> <li>T1 Updates and Releases: T1 updates the account balance to $900 and then releases the exclusive lock.</li> <li>T2 Starts: T2 now gets a chance and places its own exclusive lock on the row.</li> <li>T2 Processing: T2 reads the latest balance ($900), adds $400, computes the final balance as $1,300, and updates the account.</li> <li>Consistency Achieved: T2 then releases its lock, and the database is left in the correct state.</li> </ol> <p>Major Drawback</p> <p>While PCC achieves consistency, its major downside is performance when dealing with high concurrency:</p> <p>\u2022 Waiting: T2 had to wait until T1 was completed.</p> <p>\u2022 Scalability Issue: If a system handles millions of transactions, the waiting involved means the entire system is going to \"come to a halt\" because transactions wait endlessly for locks to be released, making the system slow and inefficient. This performance issue is why Delta Lake primarily achieves isolation using Optimistic Concurrency Control (OCC), which assumes conflicts are rare and avoids explicit locks</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#optimistic-concurrency-control","title":"Optimistic concurrency control","text":"<p>Core Assumption (Optimism)</p> <p>The name \"Optimistic\" comes from its fundamental assumption: conflicts are very unlikely to occur. Conflicts, where two or more transactions try to modify the same data and lead to an invalid state, are assumed to be rare. This assumption allows the system to prioritize speed and concurrency over preemptive conflict avoidance.</p> <p>Mechanism: Avoiding Locks</p> <p>With OCC, transactions do not obtain locks when they read or write data. This is the \"beautiful part\" of OCC. Because no locks are used, multiple transactions can read the same row simultaneously.</p> <p>When transactions (like T1 and T2) begin, they read the account balance, and crucially, they also read the associated version number and time frame of the data. This version number tracks changes within the table.</p> <p>Conflict Resolution and Validation</p> <p></p> <p>When transactions are finished processing and attempt to commit their changes, a validation procedure occurs:</p> <ol> <li>Concurrent Execution: T1 and T2 perform their calculations independently (e.g., T1 computes $900, T2 computes $1,400).</li> <li>First Commit Wins (T1): T1 attempts to commit first. It checks the current version number (let's say V1) against the version number it read (also V1). Since they match, T1 knows no one made changes while it was working. T1 succeeds, updates the balance (to $900), and updates the version number (to V2).</li> <li>Second Commit Fails (T2): T2 attempts to commit next. It checks the current version number (now V2) against the version it originally read (V1). Because V2=V1, T2 immediately understands that a person came before it and made changes to the table. T2 was working on stale data.</li> <li>The Retry: The conflicting transaction (T2) does not block; instead, it is designed to fail. It must then retry the transaction. It reads the latest state of the table (V2), re-applies its operation (adding $400 to 900),computesthecorrectfinalbalance(1,300), and attempts to commit again.</li> <li>Successful Second Attempt: T2 checks the version again (V2 = V2), finds it is working on the latest data, and updates the table (to $1,300) and the version number (to V3).</li> </ol> <p>High Concurrency Benefits</p> <p>This approach has significant advantages for modern data systems:</p> <p>\u2022 No Interference: Multiple simultaneous transactions can proceed \"without interfering with each other\". Each transaction operates as if it is the only one running. Intermediate or uncommitted changes are not visible to other transactions.</p> <p>\u2022 High Concurrency: This mechanism allows for a very high level of concurrency, as transactions avoid endless waiting caused by locks.</p> <p>\u2022 Suitability for Read-Heavy Systems: OCC works best in systems where writes (and therefore conflicts) happen very rarely. This efficient conflict resolution is why OCC, and the versioning ledger it relies on, is central to Delta Lake\u2019s architecture.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#time-travel-versioning","title":"Time Travel &amp; Versioning","text":"<p>Versioning</p> <p>It is the mechanism that helps Delta Lake track the different states of a table over time.</p> <p>\u2022 Tracking Changes: Every operation performed on a Delta table (such as a create, delete, update, or insert) results in the creation of a new version of the table.</p> <p>\u2022 Zero-Indexed: The first operation performed (like a CREATE OR REPLACE TABLE AS SELECT) creates version zero of the table. Subsequent operations increment this version number. For example, after version zero, a delete operation might create version one, an update might create version two, and a write (insert) might create version three.</p> <p>\u2022 Audit History: This version history allows users to see an audit trail of operations performed, including the timestamp, the user who performed the operation, and the operation details.</p> <p>Time Travel (Accessing Past States)</p> <p>Time travel is the ability to query or restore a Delta table to any of its previous versions. This is a core feature for data reliability, testing, and compliance.</p> <p>\u2022 Accessing Specific Versions: Users can select data from a specific historical version using either the version number or the timestamp of that version.</p> <p>Note</p> <p>Using Version Number: You can use the syntax VERSION AS OF [version_number] </p> <p>In a SELECT query (e.g., SELECT * FROM table VERSION AS OF 0).</p> <p>Using Timestamp: You can use the syntax TIMESTAMP AS OF [timestamp] in a SELECT query. The timestamp must be a time at which a version existed.</p> <p>\u2022 Handling Non-Exact Timestamps: If a user specifies a timestamp that doesn't exactly match a version's commit time, Delta Lake finds the latest version that existed before that specified time. For instance, if versions existed at 9:31 and 9:57, querying for 9:35 will return the state of version 9:31.</p> <p>\u2022 Restoring the Table: Beyond querying, a user can permanently roll the table back to a specific version using the RESTORE TABLE command (e.g., RESTORE TABLE [table_name] TO VERSION AS OF 0). This restoration itself is recorded as a new version in the table's history.</p> <p>Impact of Vacuum on Time Travel</p> <p>It is crucial to understand that the VACUUM command limits time travel capability.</p> <p>\u2022 File Deletion: When you run VACUUM, especially with a retention duration of zero, Delta Lake permanently deletes the physical data files that are no longer referenced by the current version (i.e., files that have been \"tombstoned\" or marked for soft delete by update/delete operations).</p> <p>\u2022 Version Incompleteness: If a file required to construct an older version is removed by a VACUUM operation, time travel back to that specific version will fail because the data required to fully reconstruct that historical state is incomplete or missing.</p> <p>For instance, in a demonstration, attempting to access version one of a table after a vacuum failed because the file written in version one was deleted, but accessing version zero worked because its necessary file was still present</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#schema-validation","title":"Schema Validation","text":"<p>Schema-on-Read (Traditional Data Lakes)</p> <p>The traditional approach used in standard data lakes is schema-on-read.</p> <p>Data is dumped into the lake in any format (structured, semi-structured, unstructured) and resides there. The application of the schema happens later, after the data has been stored, typically when a user runs a query to read the files.</p> <p>This flexibility leads to inconsistency. If different files are ingested with varying formats or column structures, writing a query becomes difficult, requiring complex logic to handle different fields and schemas.</p> <p>Schema-on-Write (Delta Lake Validation)</p> <p>Delta Lake uses a schema-on-write approach, which is essential for ensuring that every transaction brings the database from one valid state to another.</p> <p>Incoming data records are checked to validate their schema against the target Delta table's existing schema before ingestion occurs. This is done to prevent \"garbage data\" from corrupting the table.</p> Scenario Description INSERT Behavior MERGE Behavior 1. Column Order The source data columns are reordered relative to the target table. Matches columns by position. If data types are compatible, data corruption occurs as values are mapped incorrectly. Matches columns by name. It correctly identifies columns even if the physical order is changed. 2. Data Type Incoming data types do not match the target schema's definitions. Attempts to cast (convert) the incoming value to the table's data type (e.g., string '99499' to an integer). Fails if the string value cannot be cast (e.g., 'ABC' to integer), preventing data garbage. Similar automatic casting attempts are made. 3. Column Name Source data column names differ from the target table column names. Ignores name differences, as it matches columns by position. The write succeeds if positions align. Is strictly based on matching column names. If names are mismatched, the operation fails because it cannot resolve the column. 4. Nullability/Constraints The incoming data violates constraints (e.g., <code>NOT NULL</code> for a primary key). Fails if a <code>NOT NULL</code> constraint is violated by inserting a <code>NULL</code>. Applies to other rules like ensuring <code>price</code> is greater than zero. Fails if constraints are violated. 5. Extra Columns The source data contains more columns than the target table schema. Fails with a \"schema mishmash detected\" error. Is robust; it successfully inserts the data by focusing only on the columns defined in the target schema, ignoring the extra columns. <p>Since schema validation is critical for ensuring consistency and preventing corruption, the choice between <code>INSERT</code> and <code>MERGE</code> is often determined by how robustly you need to handle potential schema changes.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#schema-evolution","title":"Schema Evolution","text":"<p>Schema evolution is Delta Lake's capability to handle changes to a table's schema, such as adding, reordering, or modifying columns, without having to completely rewrite the underlying table or data. This functionality is crucial for maintaining flexibility in data pipelines.</p> <p>The ability to evolve a schema accommodates several common changes:</p> <ol> <li>Adding new columns.</li> <li>Upgrading a data type to a larger, more accommodating type (type widening).</li> <li>Handling changes within nested structures.</li> <li>Changing the physical position of columns.</li> </ol> <p>Adding New Columns (Scenario 1)</p> <p>New columns can be added using two main approaches:</p> <p>Manual Approach (<code>ALTER TABLE ADD COLUMN</code>): The user explicitly runs the <code>ALTER TABLE ADD COLUMN</code> statement to update the table's schema. Subsequent <code>INSERT</code> or <code>WRITE</code> operations that include this new column will succeed. Rows that existed before the new column was added will contain <code>NULL</code> values for that column.</p> <p>Automatic Schema Evolution: This approach is enabled by setting a specific table property to <code>true</code>. If the incoming data contains a new column that does not exist in the target Delta table, the system automatically accommodates it without requiring a manual <code>ALTER</code> statement.</p> <p>Type Widening (Scenario 2)</p> <p>Type widening refers to the ability to upgrade a data type to a \"bigger type\" to handle larger values, such as converting an <code>INT</code> to a <code>BIGINT</code> or a <code>FLOAT</code> to a <code>DOUBLE</code>.</p> <p>Enforcement: This capability was introduced in Delta version 3.2 (which corresponds to Databricks Runtime 15.4 used in the demonstration).</p> <p>Manual Change Required: Even with automatic schema evolution enabled, type widening often does not happen automatically. If an incoming value exceeds the capacity of the current column type (e.g., a huge number for an <code>INT</code> column), the system fails with an error. The user must manually run an <code>ALTER COLUMN</code> statement to change the type (e.g., <code>ALTER COLUMN customer ID TYPE BIGINT</code>) before the write will succeed.</p> <p>Nested Structure Evolution (Scenario 3)</p> <p>Delta Lake supports schema changes within nested <code>STRUCT</code> data types.</p> <p>Adding Attributes: You can add a new attribute to an existing <code>STRUCT</code> column using <code>ALTER TABLE ADD COLUMN</code> with the dot notation (e.g., <code>ALTER TABLE... ADD COLUMN purchase_details.store_location STRING</code>).</p> <p>Changing Nested Types: You can also manually change the data type of an attribute inside a <code>STRUCT</code> (e.g., changing <code>mall_pin_code</code> from <code>INT</code> to <code>BIGINT</code>) using <code>ALTER COLUMN purchase_details.mall_pin_code TYPE BIGINT</code>.</p> <p>Automatic Nested Evolution: To automatically add a new attribute to a nested <code>STRUCT</code> during an insert, the user must utilize a named struct. This is necessary because if a simple positional insert is used, the system won't know the key of the new attribute, leading to inconsistencies.</p> <p>Column Position Changes (Scenario 4)</p> <p>Delta Lake provides methods to control the physical placement of columns:</p> <p>Manual Positioning: When manually adding a column, you can specify its exact location using <code>FIRST</code> (to place it as the first column) or <code>AFTER [column_name]</code> (to place it after a specific column).</p> <p>Automatic Behavior: When automatic schema evolution is enabled (especially with a <code>MERGE</code> operation), if a new column is encountered in the source, Delta Lake generally appends that column as the last column in the table, even if the user intended a different position.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#parquet-to-delta","title":"Parquet to delta","text":"<p>Converting an existing Parquet file (or a folder containing Parquet files) into a Delta Lake table format is necessary to enable features like ACID transactions, time travel, and schema enforcement.</p> <p>The fundamental difference between a standard Parquet file and a Delta table is the presence of the transaction layer, or the Delta Log. When a directory contains Parquet data but no Delta Log folder, it is not yet a Delta table.</p> <p>The conversion process essentially creates this transactional layer and registers the existing data. There are two primary methods detailed in the video for converting Parquet to Delta:</p> <p>Using SQL / Shell Command</p> <p>The first method involves using the <code>CONVERT TO DELTA</code> command directly on the path of the Parquet data.</p> <p>This approach is straightforward and typically executed using Spark SQL or a shell command that interacts with the Delta Lake framework.When executed, this command instructs Delta Lake to perform the conversion.</p> <p>Using the Delta Table API</p> <p>The second method utilizes the Delta Lake API, typically within a Python or Scala environment, using the <code>DeltaTable</code> class.</p> <p>This requires importing the necessary library (e.g., <code>from delta.tables import DeltaTable</code>). The function used is <code>convertToDelta</code>. For example: <code>DeltaTable.convertToDelta(path, format='parquet')</code>.</p> <p>The Core Conversion Mechanism</p> <p>Regardless of the method used, the underlying process to transform the Parquet files into a Delta table is the same:</p> <ol> <li>Delta Log Creation: A new folder named <code>_delta_log</code> is created in the directory containing the Parquet files.</li> <li>Transaction Registration: The Delta Log is populated with its first transaction file, a JSON file (version <code>0.json</code>), which records the initial operation.</li> <li>Data Registration (The <code>ADD</code> Operation): This initial JSON file contains an ADD operation, which explicitly registers the existing Parquet file(s) as the contents of the newly created Delta table. This transaction registers the data.</li> </ol> <p>Once this transaction is recorded, the folder structure officially functions as a Delta Lake table.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#copy-on-write","title":"Copy on Write","text":"<p>Copy on Write is defined by the underlying architecture of data storage in Delta Lake: Parquet files.</p> <p>Immutable Files: Parquet files are immutable, meaning they cannot be modified directly once they are written to disk.</p> <p></p> <p>The Copy on Write Mechanism</p> <p>Since files cannot be changed directly, any operation that modifies the data must result in new files being created. Delta Lake follows the Copy on Write approach when deletion vectors are disabled.</p> <p>The mechanism involves three steps for any change (update, delete, or merge):</p> <ol> <li>Read: The system must read the entire existing Parquet file that contains the record(s) intended for modification.</li> <li>Apply Operation: The necessary changes (delete or update) are applied in memory.</li> <li>Rewrite: The entire file is then rewritten back to storage as a completely new file, either omitting the deleted records or including the updated records.</li> </ol> <p>For instance, to delete rows 1 and 6 from a file named <code>1.parquet</code>, Copy on Write rewrites the whole file into a new file (e.g., <code>2.parquet</code>) which omits those two rows. This new file (<code>2.parquet</code>) is then the one referenced for the latest version of the data.</p> <p>Computational Expense (The Drawback)</p> <p>The Copy on Write approach has a significant drawback: it is \"tremendously computationally expensive\".</p> <p>Costly Rewriting: The read and rewrite process is very costly. If a Parquet file contains 10 million rows and you only need to delete a \"few couple of rows,\" you still must read the whole file and rewrite the whole file back.</p> <p>Optimal Use Case</p> <p>Because Copy on Write involves expensive rewrites for frequent changes, it is not ideal for high-write scenarios.</p> <p>Read-Heavy Focus: CoW works best for use cases that are read heavy and where the frequency of writes (updates, deletes) is very low. If rights are high, the system will read and rewrite the whole file repeatedly, making the system slow.</p> <p>Copy on Write's performance limitations during frequent writes is the reason Delta Lake later introduced Deletion Vectors to enable the Merge on Read paradigm, which avoids these costly full file rewrites.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#merge-on-read","title":"Merge on Read","text":"<p>Merge on Read (MoR) is a critical concurrency paradigm in Delta Lake that is enabled when deletion vectors are turned on. It provides a high-performance alternative to the traditional Copy on Write approach.</p> <p></p> <ol> <li>The Core Problem Solved by MoR</li> </ol> <p>The fundamental challenge in data systems using immutable files (like Parquet, which Delta Lake uses under the hood) is how to handle updates and deletes efficiently. In the Copy on Write approach, any change requires reading the entire affected file, applying the change, and then rewriting the entire file back to storage. This rewrite process is \"tremendously computationally expensive,\" even if only a \"few couple of rows\" are modified in a file containing millions of records.</p> <ol> <li>The Mechanism: Deletion Vectors</li> </ol> <p>Merge on Read resolves this expensive rewrite problem by keeping the original Parquet files untouched.</p> <p>Recording Changes: Instead of modifying the data file, any changes (such as deletions) are recorded in a separate, much smaller file known as a deletion vector (DV).    DV Content: The deletion vector records the row number or index of the records that are intended for deletion.    Updates as Two Operations: An update operation is logically treated as a \"delete plus insert\". The old row is marked for deletion in the DV, and the newly updated row is written as a small, new Parquet file.</p> <ol> <li>The Read Process (The \"Merge\" Step)</li> </ol> <p>When a user runs a query to read the latest state of the table, the system uses the Merge on Read approach to combine the data:</p> <ol> <li>Read Files: The system reads the underlying Parquet file(s).</li> <li>Check DV: For each row read from the Parquet file, the system checks the deletion vector.</li> <li>Soft Delete: If the row's index is present in the DV, that row is treated as a soft delete and is skipped from the final output. If the row is not present in the DV, it is displayed.</li> <li> <p>Final State: The result is the latest state of the table, which is constructed from the original file combined with the information in the small DV bitmap file.</p> </li> <li> <p>Benefits and Optimal Use</p> </li> </ol> <p>Deletion vectors, and thus Merge on Read, bring significant performance advantages:</p> <p>Increased Write Performance: MoR avoids the costly process of rewriting the entire Parquet file for small changes. This dramatically reduces write latency.    Efficiency: It increases the performance of deletes, updates, and merge operations.    Best Use Case: MoR works best for use cases where data is updated frequently.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#deletion-vectors","title":"Deletion Vectors","text":"<p>The Problem Deletion Vectors Solve</p> <p>Underlying Delta Lake, data is stored in Parquet files, which are immutable.</p> <p>In the traditional approach, known as Copy on Write: if you want to update or delete just one record in a Parquet file containing millions of rows, the system must:</p> <ol> <li>Read the entire Parquet file.</li> <li>Apply the update or delete operation.</li> <li>Rewrite the entire file back, omitting the deleted row or including the updated row.</li> </ol> <p>This read-and-rewrite process is computationally expensive and is the bottleneck that deletion vectors aim to resolve.</p> <p>The Solution: Merge on Read and Deletion Vectors</p> <p>Deletion vectors enable Delta Lake to use the Merge on Read approach when they are enabled.</p> <p>Mechanism: Instead of rewriting the massive Parquet file, the original file remains untouched. The changes (deletions or updates) are recorded in a separate, small file called the deletion vector (DV).</p> <p>Content: The deletion vector acts as a bitmap, recording the row number or index of the records that are supposed to be removed or soft-deleted.</p> <p>Operation with Deletion Vectors</p> <p>When a transaction occurs, the transaction log is updated, often including the deletion vector:</p> <p>Delete Operation: A delete operation simply records the row numbers of the deleted records in the deletion vector file. For example, deleting row 1 and row 6 results in those numbers being recorded in the DV.    Update Operation: An update is treated as a \"delete plus insert\". When updating a row, the old version of the row is marked for deletion in the deletion vector, and the newly updated row is written as a new, small Parquet file.</p> <ol> <li>Reading the Data</li> </ol> <p>When a user reads the latest state of the table, the system checks both the Parquet data and the deletion vector:</p> <p>The system reads the Parquet file.    For each row read, it checks the deletion vector.    If the row's index is present in the DV, it is treated as a soft delete and is skipped from the final output, ensuring the file doesn't have to be rewritten for every minor change.</p> <ol> <li>Benefits and Tradeoffs</li> </ol> <p>Deletion vectors provide significant advantages:</p> <p>Performance: They reduce write latency by avoiding the rewriting of the whole file for small changes.    Efficiency: They bring Merge on Read capability to Delta Lake, increasing the performance of deletes, updates, and merge operations.</p> <p>However, the Merge on Read approach (with DVs enabled) works best for use cases where data is updated frequently. Conversely, the Copy on Write approach (DVs disabled) is generally better for read-heavy use cases.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#shallow-vs-deep-clone","title":"Shallow v/s Deep Clone","text":"<p>Cloning is a functionality in Delta Lake that allows you to create a snapshot of your Delta tables at a specific point in time [i, 148]. There are two main approaches or \"flavors\" to cloning: shallow clone and deep clone.</p> <p>Shallow Clone</p> <p></p> <p>A shallow clone involves taking a snapshot primarily of the table's metadata at a specific point in time.</p> <p>Mechanism and Data Storage:    When a table is shallow cloned, Delta Lake does not copy the underlying data files (such as Parquet files).    Instead, the shallow clone table simply references the underlying data files of the source table.    The target shallow clone table will start with a fresh history, beginning at version zero.    The metadata itself is duplicated, but not via a one-to-one copy of the source JSON transaction files. All information in the source table's delta log is condensed into a <code>0.checkpoint.parquet</code> file, and the latest state of the source table is computed and recorded in <code>0.json</code> in the target table's delta log.</p> <p>Performance and Cost:    The shallow clone operation is super fast and computationally cheap.    It does not use a lot of storage because the data files are only referenced, not copied.</p> <p>Independence and Limitations:    Once a shallow clone is created, it maintains its own history and set of operations.    Any changes, updates, or modifications made to the shallow clone table will not affect the source table, and similarly, changes to the source table will not affect the shallow clone table.    You can clone a table using a specific timestamp or version number from the source table, using <code>VERSION AS OF</code> or <code>TIMESTAMP AS OF</code>.    Since the shallow clone starts with a fresh version zero representing the latest state of the source, you cannot access the full history of the source table (e.g., versions 1 or 2) through the shallow clone table.    If a <code>VACUUM</code> operation is run on the source table after a shallow clone has been created, any underlying data files that are still being referenced by the shallow clone will not be removed from the source storage until those references are deleted in the shallow clone(s) as well.</p> <p>Deep Clone (or Copy)</p> <p> A deep clone, or deep copy, creates a completely independent copy of both the metadata and the data files.</p> <p>Mechanism and Data Storage:    A deep clone performs an exact replication of the underlying Parquet files from the source to the target location.    The target deep clone table is self-contained and independent.    Similar to shallow clones, the metadata starts at version zero in the target table, with the source transaction history being condensed into a checkpoint file (<code>0.checkpoint.parquet</code>) and the latest state computed and placed in <code>0.json</code>.</p> <p>Performance and Cost:    This operation takes a little bit longer and uses more storage compared to a shallow clone, because it copies the data files instead of referencing them.</p> <p>Independence and Advantages:    The deep clone is fully independent: changes in the deep clone table will not affect the source table, and changes in the source table will not affect the deep cloned table.    Deep clone operations are incremental. If you perform a subsequent deep clone (e.g., for disaster recovery synchronization), it does not copy the entire source table again. Instead, it only copies or syncs the incremental changes (such as updates or deletes) that have occurred since the last clone, making the operation robust and performant.    Deep cloning is considered a more robust way to clone than using a <code>CREATE TABLE AS SELECT (CTAS)</code> statement. A CTAS statement only uses the resulting rows to create a new table, often losing table properties, constraints, and partitioning details. A deep clone, however, also clones the metadata and properties of the table.</p> Feature Shallow Clone Deep Clone Data Files Referenced (not copied) Independent copy (copied one-to-one) Metadata Copied/Replicated Copied/Replicated Speed Super fast Takes a little bit longer Storage Use Computationally cheap, low storage Uses more storage Independence Independent history/changes post-clone Completely self-contained and independent <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#ctas-vs-deep-clone","title":"CTAS vs Deep Clone","text":"<p>The distinction between creating a table using a CTAS (CREATE TABLE AS SELECT) statement and performing a Deep Clone lies primarily in how the metadata, properties, and incremental changes are handled.</p> <p>Output Similarity</p> <p>Initially, if you were to look at the resulting tables at the output level (for example, by running a <code>SELECT *</code> query), the table generated using CTAS and the table generated using a deep clone would look exactly identical. They both contain the same underlying rows of data.</p> <p>Key Differences (Metadata and Properties)</p> <p>The crucial difference is what information is carried over from the source table to the new table:</p> <ol> <li>Handling of Properties: A CTAS statement creates a table just using the output of a select query. It takes the resulting set of rows and uses those rows to create the new table. Consequently, all of the properties are lost when using CTAS. These lost properties can include partitioning configurations and constraints.</li> <li>Robustness of Deep Clone: Deep clone, conversely, is described as a robust way to clone the metadata, the data, and the properties of the table. When you perform a deep clone, you do not need to respecify things like partitioning properties and constraints, because they are automatically cloned along with the data.</li> </ol> <p>Incremental Feature (Deep Clone Advantage)</p> <p>Another significant advantage of deep clone over CTAS is its ability to operate incrementally:</p> <ul> <li>Incremental Synchronization: Deep cloning is designed to work in an incremental manner. This capability is useful for scenarios like disaster recovery, where you need to maintain a synchronous replica of a source table.</li> <li>Performance on Resync: If you perform a deep clone to create a replica initially, and then later run the deep clone again to bring the tables back in sync (after updates or deletes occurred on the source), Delta Lake does not copy the whole source table again. Instead, it only copies or syncs the incremental changes that have occurred since the last clone. This ensures that the operation is performant and robust because only the necessary operations (updates or deletes) are applied to the replica.</li> </ul> <p>In summary, while CTAS is good for creating a simple, static snapshot of the rows, Deep Clone is a more sophisticated mechanism that ensures the integrity of the table structure and settings (metadata and properties) is maintained, and it allows for efficient, incremental synchronization.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#small-file-problem","title":"Small File Problem","text":"<p>The small file problem is a phenomenon that silently kills your Spark performance. It occurs when data intended to be part of a large dataset is scattered over thousands of smaller files.</p> <p>What is the Small File Problem?</p> <p>The small file problem arises because, although the total volume of data might be reasonable, the large number of small files introduces immense overhead for the processing engine.</p> <p>The source uses the analogy of a 300-page novel: instead of being a single 300-page PDF, someone saves each of the 300 pages as a separate PDF. To read the novel, the system would have to perform thousands of time-consuming operations:</p> <ol> <li>Find/Look up the file.</li> <li>Open the file.</li> <li>Read the file.</li> <li>Close the file.</li> </ol> <p>When reading a table scattered across thousands of smaller files, the thousands of open, close, and metadata lookup operations lead to wasted compute and poor I/O.</p> <p>Root Causes of the Small File Problem</p> <p>The video identifies three primary root causes that lead to the creation of excessive small files:</p> <ol> <li>Repartitioning to a very large number: If you have a file that is 10 GB in size and you use <code>.repartition(10000)</code>, you will end up with 10,000 resulting files, each only about 1 megabyte in size. These undersized files cause the small file problem.</li> <li>Partitioning on a high cardinality column: When partitioning data (using <code>partitionBy</code>) on a column that has a large number of distinct values (high cardinality, e.g., thousands of distinct values in a category column), you create many separate folders, each containing very small amounts of data.</li> <li>Frequently updated data sets: Data streams that receive continuous updates, perhaps every 5 minutes, write small updates in small chunks. These small chunks manifest as small files, leading to the small file problem.</li> </ol> <p>Solution: The <code>OPTIMIZE</code> Command</p> <p>Delta Lake provides a built-in operation called <code>OPTIMIZE</code> to solve the small file problem.</p> <p>The <code>OPTIMIZE</code> command works by compacting all of these small files into larger, more appropriately sized ones.</p> <p>Bin Packing Algorithm</p> <p>The compaction relies on an algorithm known as bin packing. This algorithm works as follows:</p> <ol> <li>It collects all file sizes and sorts them from high to low.</li> <li>It then places each file into a \"bin,\" where each bin represents a new, large consolidated file.</li> <li>The goal is to fill the bins up to a default target file size of 1 GB. This 1 GB target size is considered robust and should not be changed unless there is a compelling reason to do so.</li> </ol> <p>Post-Optimization Cleanup</p> <p>When <code>OPTIMIZE</code> is run, it creates the new, larger files but does not immediately remove the original small files.</p> <p>The small files that were compacted are tombstoned (marked for soft delete).    To physically remove the tombstoned files and recover storage space, you must run the <code>VACUUM</code> command.</p> <p>Incremental Compaction Approaches</p> <p>Beyond manual optimization, Delta Lake offers two automatic approaches to address small files generated during writes:</p> <ol> <li>Optimize Write: This feature combines all small writes intended for a partition into a single write command. It shuffles all the data and executes a single write command, which produces appropriately sized files for every partition. While effective for creating appropriate file sizes, it incurs a shuffle, which can be costly in terms of data transfer and right latency.</li> <li>Auto Compaction: This runs a compaction operation (similar to <code>OPTIMIZE</code>) automatically after a write finishes, if the number of small files exceeds a minimum threshold (the default minimum number of files to trigger auto compaction is 50, though this can be configured). This approach combines the small files into appropriately sized files.</li> </ol> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#optimize-write","title":"Optimize Write","text":"<p>The feature Optimize Write is an automatic optimization technique used in Delta Lake, specifically designed to address one of the root causes of the small file problem.</p> <p>Here is a detailed explanation of how Optimize Write works and its implications, as described in the video:</p> <p>The Problem it Solves (Traditional Write)</p> <p>In a traditional write scenario, multiple processes or executors (e.g., Executor 1, 2, and 3) often try to write data concurrently to the same partition (for example, a date partition like <code>date=2025-05-01</code>). Since these writes happen simultaneously in small chunks, they result in the creation of many small Parquet files (e.g., <code>1.pk</code>, <code>2.pk</code>, <code>3.pk</code>) within that single partition folder. This proliferation of tiny files leads directly to the small file problem, which silently reduces Spark performance due to excessive file opening, closing, and metadata lookups.</p> <p>Mechanism of Optimize Write</p> <p>Optimize Write operates by transforming multiple intended small writes into a single, cohesive write operation.</p> <ol> <li>Data Shuffle: When Optimize Write is enabled, all data coming from different processes (Executor 1, 2, and 3) is first shuffled.</li> <li>Single Write Command: After the shuffle, the entire dataset intended for the partition is executed as a single write command.</li> </ol> <p>This process combines all the data together before it is physically written to the storage location.</p> <p>Outcome and Benefits</p> <p>The key benefit of using Optimize Write is that it produces appropriately sized files for every partition. By consolidating the output, it ensures that instead of scattering data across numerous small files, the data is packaged efficiently into large files (up to the target size, typically 1 GB), effectively avoiding the small file problem right at the source.</p> <p>Trade-offs and Costs</p> <p>While highly effective at preventing small files, Optimize Write introduces a notable trade-off:</p> <p>Shuffle Cost: The operation necessarily incurs a shuffle. Shuffles are costly because they involve data transfer across the network.    Write Latency: If your primary concern is optimizing for write latency (the time it takes for data to be written and become available), Optimize Write might not be the best solution because the involved shuffle will add time to the overall writing process.</p> <p>Therefore, Optimize Write is generally viewed as a very good solution for optimizing for the small file problem, but users must consider the added shuffle overhead. The source demonstrated this, showing that a query operation ran much quicker (1.82 seconds) on a table created with Optimize Write compared to a table created using traditional, small-file-generating methods (6.97 seconds).</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#vacuum","title":"Vacuum","text":"<p>The <code>VACUUM</code> command in Delta Lake is a crucial optimization technique used for managing the physical storage of data files associated with a Delta table.</p> <p>Primary Purpose and Mechanism</p> <p>The core function of <code>VACUUM</code> is to physically remove files from cloud storage or disk. Delta Lake employs a soft delete mechanism for data that is no longer part of the current table state:</p> <ol> <li>Soft Deletion: Whenever records are deleted, updated, or merged in a Delta table, the older, irrelevant underlying records or files are tombstoned or marked for soft deletion. This is done to maintain time travel and ACID properties without immediate costly file rewriting or removal.</li> <li>Physical Deletion: These tombstoned files are not physically deleted from storage until the <code>VACUUM</code> command is explicitly run. Running <code>VACUUM</code> removes all such files that are no longer referenced in the transaction log and have exceeded the retention duration.</li> <li>Storage Savings: The key benefit of running <code>VACUUM</code> is that it helps save on storage cost by permanently removing unnecessary files.</li> </ol> <p>Key Considerations</p> <ol> <li> <p>Query Performance: A popular misconception is that running <code>VACUUM</code> makes queries run faster. This is not the case. Query performance is determined by file filtering and pruning handled by the Delta Log, which directs the engine to scan only the necessary active files. <code>VACUUM</code> only cleans up the retired physical files, and the data being scanned remains the same.</p> </li> <li> <p>Retention Duration and Safety Checks: <code>VACUUM</code> respects a retention duration setting to ensure that time travel capabilities are not compromised prematurely.</p> </li> </ol> <p>Default Duration: The default retention duration is 168 hours (7 days). Files created within this period are generally not deleted.    Overriding Retention: To remove files that have been marked for soft delete even if they fall within the default retention period (e.g., immediately after an <code>OPTIMIZE</code> operation), you must specify a shorter duration, such as <code>RETAIN 0 HOURS</code>.    Safety Warning: Delta Lake usually issues a warning when a user attempts to run <code>VACUUM</code> with a low retention duration (e.g., zero hours). To proceed with low retention, a configuration check must be disabled: <code>set spark.databicks.delta.retentiondurationcheck.enable = false</code>.</p> <ol> <li>Limitation on Time Travel: Running <code>VACUUM</code> limits your ability to time travel. If a file associated with a previous version is physically deleted by the <code>VACUUM</code> operation, attempting to access that specific old version (<code>VERSION AS OF</code> or <code>TIMESTAMP AS OF</code>) will fail because the required underlying file data is missing.</li> </ol> <p>Interaction with Other Operations</p> <p><code>OPTIMIZE</code> Compaction: When the <code>OPTIMIZE</code> command is run to compact small files into one large file, the original small files are tombstoned. These small files remain physically present until <code>VACUUM</code> is run to remove them and recover the storage space.    Shallow Clones: <code>VACUUM</code> operations on a source table are constrained by active references from shallow clones. If a file in the source table is deemed obsolete (orphaned) but is still being referenced by a shallow clone, running <code>VACUUM</code> on the source table will not delete that file. The file will only be deleted after all references, including those in all shallow clones, are removed (e.g., by running a delete operation on the relevant data in the shallow clone tables) and then <code>VACUUM</code> is re-run on the source.    Deep Clones: <code>VACUUM</code> runs independently on deep clone tables. Since a deep clone is a self-contained and independent copy of the source data with no references linking back, running <code>VACUUM</code> on the source table will not affect the deep clone, and vice versa.</p> <p>The <code>VACUUM</code> command is necessary housekeeping in Delta Lake, ensuring that while the transaction log maintains a consistent view of the data, the storage layer only retains necessary files, similar to how recycling services remove old, unnecessary containers to free up space.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#z-ordering","title":"Z-ordering","text":"<p>The concept of Z-Ordering is one of the optimization techniques provided by Delta Lake, designed primarily to enhance query performance by maximizing data skipping and minimizing costly data transfers.</p> <p>The Problem Z-Ordering Solves (Costly Data Transfer)</p> <p>Before Z-ordering is applied, Delta Lake tables, like standard Parquet tables, store statistics (such as minimum and maximum values) for columns within the Delta Log.</p> <p>When a user submits a query with a filter (e.g., <code>WHERE age &gt;= 5 AND age &lt;= 10</code>), the query engine uses these statistics to determine which underlying data files need to be scanned and transferred into the memory of the compute cluster.</p> <p>If the required filter range overlaps with the minimum and maximum values recorded for a column in a specific Parquet file, that entire file must be transferred over the wire.    In poorly organized data, the statistics in different files often overlap significantly. This means that a filtered query may result in the necessary transfer and reading of all underlying data files, even if only a few records within those files actually match the criteria. Transferring all these files is a costly operation that consumes compute resources and leads to poor I/O performance.</p> <p>Mechanism of Z-Ordering</p> <p>Z-Ordering fundamentally optimizes the physical layout of data to minimize these file overlaps. It achieves this through a process often described as a sort and repartition.</p> <ol> <li>Collocating Similar Data</li> </ol> <p>The central idea is to collocate similar data points into the same physical data files. By grouping data together, the statistical ranges (min/max) of the resulting files become much tighter, meaning they are less likely to overlap.</p> <p>When a query is run after Z-ordering, the tight statistical ranges allow the engine to prune (filter out or skip) files that clearly do not contain the relevant data, avoiding their transfer over the network.</p> <ol> <li>Multi-Dimensional Mapping and Locality</li> </ol> <p>Z-Ordering is particularly powerful because it can apply this sorting principle across multiple columns simultaneously (multi-dimensional filtering).</p> <p>It works by taking the values from the selected columns and mapping this multi-dimensional space into a single dimension.    This mapping is done using a mathematical technique called a space-filling curve.    The curve ensures that data points which are close to each other in the original multi-dimensional space remain close to each other after being mapped to the single dimension (this concept is called preserving locality).</p> <p>This single, optimized dimension is then used to physically sort and arrange the data within the files, ensuring similar data points are grouped together.</p> <p>Using Z-Order with <code>OPTIMIZE</code></p> <p>The <code>ZORDER BY</code> command is not run in isolation; it must be executed together with the <code>OPTIMIZE</code> command.</p> <p>The two operations work hand in hand:</p> <ol> <li><code>OPTIMIZE</code> Compaction: The <code>OPTIMIZE</code> command\u2019s role is to solve the small file problem by merging many small files into fewer, larger files (called \"bins\"), typically targeting a size of 1 GB.</li> <li>Z-Ordering Collocation: While <code>OPTIMIZE</code> is consolidating the small files, Z-Order simultaneously performs the necessary collocation (sorting and rearranging) of the data into the new, larger files.</li> </ol> <p>Running <code>OPTIMIZE ZORDER BY [columns]</code> leads to significant performance improvements, often seeing query runtimes decrease substantially because the number of files scanned is drastically reduced.</p> <p>Z-Ordering with Partitions</p> <p>Z-Ordering can be applied incrementally and selectively, particularly when dealing with Hive-style partitions (e.g., partitioning by date). If a table is partitioned (e.g., by <code>invoice date</code>), you can use a predicate or <code>WHERE</code> condition to apply Z-Ordering only to specific, recently updated partitions (e.g., <code>OPTIMIZE ... WHERE category = 'fruit'</code>). This means the expensive Z-Ordering operation is only performed on the small subset of new data, rather than the entire table.</p> <p>Z-Ordering is like organizing a massive library collection not just by author (which is like partitioning) but also simultaneously by subject, genre, and length, so that when a reader asks for \"short sci-fi books from 2020,\" the librarian (the query engine) only needs to walk to one aisle and look at two shelves, instead of checking every aisle and every shelf just in case a relevant book overlapped into that section.</p> <p>-------------------------------------------------------------------------------------------------------------</p>"},{"location":"delta/deltalake/#liquid-clustering","title":"Liquid Clustering","text":"<p>Liquid Clustering is an advanced optimization technique in Delta Lake designed to provide flexibility and robust query performance by dynamically managing the physical data layout.</p> <p>It solves the primary problem of inflexibility inherent in traditional Hive-style partitioning and Z-Ordering.</p> <p>Limitations of Traditional Methods Solved by Liquid Clustering</p> <p>Hive-style partitioning and Z-Ordering require the user to decide the columns upfront when defining the table.</p> <p>If your query filter pattern changes over time (e.g., switching from filtering by <code>country</code> to filtering by <code>category</code>), the original data layout becomes unhelpful.    To address the new filter pattern, you would typically need to rewrite the entire dataset according to the new schema or layout.</p> <p>Liquid Clustering solves this by allowing you to change the clustering columns anytime, making the approach very flexible. It is also incremental, meaning that when you change clustering columns later on, the data layout changes going ahead to reflect the new desired pattern.</p> <p>Mechanism and Core Principles</p> <p>The liquid clustering algorithm is designed to maintain a balanced layout in the data files. It ensures several key outcomes simultaneously:</p> <ol> <li>Uniform File Size: It ensures that files are of uniform or appropriate file size. It avoids creating large files that skew processing or very small files that harm performance.</li> <li>Appropriate Number of Files: It manages the data layout to ensure an appropriate number of files, thereby helping to avoid the small file problem.</li> <li>Data Collocation: Similar data is collocated (placed close together) within the same physical files, which maximizes data skipping during query execution.</li> </ol> <p>How Liquid Clustering Addresses Small Files and Data Skew</p> <p>Liquid Clustering directly addresses two significant performance inhibitors: the small file problem and data skew.</p> <ol> <li>Avoiding the Small File Problem</li> </ol> <p>By ensuring uniform and appropriate file sizes, LC avoids the small file problem. It achieves this by:</p> <p>Merging Small Files: It combines many small files into appropriately sized files.    Breaking Down Large Files: Conversely, it can also break down bigger files that are too large (which might otherwise cause data skew) into appropriately sized chunks.</p> <ol> <li>Avoiding Data Skew</li> </ol> <p>Data skew occurs when some partitions contain a disproportionately large amount of data compared to others, creating bottlenecks and leaving compute resources idle.</p> <p>In a real-world scenario where partitions (e.g., partitioned by year) might vary drastically in size, the tasks processing the largest partitions must complete before the job finishes (these are on the \"critical path\").    Liquid Clustering works by dividing the larger chunks into smaller buckets. This ensures that all processing tasks or cores receive a uniform amount of data to process, avoiding the skew problem and resulting in higher resource utilization and faster completion times.</p> <p>Relationship with Optimization Techniques</p> <p>Liquid clustering cannot be used together with Z-Ordering or Hive-style partitioning; it serves as the independent, primary mechanism for organizing the data layout.</p> <p>When choosing clustering columns, best practices suggest:    Select the columns most frequently used in your query filters.    If transitioning from a table that used both Hive partitioning and Z-Ordering, use both the partition column and the Z-Order column as the new clustering key.    If certain columns are highly correlated (meaning choosing one implies the value of the other), only include the primary one as the clustering key.</p> <p>Liquid clustering provides the flexibility to change clustering columns down the line, avoiding the need to fix partitioning or Z-Order upfront, which protects against changing query patterns.</p> <p>Liquid clustering functions like a smart warehouse management system that doesn't just sort inventory based on a single, fixed attribute (like traditional partitioning/Z-Ordering). Instead, it continuously and dynamically rearranges the goods into uniformly sized boxes, grouping similar items together, and automatically breaking down massive shipments into manageable loads, ensuring that no worker is overwhelmed by one oversized box and that retrieval remains efficient even if customer demand shifts to a different set of categories.</p>"},{"location":"devops/docker/","title":"Docker","text":""},{"location":"devops/docker/#what-is-docker","title":"What is Docker?","text":"<p>Docker is a platform for packaging, distributing, and running applications. It achieves this by enclosing applications within lightweight, portable units called containers.</p> <p>The Packaging Process to distribute an application (e.g., Python code or ETL application) across different environments (QA, Prod), the code must first be packaged. Docker provides the platform to package the code and run it within isolated containers</p> <p>Docker was first publicly released in March 2013, developed by Solomon Hykes and his team.</p> <p>Docker's logo features a whale carrying containers, symbolizing its ability to transport and deploy applications easily from one place to another, much like ships carry physical containers across the ocean.</p> <p>Docker is open-source, meaning it's free to use, though enterprise versions with added support are available.</p> <p>Docker operates as a Platform as a Service (PaaS) in the cloud. It provides a platform to run your applications, similar to how a railway track (platform) allows a train (code) to run.</p>"},{"location":"devops/docker/#docker-vs-traditional-virtualization-vmwarehypervisor","title":"Docker vs. Traditional Virtualization (VMware/Hypervisor)","text":"Feature Traditional Virtual Machine (VMware/Hypervisor) Docker (Containerization) Virtualization Level Hardware-level virtualization. A hypervisor (e.g., ESXi, VMware Workstation) is installed directly on the physical hardware or on top of an OS. OS-level virtualization. Docker Engine runs on top of the host operating system. Operating System Each VM requires its own full guest operating system (OS) (e.g., Windows, Linux). This consumes significant storage and RAM. Containers do not have their own full guest OS. They share the host OS's kernel. Resource Allocation Pre-allocated resources (RAM, storage, CPU) for each VM. These resources are dedicated to the VM whether it's actively using them or not. Resources are consumed on demand. Containers take resources from the host OS as needed and release them when done. Size VMs are heavyweight (e.g., 2-4 GB for an OS, plus application data). Containers are lightweight (e.g., 33MB for an Ubuntu image). Boot Time Slower boot times as each VM's OS needs to boot up. Extremely fast creation and startup. Portability VMs are portable but larger and less efficient to move. Highly portable. Containers can run on physical hardware, virtual hardware, or cloud platforms (e.g., AWS, Azure, Google Cloud). Dependency Management Manually manage dependencies within each VM's OS. Docker automatically pulls required dependencies from Docker Hub when creating containers or images. Cost Can be more expensive due to higher resource consumption and potential licensing costs for each OS. Cost-effective due to less resource consumption and no need for separate OS licenses. Isolation Provides strong isolation at the hardware level. Provides OS-level isolation. Processes within a container are isolated from other containers and the host system. <p>In Docker, 95% of the operating system code needed by a container is taken from the host operating system's Linux kernel (since Docker's native design is for Linux). The remaining 5% are specific files for the desired OS distribution (like Ubuntu or Kali Linux), which are part of the Docker image. This makes containers extremely lightweight and efficient.</p>"},{"location":"devops/docker/#docker-ecosystem-components","title":"Docker Ecosystem Components","text":"<p>Docker's core functionality revolves around OS-level virtualization, also known as containerization, which is an advanced form of virtualization.</p> <p>The Docker ecosystem is a set of software tools and packages that work together to create and manage containers.</p> <p>Docker Client: This is where Docker users interact with the Docker Daemon (server). Users write commands in the CLI (Command Line Interface) or use REST APIs to communicate with the Daemon. A single Docker client can communicate with multiple Docker servers.</p> <p>Docker Daemon (Docker Engine/Docker Server): This runs on the host operating system and is responsible for building images, running containers, and managing Docker services. It converts Docker images into running containers and executes commands from the Docker Client.</p> <p>Docker Hub (Registry): This is a cloud-based storage and management service for Docker images. It stores and categorizes images. </p> <p>Note</p> <p>Public Registry - Images are publicly accessible to everyone (e.g., Docker Hub itself).</p> <p>Private Registry - Companies use this to store images exclusively for their internal use and employees.</p> <p>Docker Images: These are read-only binary templates used to create Docker containers. They are like templates or Amazon Machine Images (AMIs). An image contains all the dependencies and configurations required to run a program or application.</p> <p>Note</p> <p>Images are read-only - You cannot make changes directly to an image. Any modifications require creating a new image from a modified container.</p> <p>Image states - When an image is running, it's called a container. When a container is stopped or paused, it becomes an image.</p> <p>Docker Containers: These are running instances of Docker images. They hold the entire package needed to run an application, including the OS files (the 5% specific ones) and supported software. Containers act like virtual machines but are much lighter. Containers are built in layers. When files or software are added to a container, they form new layers on top of previous ones. This layering allows for efficient storage and reusability. You can make modifications within a container, and then create a new image from that modified container.</p> <p>Docker Host: This refers to the physical hardware (laptop, server, cloud instance) on which the Docker Engine runs. It provides the environment and resources (CPU, RAM, storage) for Docker and its containers.</p>"},{"location":"devops/docker/#advantages-of-docker","title":"Advantages of Docker","text":"<p>Lightweight: Containers consume minimal resources (CPU, RAM, storage) compared to VMs because they don't include a full operating system. They only include the necessary components and share the host OS kernel.</p> <p>Cost-Effective: Due to lower resource consumption, Docker is cheaper than VMs. There's no pre-allocation of RAM, and no need for a separate OS installation for each application.</p> <p>No Pre-allocation of RAM: Docker only allocates RAM to a container when it's needed for an application, releasing it once the task is complete. This avoids resource wastage, unlike VMs where RAM is pre-reserved.</p> <p>Continuous Integration/Continuous Deployment (CI/CD) Efficiency: Docker simplifies the CI/CD pipeline. Developers build a container image, which can then be used consistently across every step of the deployment process (development, testing, production). This eliminates \"it works on my machine\" issues.</p> <p>Portability: Docker containers can run on any physical hardware, virtual hardware, or cloud platform (AWS, Azure, Google Cloud). This flexibility ensures applications run consistently across different environments.</p> <p>Image Reusability: Once an image is created (either from Docker Hub, a Dockerfile, or an existing container), it can be reused to create multiple containers or shared with other teams. This saves time and effort.    Faster Container Creation: Containers can be created in seconds or milliseconds, significantly faster than VMs, which can take minutes.</p>"},{"location":"devops/docker/#limitations-of-docker","title":"Limitations of Docker","text":"<p>Cross-Platform Compatibility Issues: Applications designed to run in Docker containers on Windows generally will not run on Linux Docker containers, and vice-versa.</p> <p>Even within Linux distributions, for optimal performance and compatibility, it's recommended that the development and testing/production environments use the same Linux distribution (e.g., Ubuntu development should be tested/deployed on Ubuntu).</p> <p>While a Linux container might run on a different Linux distribution (e.g., Ubuntu on CentOS), it might require downloading additional dependency files from Docker Hub.</p> <p>Docker was originally designed for Linux, and 99% of industry usage is on Linux. While Windows 10 (Pro, Enterprise, not Home) now supports Docker with Hyper-V enabled, it still uses Linux files within the Docker Desktop tool.</p> <p>Not Ideal for Rich Graphical User Interface (GUI) Applications: Docker is better suited for command-line interface (CLI) based applications rather than those requiring extensive graphical user interfaces. For rich GUI applications, VMs might be a better solution.</p> <p>Difficulty Managing Large Number of Containers: While Docker Swarm and Kubernetes help, managing a very large number of Docker containers can become complex.</p>"},{"location":"devops/docker/#dockerfile","title":"Dockerfile","text":"<p>A Dockerfile is a text file that contains a set of instructions for Docker to automatically build a Docker image. It's a method for automating Docker image creation and ensures consistent builds.</p> <p>The filename must be exactly <code>Dockerfile</code>, with the 'D' capitalized.</p> <p>Instructions within the Dockerfile must be in capital letters.</p> <p>Reusability: You can change instructions in a Dockerfile multiple times to create different new images. Each build from a Dockerfile creates a new, independent image.</p> <p>Dockerfile Instructions &amp; Commands</p> <p><code>FROM &lt;image&gt;:&lt;tag&gt;</code>: (Mandatory, must be the first instruction) Specifies the base image for your new image (e.g., <code>FROM ubuntu</code>). This is the foundation upon which your custom image will be built.</p> <p>Example</p> <p><code>FROM ubuntu</code></p> <p><code>RUN &lt;command&gt;</code>: Executes commands during the image build process. Each <code>RUN</code> instruction creates a new layer in the image.</p> <p>Example</p> <p><code>RUN echo \"Subscribe Technical Guftgu\" &gt; /tmp/test_file</code></p> <p><code>COPY &lt;source&gt; &lt;destination&gt;</code>: Copies files/directories from the local machine (where the Dockerfile is located) into the Docker image.</p> <p>Example</p> <p><code>COPY test_file_1 /tmp/</code></p> <p><code>ADD &lt;source&gt; &lt;destination&gt;</code>: Similar to <code>COPY</code>, but has additional functionalities: Can fetch files from a URL. Can automatically extract (unzip) compressed files (e.g., <code>.tar</code>, <code>.zip</code>) into the destination directory.</p> <p>Example</p> <p><code>ADD test.zip /tmp/</code></p> <p><code>EXPOSE &lt;port&gt;</code>: Informs Docker that the container listens on the specified network ports at runtime. It's a declaration, not an actual publishing of the port. For public access, ports must be published (e.g., using <code>-p</code> option with <code>docker run</code>).</p> <p>Example</p> <p><code>EXPOSE 80</code> (for HTTP) or <code>EXPOSE 8080</code> (for Jenkins).</p> <p><code>WORKDIR &lt;path&gt;</code>: Sets the working directory for any <code>RUN</code>, <code>CMD</code>, <code>ENTRYPOINT</code>, <code>COPY</code>, or <code>ADD</code> instructions that follow it. If the directory doesn't exist, it will be created.</p> <p>Example</p> <p><code>WORKDIR /tmp</code></p> <p><code>CMD &lt;command&gt;</code>: Provides defaults for an executing container. It's the primary command that runs when a container starts. There can only be one <code>CMD</code> instruction per Dockerfile. If multiple <code>CMD</code> instructions are present, only the last one takes effect.</p> <p><code>ENTRYPOINT &lt;command&gt;</code>: Similar to <code>CMD</code>, but it sets the main command for the container's execution. Arguments to <code>ENTRYPOINT</code> are always run, even if <code>CMD</code> is also provided. It has higher priority than <code>CMD</code>.</p> <p><code>ENV &lt;key&gt;=&lt;value&gt;</code>: Sets environment variables within the Docker image. These variables persist when the container is run.</p> <p>Example</p> <p><code>ENV MY_NAME=\"Bhupender Rajput\"</code></p> <p><code>ARG &lt;name&gt;[=&lt;default value&gt;]</code>: Defines build-time variables that users can pass to the builder with the <code>docker build --build-arg &lt;name&gt;=&lt;value&gt;</code> command. (User's homework in source 67)</p>"},{"location":"devops/docker/#building-an-image-from-dockerfile","title":"Building an Image from Dockerfile","text":"<p>To build an image from a Dockerfile, navigate to the directory containing the Dockerfile and run the <code>docker build</code> command.</p> <p>Command</p> <p><pre><code>docker build -t &lt;image_name&gt; .\n</code></pre> <code>-t &lt;image_name&gt;</code>: Tags the image with a name (e.g., <code>my_image</code>).</p> <p><code>.</code> (dot): Refers to the current directory, indicating that the Dockerfile is in the current working directory.</p> <p>Example</p> <pre><code># Assuming Dockerfile is in current directory\ndocker build -t new_image .\n</code></pre> <p>When a Docker Image runs as a Container, the image content is held in read-only layers. The container adds a writable layer on top of the read-only layer.</p> <ol> <li> <p>Function: The writable layer allows the running application to write logs, create files, or make temporary on-the-spot changes within the container.</p> </li> <li> <p>Isolation/Security: This design promotes isolation and security because the container cannot directly modify the underlying image layers. If the container were allowed to change the image, those changes might break the image when deployed to a production environment.</p> </li> </ol>"},{"location":"devops/docker/#docker-volumes","title":"Docker Volumes","text":"<p>Docker Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. They are normal directories that are declared to act as volumes.</p> <p>Data in volumes persists even if the container is deleted. A single volume can be shared and accessed by multiple containers simultaneously. Volumes can map a directory on the Docker host machine to a directory inside the container, allowing seamless data synchronization between them.</p> <p>A directory must be declared as a volume only while creating a container or in a Dockerfile (for image creation). You cannot create a volume from an already existing container.</p> <p>Data put into a shared volume by one container will be visible to all other containers sharing that same volume.</p> <p>If you create an image from a running container that has a volume, the volume's content will be included in the image. However, when a new container is created from this image, the volume will act as a simple directory; it will not be shared as a volume with other containers by default. Sharing needs to be explicitly re-established for new containers.</p> <p>Storage layers are required to either handle large data inputs or persist data generated by the application (e.g., database records or logs).</p> Storage Type Bind Mounts Named Volumes Purpose Used for reading large input data from the host machine into the running container. Used for persisting data generated by the container (e.g., SQL records). Mechanism A local host directory is mounted into the container path (available only during the container's runtime). An isolated storage location managed by Docker (residing on the host machine but inaccessible directly by the user) is connected to a container's internal data path (e.g., <code>/var/lib/mysql</code>). Persistence Data is available from the host source. Data persists even after the container is destroyed; subsequent containers can attach to the volume to retrieve the data. Command (Run) <code>-v /host/path:/container/path</code> (Used via Docker Compose to mount initialization scripts). <code>-v &lt;volume_name&gt;:&lt;container_path&gt;</code>."},{"location":"devops/docker/#methods-to-create-and-share-volumes","title":"Methods to Create and Share Volumes","text":"<p>Using Dockerfile (for image creation): Declare the volume within your <code>Dockerfile</code> using the <code>VOLUME</code> instruction.</p> <p>Example</p> <pre><code>FROM ubuntu\nVOLUME /my_volume  # Creates a directory named 'my_volume' in the container's root, declared as a volume\n</code></pre> <p>Build the image from this Dockerfile: <code>docker build -t my_image .</code></p> <p>Create a container from this image: <code>docker run -it --name container_one my_image /bin/bash</code></p> <p>Now, <code>/my_volume</code> inside <code>container_one</code> is a volume. You can navigate into it (<code>cd /my_volume</code>) and create files (<code>touch file_A file_B</code>).</p> <p>Using <code>docker run</code> command (direct volume declaration): This method allows direct volume creation and mapping when launching a container.</p> <p>Example</p> <pre><code>docker run -it --name container_three -v /volume_two ubuntu /bin/bash\n</code></pre> <p><code>-v /volume_two</code>: Creates a directory named <code>volume_two</code> inside <code>container_three</code> and declares it as a volume.</p> <p>Example</p> <pre><code>docker run -it --name host_container -v /home/ec2-user:/rajput ubuntu /bin/bash\n</code></pre> <p><code>-v &lt;host_path&gt;:&lt;container_path&gt;</code>: Maps a directory on the host machine (<code>/home/ec2-user</code>) to a directory inside the container (<code>/rajput</code>). Changes in either location will reflect in the other.</p> <p>Sharing Volumes Between Containers</p> <p>Volumes can be shared between containers using the <code>--volumes-from</code> option during container creation.</p> <p>Scenario: Share <code>my_volume</code> from <code>container_one</code> with <code>container_two</code>.</p> <p>Create <code>container_one</code> with a volume:</p> <p>Example</p> <p>docker run -it --name container_one my_image /bin/bash cd /my_volume touch file_1 file_2 file_3 exit</p> <p>Create <code>container_two</code> and share volume from <code>container_one</code>:</p> <p>Example</p> <p>docker run -it --name container_two --privileged=true --volumes-from container_one ubuntu /bin/bash</p> <p><code>--privileged=true</code>: Grants full read/write privileges to the shared volume.</p> <p><code>--volumes-from container_one</code>: Tells Docker to share all volumes from <code>container_one</code> with <code>container_two</code>.</p> <p>Verify Sharing:</p> <p>Inside <code>container_two</code>, navigate to <code>/my_volume</code>. You will see <code>file_1</code>, <code>file_2</code>, <code>file_3</code>.</p> <p>Create a new file in <code>/my_volume</code> from <code>container_two</code>: <code>touch new_file_from_two</code>.</p> <p>Exit <code>container_two</code>.</p> <p>Attach back to <code>container_one</code>: <code>docker attach container_one</code>.</p> <p>Navigate to <code>/my_volume</code> in <code>container_one</code> and list contents: <code>ls</code>. You will see <code>new_file_from_two</code>. This confirms bi-directional sharing.</p> <p>Volume Management Commands</p> <ol> <li> <p><code>docker volume ls</code>: Lists all Docker volumes on the local machine.</p> </li> <li> <p><code>docker volume create &lt;volume_name&gt;</code>: Creates a new Docker volume.</p> </li> <li> <p><code>docker volume rm &lt;volume_name&gt;</code>: Deletes a specific Docker volume.</p> </li> <li> <p><code>docker volume prune</code>: Deletes all unused (not attached to any container) Docker volumes.</p> </li> <li> <p><code>docker volume inspect &lt;volume_name&gt;</code>: Displays detailed information about a specific Docker volume.</p> </li> </ol>"},{"location":"devops/docker/#essential-docker-commands","title":"Essential Docker Commands","text":"<p>Here are basic and important Docker commands:</p> <p>Note</p> <p>Check Docker Service Status:     <pre><code>service docker status\n</code></pre>     Output: Shows if Docker is <code>running</code> or <code>stopped</code>.</p> <p>Note</p> <p>Start Docker Service:     <pre><code>service docker start\n</code></pre>     Purpose: Starts the Docker Engine.</p> <p>Note</p> <p>Check Docker Info (Detailed Information):     <pre><code>docker info\n</code></pre>     Purpose: Provides comprehensive details about Docker installation, including OS, memory usage, root directory, etc..</p> <p>Note</p> <p>List Docker Images:     <pre><code>docker images\n</code></pre>     Purpose: Lists all Docker images available on your local Docker Engine.</p> <p>Note</p> <p>Search Docker Hub for Images:     <pre><code>docker search &lt;image_name&gt;\n</code></pre>     Purpose: Searches Docker Hub for images related to the specified name (e.g., <code>ubuntu</code>, <code>jenkins</code>, <code>centos</code>).</p> <p>Note</p> <p>Pull an Image from Docker Hub:     <pre><code>docker pull &lt;image_name&gt;\n</code></pre>     Purpose: Downloads a specified image from Docker Hub to your local Docker Engine.     Example: <code>docker pull jenkins</code></p> <p>Note</p> <p>Run a Docker Container:     <pre><code>docker run -it --name &lt;container_name&gt; &lt;image_name&gt; /bin/bash\n</code></pre>     Purpose:</p> <pre><code>Creates a new container from an image and runs it.\n`-it`:\n    `-i`: Interactive mode (keeps STDIN open even if not attached).\n    `-t`: Allocate a pseudo-TTY (provides a terminal inside the container).\n`--name &lt;container_name&gt;`: Assigns a custom name to your container. If not specified, Docker generates a random name.\n`&lt;image_name&gt;`: The name of the image to create the container from (e.g., `ubuntu`, `centos`, `jenkins`).\n`/bin/bash`: Command to execute inside the container to get a bash shell.\nExample: `docker run -it --name my_ubuntu_container ubuntu /bin/bash`\n</code></pre> <p>Note</p> <p>Run a Container with Port Mapping:     <pre><code>docker run -d --name &lt;container_name&gt; -p &lt;host_port&gt;:&lt;container_port&gt; &lt;image_name&gt;\n</code></pre>         Purpose: </p> <pre><code>Creates and runs a container, mapping a port on the host to a port in the container, allowing external access.\n\n`-d`: Detached mode (runs the container in the background).\n\n`-p &lt;host_port&gt;:&lt;container_port&gt;`: Publishes (exposes) a container's port to the host.\n    `host_port`: The port on the Docker host (e.g., EC2 instance).\n    `container_port`: The port inside the container where the application is listening (e.g., 80 for HTTP, 8080 for Jenkins).\n\nExample (HTTP server): `docker run -d --name web_server -p 80:80 ubuntu`\n    (Note: For the web server example, you would then need to `docker exec` into the container, install Apache, and create a webpage as shown in the source to serve content.)\n\nExample (Jenkins server): `docker run -d --name jenkins_server -p 8080:8080 jenkins/jenkins`\n    (Remember to enable the `8080` port in your EC2 instance's security group for external access.)\n</code></pre> <p>Note</p> <p>List Running Containers:     <pre><code>docker ps\n</code></pre>     Purpose: Shows only the currently running containers.</p> <p>Note</p> <p>List All Containers (Running and Stopped):     <pre><code>docker ps -a\n</code></pre>     Purpose: Shows all containers, including those that have exited (stopped).</p> <p>Note</p> <p>Start a Stopped Container:     <pre><code>docker start &lt;container_name_or_id&gt;\n</code></pre>     Purpose: Starts one or more stopped containers.</p> <p>Note</p> <p>Stop a Running Container:     <pre><code>docker stop &lt;container_name_or_id&gt;\n</code></pre>     Purpose: Stops one or more running containers.</p> <p>Note</p> <p>Remove a Container:     <pre><code>docker rm &lt;container_name_or_id&gt;\n</code></pre>     Purpose: Removes one or more stopped containers.     Important: You cannot remove a running container directly. You must stop it first.</p> <p>Note</p> <p>Attach to a Running Container:     <pre><code>docker attach &lt;container_name_or_id&gt;\n</code></pre>     Purpose: Connects to the main process of a running container. If the main process exits, the container will stop.</p> <p>Note</p> <p>Execute a Command in a Running Container:     <pre><code>docker exec -it &lt;container_name_or_id&gt; &lt;command&gt;\n</code></pre>     Purpose: Runs a new command in a running container, typically to get a shell. It creates a new process inside the container without stopping the main process.     Example: <code>docker exec -it my_ubuntu_container /bin/bash</code> (to get a bash shell inside the container)</p> <p>Note</p> <p>Create an Image from a Container:     <pre><code>docker commit &lt;container_name_or_id&gt; &lt;new_image_name&gt;\n</code></pre>     Purpose: Creates a new image from the current state of a running or stopped container. This is useful after making changes inside a container that you want to persist in a new image.     Example: <code>docker commit my_container updated_image</code></p> <p>Note</p> <p>Check Port Mappings of a Container:     <pre><code>docker port &lt;container_name_or_id&gt;\n</code></pre>     Purpose: Lists the port bindings for a container, showing which container ports are mapped to which host ports.</p>"},{"location":"devops/git/","title":"Git","text":"<p>Git is a Version Control System (VCS), specifically a distributed Version Control System, that was created by Linus Torvalds in 2005. It serves as a tool to manage and track changes in code over time, allowing developers to create \"snapshots\" or versions of their project. This enables capabilities like rewinding to previous states, fast-forwarding, and critically, facilitates teams working collaboratively on the same codebase without overwriting each other's work. Today, Git has become the de facto standard for developers, with 93% of them reportedly using it. It's important to distinguish Git itself from hosting services like GitHub, GitLab, or Bitbucket; these platforms utilize Git but are not Git inherently.</p> <p></p> <p>To effectively manage code, Git operates across three primary zones or stages on your local machine: the working directory, the staging area, and the local repository.</p> <ol> <li>The working directory is where you actively write code, fix bugs, and make changes to your files. When you're using an Integrated Development Environment (IDE) like VS Code, you're working within this directory, which is simply your local project folder. At this stage, Git is passively tracking what's happening but has not yet \"remembered\" any of your changes.</li> <li>The staging area, also referred to as the index or a \"waiting room\" or \"shopping cart\" for changes, is the intermediate step. When you execute the <code>git add</code> command on a file, you are taking changes from your working directory and placing them into this area, explicitly telling Git that you intend to include these specific changes in the next snapshot. This allows you to handpick exactly what changes go into each commit, giving you fine-grained control and helping to keep your commit history clean. It also functions as a safety net, enabling you to review what you're about to commit using <code>git status</code> or <code>git diff</code> before making it permanent.</li> <li>The local repository is where Git permanently saves the snapshots of your project's history on your computer. This is housed within a hidden <code>.git</code> folder in your project directory. When you run <code>git commit</code> with a message, Git takes all the changes currently in the staging area and records them as a commit in your local repository. Each commit acts like a checkpoint in time, allowing you to always revert to it. Git stores an entire snapshot of files at a per-commit level, but it does so efficiently by only storing new or changed file content and pointing to existing unchanged file content, which contributes to its efficiency. Git objects like Blobs (for files) and Trees (for directories) are used internally to manage this data. The <code>git log</code> command helps you view this commit history.</li> </ol> <p>The full local Git workflow typically involves you editing files in your working directory, then staging them with <code>git add</code>, and finally committing them with <code>git commit</code> to your local repository. If you need to temporarily set aside changes without committing, <code>git stash</code> can move them out of the working directory onto a stack.</p> <p>Beyond your local machine, there's the remote repository, which can be thought of as the cloud version of your codebase, serving as a central hub for team collaboration. Services like GitHub, GitLab, or Bitbucket host these remote repositories. After making commits in your local repository, you use <code>git push</code> to upload those local commits to a designated remote branch, making them visible to your team. Conversely, to incorporate updates from your teammates, you use <code>git pull</code>, which effectively fetches new changes from the remote and merges them into your local copy. The <code>git fetch</code> command specifically downloads commits and references without merging, updating remote-tracking branches (e.g., <code>origin/main</code>) in your local repository. The <code>origin</code> remote is conventionally set up automatically when you clone a repository, pointing back to the source you cloned from.</p>"},{"location":"devops/git/#branching-and-merging-strategies","title":"Branching and Merging Strategies","text":"<p>A branch in Git is simply a pointer to a commit, making branches lightweight and inexpensive to create. This feature is fundamental to Git workflows, allowing developers to work on new features or bug fixes in isolation. The <code>git switch</code> command (or the older <code>git checkout</code>) allows you to move between different branches, effectively updating your working directory to reflect the state of that branch.</p> <p>When independent changes are made on different branches that originated from a common point, those branches are said to be diverging. To bring these changes together, Git provides two primary mechanisms: merging and rebasing.</p> <p>Merging combines the history from one branch into another. Git first identifies the merge base, which is the nearest common ancestor commit of the two branches. It then integrates the changes from both branches, creating a new commit called a merge commit. A merge commit is unique because it has two parents, representing the tips of the two merged branches. While merging preserves the true history of the project, showing where branches diverged and merged, it can lead to many merge commits, which might make the history harder to read. A fast-forward merge occurs when there is no diverging history on the target branch; Git simply moves the pointer of the target branch forward to the tip of the merged branch without creating a new merge commit.</p> <p>Rebasing, in contrast to merging, rewrites history. When you rebase one branch onto another (e.g., <code>feature</code> onto <code>main</code>), Git takes the commits from the <code>feature</code> branch, moves them back to their common ancestor, and then \"replays\" them one by one on top of the latest commit of the <code>main</code> branch. This creates new commit objects for the replayed changes, resulting in a linear history that can be easier to read and manage, especially for preparing pull requests. However, you should never rebase a public branch like <code>main</code>, as this rewrites history that others may have already pulled, leading to significant synchronization issues and conflicts for collaborators. Rebasing your own private branches onto a public one is generally acceptable.</p>"},{"location":"devops/git/#handling-conflicts","title":"Handling Conflicts","text":"<p>Conflicts arise when Git attempts to combine changes (during a merge or rebase) where the same lines of code have been modified differently in the diverging branches, and Git cannot automatically decide which change to keep.</p> <p>Git will insert conflict markers (<code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code>, <code>=======</code>, <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code>) into the affected files. To resolve these, you must manually edit the file to remove the markers and choose the desired code. After resolving, you must <code>git add</code> the file to stage the resolution, and then either <code>git commit</code> (for a merge) or <code>git rebase --continue</code> (for a rebase) to finalize the process.</p> <p>The <code>git checkout --ours</code> and <code>git checkout --theirs</code> commands can assist in conflict resolution by picking changes from either the current branch (<code>ours</code> in a merge) or the branch being merged/rebased (<code>theirs</code> in a merge, but <code>ours</code> during rebase refers to the target branch and <code>theirs</code> to the branch being replayed). For recurring conflicts, the <code>git rerere</code> (reuse recorded resolution) feature, if enabled, can remember how you resolved a specific conflict hunk and automatically resolve it the next time it appears.</p>"},{"location":"devops/git/#undoing-changes","title":"Undoing Changes","text":"<p>Git offers several powerful commands for undoing changes:</p> <ul> <li><code>git reset</code>: This command is used to undo recent commits or changes in the index (staging area) or working tree.<ul> <li><code>git reset --soft &lt;commit&gt;</code>: Moves the branch pointer back to the specified commit but keeps all changes from the undone commits as staged changes in your staging area, leaving your working directory untouched. This is useful for redoing a commit or fixing its message before pushing.</li> <li><code>git reset --hard &lt;commit&gt;</code>: Moves the branch pointer back to the specified commit and discards all changes in both the staging area and the working directory that were made after that commit. All uncommitted or staged changes are permanently lost from your working files, though the commit objects themselves may still be recoverable using <code>git reflog</code>. Untracked files are not removed by <code>git reset --hard</code>.</li> </ul> </li> <li><code>git revert &lt;commit&gt;</code>: Instead of erasing history, <code>git revert</code> undoes the changes introduced by a specific commit by creating a new commit that is the inverse of the commit being reverted. This preserves the project's history, showing both the original commit and the subsequent commit that undid its effects.</li> <li><code>git reflog</code>: This is an extremely useful command that keeps a record of where your <code>HEAD</code> (your current position in history) has been. It allows you to recover commits or states that might seem lost after actions like hard resets, rebases, or accidental branch deletions, because the underlying commit objects are still referenced in the reflog.</li> <li><code>git commit --amend</code>: This command allows you to change the message of your last commit. You can also stage additional changes before running <code>git commit --amend</code> to incorporate them into the previous commit. Be cautious: this command rewrites history by changing the SHA hash of the last commit, so do not amend commits that have already been pushed to a public branch.</li> </ul>"},{"location":"devops/git/#remote-collaboration-features-hosting-services","title":"Remote Collaboration Features (Hosting Services)","text":"<p>While Git is the underlying VCS, many collaborative features are provided by hosting services like GitHub:</p> <ul> <li>Forking: This is a feature offered by hosting services, not a core Git command. A fork creates a personal copy of an original repository under your own account, allowing you to modify the project without directly affecting the original. This is the standard method for contributing to open-source projects. When working with forks, it's common to add a second remote, typically named <code>upstream</code>, pointing to the original repository to easily pull in its latest changes.</li> <li>Pull Request (PR): Also a feature of hosting services, a pull request is a mechanism to propose changes from one branch (often a feature branch on your fork) to another (e.g., <code>main</code> on the original repository). PRs facilitate discussion and code review before changes are merged into the main codebase.</li> </ul>"},{"location":"devops/git/#ignoring-files","title":"Ignoring Files","text":"<p>The <code>.gitignore</code> file is crucial for telling Git which files or patterns of files it should ignore and not track. This is commonly used for generated files (like build outputs), log files, or dependency directories (<code>node_modules/</code>). You can place <code>.gitignore</code> files at the root of your repository or in subdirectories, and their rules apply to that directory and its subdirectories. The file supports standard shell wildcards, anchored patterns, and negation rules.</p>"},{"location":"devops/git/#advanced-git-commands","title":"Advanced Git Commands","text":"<ul> <li><code>git stash</code>: Temporarily saves changes in your working directory and staging area without committing them, allowing you to switch to a clean working state. <code>git stash list</code> shows stashes, <code>git stash pop</code> applies and removes the most recent, and <code>git stash drop</code> removes a stash.</li> <li><code>git squash</code>: Not a standalone command, but an action typically performed using <code>git rebase -i</code> (interactive rebase). It allows you to combine multiple commits into a single, cleaner commit, often used before merging a feature branch to streamline history.</li> <li><code>git cherry-pick</code>: Applies the changes introduced by a specific commit from one branch onto another branch by creating a new commit on the target branch with those changes. It requires a clean working tree.</li> <li><code>git bisect</code>: A debugging tool that uses a binary search algorithm to efficiently find the specific commit that introduced a bug. You mark commits as \"good\" (bug not present) or \"bad\" (bug present), and Git iteratively checks out commits in the middle of the range until the culprit is found.</li> <li><code>git worktree</code>: Allows you to have multiple working directories linked to the same repository, with each worktree potentially having a different branch checked out simultaneously. You cannot work on a branch that is already checked out by another worktree.</li> <li><code>git tag</code>: Used to mark specific points in history as important, such as release versions (e.g., v1.0.0). A tag is an immutable pointer to a commit. When you <code>git checkout</code> a tag, you enter a \"detached HEAD\" state, meaning you cannot commit directly to it.</li> </ul>"},{"location":"devops/git/#popular-branching-strategies","title":"Popular Branching Strategies","text":"<p>Understanding different branching strategies is key for team collaboration:</p> <ul> <li>Feature Branching: This is a straightforward approach where for each new feature or bug fix, a dedicated branch is created off the <code>main</code> branch. All work for that feature happens on this isolated branch, and once complete and tested, it's merged back into <code>main</code>, often via a pull request. The core idea is that the <code>main</code> branch always contains production-ready code, keeping incomplete work separated. This is extremely common and forms the basis for many workflows, including those on GitHub. While it scales well and allows multiple developers to work without interfering, it requires eventual integration, which can lead to merge conflicts if <code>main</code> has diverged significantly. Regularly updating feature branches from <code>main</code> can mitigate this.</li> <li>Gitflow: A more structured and complex model suited for projects with regular release cycles and multiple versions to maintain. Gitflow introduces specific long-lived branches for different purposes: a <code>master</code> or <code>main</code> branch for production code (where each commit is a release), and a <code>develop</code> branch where integration of the latest development changes occurs. Feature branches are created from <code>develop</code> and merged back into it. Release branches are used to prepare new versions, branching off <code>develop</code> for final polish and bug fixes, and then merged into both <code>master</code> (for release) and <code>develop</code> (to incorporate fixes into ongoing development). Hotfix branches are for quickly addressing critical issues found in production, branching off <code>master</code> and merging back into both <code>master</code> and <code>develop</code>. While robust for large, structured projects, its complexity can be overkill for smaller teams or continuous development environments.</li> <li>GitHub Flow: A lightweight and simpler workflow popularized by GitHub, commonly seen in open-source projects. It centers around one main branch (often called <code>main</code>) that is always deployable. For any new work, a short-lived feature branch is created directly off <code>main</code>, work is done on it, and then a pull request (PR) is opened to merge it back into <code>main</code>. Code review and automated tests happen during the PR process. There are no dedicated <code>develop</code> or <code>release</code> branches, simplifying the process. If a critical bug arises, a quick fix branch is made from <code>main</code>. This flow is ideal for smaller teams and emphasizes simplicity and speed.</li> <li>GitLab Flow: This is a hybrid model that combines the simplicity of GitHub Flow with the idea of environment-specific or release branches. Like GitHub Flow, feature branches are often developed off <code>main</code> and merged back into <code>main</code> when ready, with the code in <code>main</code> considered deployable. However, GitLab Flow suggests using environment branches (e.g., <code>staging</code>, <code>production</code>) or tags to manage deployments to different environments. After merging features into <code>main</code>, code might be deployed to staging by merging <code>main</code> into <code>staging</code>, and then to production by merging <code>main</code> into <code>production</code> or deploying a tagged release. This provides more structure for teams with multiple deployment targets, clearly defining which commit is on which environment. While more complex than GitHub Flow, it's less heavy than Gitflow and is often adopted by teams using GitLab CI/CD pipelines.</li> <li>Trunk-Based Development (TBD): This strategy prioritizes simplicity and continuous integration, relying on a single, long-lived branch, usually called <code>trunk</code> or <code>main</code>. Developers commit to this main branch directly or via very short-lived branches, ideally merging back daily or multiple times a day. The \"trunk\" signifies it as the single source of truth where all work quickly lands. This approach minimizes large merge conflicts, as changes are incremental and integration delays are reduced. New features are often hidden behind feature flags, meaning incomplete code can be merged into <code>main</code> but disabled in production, ensuring the codebase is always deployable. TBD is the foundation for modern DevOps, enabling rapid integration and deployment (e.g., 100 deployments a day) and is enforced by large tech companies like Google, Meta, and Amazon to keep vast codebases clean and moving. This strategy is preferred by large tech companies aiming for rapid integration and deployment.</li> </ul>"},{"location":"devops/kube2/","title":"Kube2","text":"<p>Kubernetes provides powerful mechanisms for organizing and managing containerized applications. At the core of this organization are Labels, which serve as a foundational concept, enabling the grouping and identification of objects within a cluster. Building upon labels, Selectors allow users to filter and retrieve these labeled objects efficiently. These two concepts are then leveraged by higher-level controllers like ReplicationController and ReplicaSet to manage the desired state and scaling of application Pods.</p>"},{"location":"devops/kube2/#labels-organizing-kubernetes-objects","title":"Labels: Organizing Kubernetes Objects","text":"<p>Labels are fundamental to organizing Kubernetes objects. They are essentially key-value pairs that can be attached to any Kubernetes object, such as Pods, Nodes, or other resources, to provide identifying attributes. The video likens labels to stickers on kitchen containers, where a label like \"salt\" on a box helps you identify its contents without opening it. Similarly, in a Kubernetes environment, labels help identify and categorize objects, especially when dealing with hundreds of Pods.</p> <p>The primary purpose of labels is to help in organizing and managing objects. For instance, if you have multiple Pods running a specific application (e.g., \"Technical Guftgu application\"), you can attach a label like <code>app: technical-guftgu</code> to those Pods. This allows you to quickly sort and search for all Pods associated with that application. Labels are flexible and user-defined, meaning there's no predefined set of labels; you can create any key-value pair that is meaningful to your specific needs. For example, <code>class: pods</code> or <code>environment: development</code> could be custom labels. An object can have multiple labels attached to it, similar to how a person might have multiple labels like \"Bhupinder Rajput,\" \"Teacher,\" or \"Technical Guftgu\".</p> <p>Here\u2019s an example of how labels are defined within a Pod's YAML manifest, specifically under the <code>metadata.labels</code> section:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-daily-pod\n  labels:\n    environment: development\n    class: pods\n    my-name: bhupinder # This label was added later using an imperative command\nspec:\n  containers:\n  - name: c0\n    image: ubuntu\n    command: [\"/bin/bash\", \"-c\", \"echo Hello Bhupinder &amp;&amp; sleep 3600\"]\n</code></pre> <p>The video demonstrates adding initial labels (<code>environment: development</code> and <code>class: pods</code>) via a YAML file, which is a declarative method. It also shows how to add an additional label (<code>my-name: bhupinder</code>) to an already existing Pod using an imperative command, demonstrating the flexibility of managing labels directly:</p> <pre><code>kubectl label pods my-daily-pod my-name=bhupinder\n</code></pre> <p>To view the labels attached to Pods, you can use the command:</p> <pre><code>kubectl get pods --show-labels\n</code></pre>"},{"location":"devops/kube2/#selectors-filtering-objects-by-labels","title":"Selectors: Filtering Objects by Labels","text":"<p>Selectors are the mechanism used to filter and retrieve Kubernetes objects based on their attached labels. While labels help in defining and organizing, selectors help in finding those objects. The Kubernetes API currently supports two main types of label selectors: Equality-Based Selectors and Set-Based Selectors.</p> <ol> <li> <p>Equality-Based Selectors: These selectors match objects based on whether a label's value is strictly equal to or not equal to a specified value.</p> <ul> <li>Equal (<code>=</code> or <code>==</code>): Selects objects where the label's value matches exactly.</li> <li>Not Equal (<code>!=</code>): Selects objects where the label's value does not match exactly.</li> </ul> <p>For example, to find all Pods with the label <code>environment</code> set to <code>development</code>, you would use:</p> <pre><code>kubectl get pods -l environment=development\n</code></pre> <p>To find Pods where the <code>environment</code> label is not <code>development</code>, you would use:</p> <pre><code>kubectl get pods -l environment!=development\n</code></pre> </li> <li> <p>Set-Based Selectors: These selectors allow for more complex matching conditions based on sets of values or the existence/absence of a label.</p> <ul> <li><code>in</code>: Selects objects where the label's value is within a specified set of values.</li> <li><code>notin</code>: Selects objects where the label's value is not within a specified set of values.</li> <li><code>exists</code> (just the key): Selects objects that have a label with a specific key, regardless of its value. The video doesn't provide a direct command for <code>exists</code> but mentions it as a type of set-based selector.</li> </ul> <p>Example using <code>in</code> for Pods where <code>environment</code> is either <code>development</code> or <code>testing</code>:</p> <pre><code>kubectl get pods -l 'environment in (development,testing)'\n</code></pre> <p>Example using <code>notin</code> for Pods where <code>environment</code> is not <code>development</code> and not <code>testing</code>:</p> <pre><code>kubectl get pods -l 'environment notin (development,testing)'\n</code></pre> <p>Selectors can also be combined using commas to match multiple labels. For instance, to find Pods that have both <code>class: pods</code> AND <code>my-name: bhupinder</code> labels:</p> <pre><code>kubectl get pods -l 'class=pods,my-name=bhupinder'\n</code></pre> <p>Selectors are also crucial for deleting objects based on labels. For example, to delete Pods where the <code>environment</code> label is <code>development</code>:</p> <pre><code>kubectl delete pods -l environment=development\n</code></pre> <p>Another application of selectors is in Node selection. A Pod can be explicitly scheduled onto a specific Node by first applying a label to the Node, and then configuring the Pod's manifest with a <code>nodeSelector</code> that targets that Node's label. For example, if a Node is labeled <code>hardware: medium</code>, a Pod can be configured to run only on that Node. This is particularly useful in heterogeneous clusters where certain Pods require specific hardware capabilities.</p> <p>First, label the desired Node (e.g., <code>minikube</code>):</p> <pre><code>kubectl label nodes minikube hardware=medium\n</code></pre> <p>Then, define the <code>nodeSelector</code> in the Pod's YAML:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: node-labels\n  labels:\n    environment: development\nspec:\n  nodeSelector:\n    hardware: medium # This ensures the pod runs on a node with this label\n  containers:\n  - name: c0\n    image: ubuntu\n    command: [\"/bin/bash\", \"-c\", \"echo Hello Bhupinder &amp;&amp; sleep 3600\"]\n</code></pre> </li> </ol>"},{"location":"devops/kube2/#replicationcontroller-ensuring-pod-availability-and-scaling","title":"ReplicationController: Ensuring Pod Availability and Scaling","text":"<p>While individual Pods are the smallest deployable units in Kubernetes, they are not self-healing by default. If a Pod crashes or is deleted, it will not automatically restart or be re-created by the cluster. This is where ReplicationController (RC) comes into play. The RC is a Kubernetes object designed to ensure that a specified number of identical Pod replicas are always running at any given time. It acts as a controller that continuously monitors the desired state (number of replicas) and the current state of Pods matching its selector.</p> <p>The key benefits of using a ReplicationController include: *   Reliability/High Availability: If a Pod fails, crashes, or is terminated, the RC automatically detects the discrepancy between the desired and current state and creates a new Pod to maintain the specified replica count. This ensures continuous service availability for users. The video clarifies that a new Pod is created, not the old one restarted, and it will have a new IP address. *   Load Balancing: By ensuring multiple replicas, the RC inherently supports distributing incoming traffic across these identical Pods, preventing a single instance from being overloaded. *   Scaling: The RC allows for both scaling up (increasing the number of Pods to handle more load) and scaling down (decreasing the number of Pods when demand is low). This is crucial for real-time applications like streaming services (e.g., Netflix, Disney+ Hotstar) that experience fluctuating user loads.</p> <p>A ReplicationController manifest defines the desired number of replicas, a selector to identify the Pods it manages, and a Pod template that describes the Pods to be created.</p> <p>Here\u2019s a sample ReplicationController manifest:</p> <pre><code>apiVersion: v1\nkind: ReplicationController # Specifies the object type\nmetadata:\n  name: my-replica # Name of the ReplicationController\nspec:\n  replicas: 5 # Desired number of Pod replicas\n  selector:\n    my-name: bhupinder # RC will manage Pods with this label\n  template: # Defines the Pods to be created\n    metadata:\n      name: test-pod # Name for the Pods created by this RC\n      labels:\n        my-name: bhupinder # Pods must have this label to be managed by the selector\n    spec:\n      containers:\n      - name: c0\n        image: ubuntu\n        command: [\"/bin/bash\", \"-c\", \"echo Hello Bhupinder &amp;&amp; sleep 3600\"]\n</code></pre> <p>To apply this manifest and create the ReplicationController:</p> <pre><code>kubectl apply -f my-rc.yaml\n</code></pre> <p>To check the status of the ReplicationController and the Pods it manages:</p> <pre><code>kubectl get rc\nkubectl get pods\n</code></pre> <p>The video demonstrates the auto-healing capability by manually deleting a Pod created by the RC. Upon deletion, the RC immediately creates a new Pod to maintain the desired count of five replicas.</p> <p>To scale the number of replicas, you can use the <code>kubectl scale</code> command. For instance, to change the number of Pods from 5 to 8, or from 8 to 1:</p> <pre><code># Scale up from 5 to 8 replicas\nkubectl scale --replicas=8 rc my-replica\n\n# Scale down from 8 to 1 replica\nkubectl scale --replicas=1 rc my-replica\n</code></pre>"},{"location":"devops/kube2/#replicaset-the-advanced-replication-controller","title":"ReplicaSet: The Advanced Replication Controller","text":"<p>ReplicaSet (RS) is considered an advanced version of the ReplicationController. While both serve the same core purpose of maintaining a stable set of identical Pod replicas, the key distinction lies in their selector capabilities.</p> <p>The primary difference is that ReplicationController only supports equality-based selectors, meaning it can only match Pods whose labels are exactly equal to or not equal to a specified value. In contrast, ReplicaSet supports both equality-based selectors and set-based selectors. This expanded selector capability allows ReplicaSet to manage Pods with more complex label matching requirements, using operators like <code>in</code>, <code>notin</code>, or simply checking for the existence of a label.</p> <p>Another notable difference is their API version: ReplicationController typically uses <code>v1</code>, whereas ReplicaSet is available in <code>apps/v1</code>.</p> <p>Here\u2019s a sample ReplicaSet manifest:</p> <pre><code>apiVersion: apps/v1 # API version for ReplicaSet\nkind: ReplicaSet # Specifies the object type\nmetadata:\n  name: my-rs # Name of the ReplicaSet\nspec:\n  replicas: 2 # Desired number of Pod replicas\n  selector:\n    matchLabels: # Equality-based selector\n      my-name: bhupinder\n    matchExpressions: # Set-based selector (can be used in addition or instead)\n      - {key: env, operator: In, values: [dev, prod]} # Example of set-based selector\n  template: # Defines the Pods to be created\n    metadata:\n      name: test-seven # Name for the Pods created by this RS\n      labels:\n        my-name: bhupinder # Pods must have this label\n        env: dev # Example label for set-based selector\n    spec:\n      containers:\n      - name: c0\n        image: ubuntu\n        command: [\"/bin/bash\", \"-c\", \"echo Technical Guftgu &amp;&amp; sleep 3600\"]\n</code></pre> <p>To apply this manifest and create the ReplicaSet:</p> <pre><code>kubectl apply -f my-rs.yaml\n</code></pre> <p>To check the status of the ReplicaSet and the Pods it manages:</p> <pre><code>kubectl get rs\nkubectl get pods\n</code></pre> <p>Similar to ReplicationController, ReplicaSet also provides auto-healing and scaling capabilities. If a Pod managed by the ReplicaSet is deleted, a new one will be created instantly to maintain the desired replica count. Scaling up or down is also achieved using the <code>kubectl scale</code> command, similar to RC:</p> <pre><code># Scale down from 2 to 1 replica\nkubectl scale --replicas=1 rs my-rs\n\n# Scale up from 1 to 3 replicas (example not in source, but derived from scale command)\nkubectl scale --replicas=3 rs my-rs\n</code></pre>"},{"location":"devops/kube2/#relationships-between-concepts","title":"Relationships Between Concepts","text":"<p>The concepts of Labels, Selectors, ReplicationController, and ReplicaSet are deeply interconnected in Kubernetes:</p> <ol> <li>Labels serve as the fundamental identifying metadata attached to Kubernetes objects. They provide context and categorisation for various resources within the cluster.</li> <li>Selectors depend entirely on labels. They are the querying mechanism used to find and group objects that possess specific labels. Without labels, selectors would have no information to filter upon.</li> <li>ReplicationController and ReplicaSet are higher-level Kubernetes objects that utilize both labels and selectors to manage the lifecycle and scaling of Pods. Both controllers maintain a desired number of Pods by continuously checking the Pods that match their defined <code>selector</code>. The <code>template</code> within these controllers specifies the characteristics of the Pods they are responsible for creating, including the labels that must match the controller's <code>selector</code>. The ReplicaSet offers more flexibility in its selector due to supporting set-based matching.</li> <li>In essence, labels organize, selectors find, and ReplicationControllers/ReplicaSets use these two mechanisms to reliably and scalably manage Pods.</li> </ol> <p>```</p>"},{"location":"devops/kube3/","title":"Kube3","text":""},{"location":"devops/kube3/#kubernetes-jobs","title":"Kubernetes Jobs","text":"<p>Kubernetes Jobs are a distinct type of object in Kubernetes, designed for a specific purpose that differs from standard Pods like those managed by Deployments or ReplicaSets. While standard Pods are engineered to ensure an application or service remains continuously running\u2014meaning if a Pod fails or is deleted, a new one is automatically created to replace it\u2014Jobs serve a different function. A Job is meant to perform a task once and then terminate. Once the task is complete, the Job's Pod does not get recreated automatically.</p> <p>The fundamental difference lies in their recreation policy. Pods (via ReplicaSets, Deployments, or StatefulSets) ensure high availability by recreating Pods if they fail, thereby maintaining service uptime. In contrast, a Job executes a task to completion, and then the associated Pod automatically terminates. It will not recreate itself after finishing its work.</p> <p>Jobs are highly useful for various one-time or scheduled tasks. Examples include: *   Log rotation: Deleting old logs and creating new ones at specific intervals, for instance, every two hours. *   Database backups: Creating a Pod to take a database backup, which then terminates after the backup is complete. *   Helm chart installations: Using Jobs to perform specific installation tasks within a Helm chart. *   Background processes: Running a specific, time-bound task in the background that needs to finish and then stop, without continuous recreation. *   Scheduled tasks: Performing a specific job after a certain time period, like an hour or thirty minutes, and then ensuring it stops automatically.</p> <p>When you define a Job in a YAML manifest, the <code>apiVersion</code> will be <code>batch/v1</code>, as Jobs operate within the batch category. The <code>kind</code> field will be <code>Job</code>. Inside the <code>spec.template.spec</code> section, you define the container(s) that will perform the task, similar to a regular Pod. A crucial aspect of a Job's Pod definition is the <code>restartPolicy</code>, which should be set to <code>Never</code>. This policy ensures that once the container completes its task, it terminates and is not restarted.</p> <p>Here is an example of a simple Job YAML:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\nspec:\n  template:\n    spec:\n      containers:\n      - name: my-container\n        image: busybox\n        command: [\"sh\", \"-c\", \"echo 'Technical Guru'; sleep 5\"] # This command prints and then waits 5 seconds\n      restartPolicy: Never # Crucial for Jobs\n</code></pre> <p>In this example, the container will print \"Technical Guru\" and then sleep for 5 seconds. After 5 seconds, the container will stop, and the Job will mark it as complete.</p> <p>An important distinction from other Kubernetes objects is that the Job object itself is not deleted automatically once its task is complete. While the Pod created by the Job will terminate, the Job resource will persist. You must manually delete the Job using <code>kubectl delete -f &lt;your-job-manifest.yaml&gt;</code>.</p>"},{"location":"devops/kube3/#parallelism-in-kubernetes-jobs","title":"Parallelism in Kubernetes Jobs","text":"<p>Kubernetes Jobs also support parallelism, allowing you to run multiple Pods concurrently for a specific task. This is useful when you have a job that can be broken down into several independent sub-tasks that can be processed simultaneously.</p> <p>To configure parallelism, you add the <code>parallelism</code> field within the <code>spec</code> section of your Job manifest. For instance, setting <code>parallelism: 5</code> would instruct Kubernetes to create and run five Pods concurrently for the Job.</p> <p>Additionally, you can define an <code>activeDeadlineSeconds</code> field, which specifies the maximum duration in seconds that a Job can be active. If the Job exceeds this time limit, all its running Pods will be terminated, and the Job will be marked as failed. For example, <code>activeDeadlineSeconds: 40</code> would mean the Job will run for a maximum of 40 seconds before its Pods are terminated and deleted, even if the task isn't fully completed.</p> <p>Here is an example of a Job YAML demonstrating parallelism and an active deadline:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: parallel-job\nspec:\n  parallelism: 5 # Creates 5 pods to run in parallel\n  activeDeadlineSeconds: 40 # Max 40 seconds for the job to be active\n  template:\n    spec:\n      containers:\n      - name: my-container\n        image: busybox\n        command: [\"sh\", \"-c\", \"echo 'Working in parallel!'; sleep 20\"] # Work for 20 seconds\n      restartPolicy: Never\n</code></pre> <p>In this scenario, five Pods will be created, each executing the specified command. The <code>activeDeadlineSeconds</code> ensures that after 40 seconds, all these Pods will be terminated, regardless of their completion status.</p>"},{"location":"devops/kube3/#cronjobs","title":"CronJobs","text":"<p>CronJobs extend the functionality of Jobs by allowing them to be scheduled to run periodically. They are similar to the traditional <code>cron</code> utility found in Unix-like operating systems. CronJobs are ideal for tasks that need to run at regular intervals, such as daily reports, hourly data cleanups, or backups.</p> <p>For a CronJob, the <code>kind</code> field in the YAML manifest is <code>CronJob</code>. The crucial field for scheduling is <code>schedule</code>, which takes a cron format string (e.g., <code>* * * * *</code> for every minute, <code>0 0 * * *</code> for once a day at midnight). The actual Job definition is nested under <code>jobTemplate.spec</code>, which has the same structure as a standard Job's <code>spec</code>.</p> <p>Here is an example of a CronJob YAML:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cronjob\nspec:\n  schedule: \"* * * * *\" # Runs every minute\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: cron-container\n            image: busybox\n            command: [\"sh\", \"-c\", \"echo 'Technical Guru printing every minute for 5 seconds'; sleep 5\"]\n          restartPolicy: OnFailure # Or Never, depending on task\n</code></pre> <p>This CronJob will create a new Pod every minute. Each Pod will run for 5 seconds, printing \"Technical Guru printing every minute for 5 seconds,\" and then terminate. After its completion, the Pod is marked as <code>Completed</code>.</p>"},{"location":"devops/kube3/#init-containers","title":"Init Containers","text":"<p>Init Containers (short for \"Initialization Containers\") are specialized containers that run before the main application containers in a Pod. Their primary purpose is to perform setup or initialization tasks that are required for the main application to function correctly.</p> <p>A key characteristic of Init Containers is their sequential execution and dependency: *   Order: Init Containers run one by one, in the order they are defined. *   Completion: Each Init Container must complete successfully before the next one starts. *   Success Requirement: If an Init Container fails, Kubernetes will repeatedly restart the Pod (including all Init Containers) until that specific Init Container succeeds. The main application container will not start until all Init Containers have successfully completed.</p> <p>Init Containers are valuable for managing dependencies or preparing the environment for the main application. Some common use cases include: *   Database Seeding: Populating a database with initial data or creating necessary tables before the application starts consuming it. *   Waiting for Dependencies: Delaying the main application's start until external services (like a database or another microservice) are fully ready and reachable. *   Configuration Provisioning: Fetching configuration files or secrets from an external source and writing them to a shared volume that the main application container can then read. *   Pre-installation/Setup: Installing required packages, binaries, or performing complex setup scripts that are prerequisites for the main application. *   Volume Population: Downloading or preparing data into a shared volume, which the main container then uses.</p> <p>Init Containers share volumes with the main application container, allowing them to pass data or prepared files. For example, an Init Container can download a file to a volume, and the main container can then access that file from the same mounted volume.</p> <p>Here is an example of a Pod YAML with an Init Container:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: init-demo\nspec:\n  initContainers: # Init Containers defined here\n  - name: init-myservice\n    image: busybox\n    command: [\"sh\", \"-c\", \"echo 'Like and Subscribe Technical Guftgu' &gt; /tmp/exchange/test-file; sleep 30\"] # Creates a file with content\n    volumeMounts:\n    - name: workdir\n      mountPath: /tmp/exchange\n  containers: # Main application container\n  - name: main-app\n    image: busybox\n    command: [\"sh\", \"-c\", \"while true; do cat /tmp/data/test-file; sleep 5; done\"] # Accesses the file\n    volumeMounts:\n    - name: workdir\n      mountPath: /tmp/data # Note: different mountPath for demonstration of access\n  volumes: # Shared volume\n  - name: workdir\n    emptyDir: {}\n</code></pre> <p>In this example, the <code>init-myservice</code> Init Container will first run. It creates a file named <code>test-file</code> in the <code>/tmp/exchange</code> directory (which is mounted from the <code>workdir</code> volume) and writes \"Like and Subscribe Technical Guftgu\" into it, then sleeps for 30 seconds. Only after this Init Container successfully completes will the <code>main-app</code> container start. The <code>main-app</code> container then continuously reads the content of <code>test-file</code> from its own mount path <code>/tmp/data</code> (which maps to the same shared <code>workdir</code> volume), demonstrating that it can access the data prepared by the Init Container.</p>"},{"location":"devops/kube3/#pod-lifecycle-and-conditions","title":"Pod Lifecycle and Conditions","text":"<p>Understanding the Pod lifecycle and its various phases and conditions is fundamental to troubleshooting and monitoring applications in Kubernetes. These represent the different states a Pod can be in from creation to termination.</p>"},{"location":"devops/kube3/#pod-phases","title":"Pod Phases","text":"<p>The Pod lifecycle can go through several distinct phases:</p> <ul> <li> <p>Pending: In this phase, the Pod has been accepted by the Kubernetes system, but one or more of its containers are not yet running. This could mean the Pod is waiting for a node to be assigned, or the container images are still being downloaded, or the required resources (like CPU or memory) are not yet available on any node.</p> </li> <li> <p>Running: The Pod has been bound to a node, and at least one container within it is actively running. This indicates that the Pod has been successfully scheduled and its application is operational. Even if multiple containers are defined in a Pod, as long as one is running, the Pod's phase will be 'Running'.</p> </li> <li> <p>Succeeded: This phase means that all containers in the Pod have successfully terminated, and they will not be restarted. This is the expected phase for Jobs after they complete their tasks.</p> </li> <li> <p>Failed: If all containers in the Pod have terminated, and at least one container terminated in failure (i.e., exited with a non-zero exit code), or was terminated by the system, the Pod enters the 'Failed' phase. This often indicates an issue with the application or its environment.</p> </li> <li> <p>Unknown: This phase indicates that the state of the Pod could not be obtained, typically due to a communication error between the Kubernetes Master (API Server) and the node where the Pod is supposed to be running. This can happen because of network issues or problems with the Kubelet on the node.</p> </li> </ul>"},{"location":"devops/kube3/#pod-conditions","title":"Pod Conditions","text":"<p>When you use the command <code>kubectl describe pod &lt;pod-name&gt;</code>, you can inspect the conditions of a Pod, which provide more detailed insights into its current state and any events that have occurred. These conditions are Boolean values (True, False, Unknown) that indicate whether a particular state is met. Some common Pod conditions include:</p> <ul> <li> <p>PodScheduled: This condition is <code>True</code> if the Pod has been successfully scheduled to a specific node. It indicates that the scheduler has found a suitable node for the Pod to run on.</p> </li> <li> <p>Initialized: This condition becomes <code>True</code> once all Init Containers in the Pod have successfully completed their tasks. As seen in the Init Container example, this would be <code>False</code> while the Init Container is still running or failing.</p> </li> <li> <p>ContainersReady: This condition is <code>True</code> when all containers within the Pod are ready. This means they are running and passing their readiness probes (if configured).</p> </li> <li> <p>Ready: This condition signifies that the Pod is ready to serve traffic. It is <code>True</code> only if both <code>PodScheduled</code>, <code>Initialized</code>, and <code>ContainersReady</code> are <code>True</code>, and all readiness probes (if defined for the main containers) are successful.</p> </li> <li> <p>Unschedulable: If the Pod cannot be scheduled to any available node, perhaps due to insufficient resources (like CPU or memory) or other scheduling constraints, this condition might become <code>True</code>.</p> </li> </ul> <p>Observing these phases and conditions through commands like <code>kubectl get pods</code> and <code>kubectl describe pod</code> is a crucial skill for monitoring and debugging your Kubernetes deployments.</p>"},{"location":"devops/kube4/","title":"Kube4","text":"<p>Kubernetes Deployments are a crucial object that provides declarative updates for Pods and ReplicaSets. They act as a top-level object or a manager (like a general manager or supervisor) that controls ReplicaSets, which in turn control Pods. This hierarchy offers significant advantages, especially when dealing with application updates and rollbacks.</p> <p>Historically, ReplicaSets and Replication Controllers were not able to handle application updates or rollbacks effectively in real-time environments. For instance, if an update to an application like WhatsApp or Facebook caused issues, there was no straightforward way to revert to a previous, stable version. Deployments were created to address this limitation.</p>"},{"location":"devops/kube4/#how-deployment-works-and-its-core-functionality","title":"How Deployment Works and its Core Functionality","text":"<p>When you use a Deployment, you define the desired state of your application. The Deployment object then monitors the cluster to ensure that this desired state is maintained.</p> <ol> <li>Controlling ReplicaSets: A Deployment controls ReplicaSets. When you apply changes to your application's code or configuration through a Deployment, it creates a new ReplicaSet for each new version. For example, if you have an application running as Version 1 (V1) under <code>ReplicaSet-V1</code>, and you update your code, the Deployment will create <code>ReplicaSet-V2</code> to manage the new version, while maintaining a record of V1. This process allows Deployment to maintain multiple versions of your application.</li> <li>Pod Management: ReplicaSets are responsible for creating and maintaining the desired number of Pods. If you specify two replicas, the ReplicaSet will ensure two Pods are running at all times. If a Pod fails or is deleted, the ReplicaSet will automatically create a new one to maintain the desired count, a mechanism known as self-healing.</li> <li>Updates and Rollbacks: Deployments provide fine-grained control over how new Pods are rolled out, updated, or rolled back to a previous state. When you update an application through a Deployment, it will gracefully move Pods from the old ReplicaSet to the new one. The old ReplicaSet is typically stopped but kept as a version for potential future rollbacks.</li> </ol> <p>An important concept to remember during rollbacks is that while the code or application version reverts to an older state, the number of Pods remains the same as in the current state. For example, if your current deployment is running on four Pods and you roll back to a version that previously ran on only one Pod, the application on all four current Pods will revert to the old code, but all four Pods will continue to run.</p>"},{"location":"devops/kube4/#key-features-and-use-cases-of-deployments","title":"Key Features and Use Cases of Deployments","text":"<p>Deployments offer several powerful features and use cases:</p> <ul> <li>Rolling Out New Versions: You can declaratively update your Pods and ReplicaSets to roll out a new version of your application.</li> <li>Declaring New States: You can define a new desired state for your Deployment by updating its Pod template specification. This will create a new ReplicaSet and manage the transition of Pods from the old ReplicaSet to the new one.</li> <li>Monitoring Rollout Status: Deployments allow you to check the status of a rollout to see if it succeeded or not.</li> <li>Rolling Back to Previous Revisions: If a new update causes issues, you can easily roll back to any previous version of your application that the Deployment has maintained.<ul> <li>To undo the last rollout, you can use the <code>kubectl rollout undo</code> command.</li> <li>To roll back to a specific revision, you can use <code>kubectl rollout undo deploy &lt;deployment-name&gt; --to-revision=&lt;number&gt;</code>.</li> </ul> </li> <li>Scaling: Deployments facilitate scaling up (increasing the number of Pods) and scaling down (decreasing the number of Pods) to manage workload. For example, you can scale from two Pods to four, or from four Pods down to two. When scaling down, the Pods that were most recently created are typically deleted first.</li> <li>Pausing and Resuming: Deployments can be paused to apply multiple updates to a Pod template specification or to make configuration changes without triggering rollouts. Once changes are complete, the Deployment can be resumed.</li> <li>Cleaning Up Old ReplicaSets: Deployments help in managing and cleaning up old, unneeded ReplicaSets.</li> </ul>"},{"location":"devops/kube4/#practical-examples-and-commands","title":"Practical Examples and Commands","text":"<p>To demonstrate Deployment functionality, let's look at some conceptual YAML structures and command-line instructions.</p>"},{"location":"devops/kube4/#1-deployment-yaml-structure","title":"1. Deployment YAML Structure","text":"<p>A Deployment is defined in a YAML file. The <code>kind</code> field specifies <code>Deployment</code>, and other fields define its behavior, such as the number of replicas, the container image, and commands to run.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment # Specifies the object type as Deployment\nmetadata:\n  name: my-deployment # Name of the Deployment object\nspec:\n  replicas: 2 # Desired number of Pod replicas\n  selector:\n    matchLabels:\n      app: my-deployment # Labels to select Pods managed by this Deployment\n  template:\n    metadata:\n      labels:\n        app: my-deployment # Labels applied to Pods created by this Deployment\n    spec:\n      containers:\n      - name: my-container # Name of the container\n        image: ubuntu # Container image to use, e.g., 'ubuntu' or 'centos'\n        command: [\"/bin/bash\", \"-c\", \"while true; do echo 'Technical Guftgu'; sleep 5; done\"] # Command to run inside the container\n</code></pre>"},{"location":"devops/kube4/#2-deploying-your-application","title":"2. Deploying Your Application","text":"<p>After creating the YAML file (e.g., <code>my-deploy.yaml</code>), you apply it to your Kubernetes cluster:</p> <ul> <li>Create/Edit YAML file: <code>vi my-deploy.yaml</code></li> <li>Apply the Deployment: <code>kubectl apply -f my-deploy.yaml</code><ul> <li>This command will create the Deployment, which in turn creates a ReplicaSet, and then the specified number of Pods.</li> </ul> </li> </ul>"},{"location":"devops/kube4/#3-checking-deployment-status-and-details","title":"3. Checking Deployment Status and Details","text":"<p>You can verify the status and details of your Deployment, ReplicaSets, and Pods using <code>kubectl</code> commands:</p> <ul> <li>Check Deployment status: <code>kubectl get deploy</code><ul> <li>This command shows the Deployment name, how many Pods are <code>READY</code> (e.g., <code>2/2</code> means 2 are ready out of 2 desired), how many are <code>UP-TO-DATE</code>, how many are <code>AVAILABLE</code>, and the <code>AGE</code> of the Deployment.</li> </ul> </li> <li>Describe a Deployment for detailed information: <code>kubectl describe deploy my-deployment</code><ul> <li>This provides extensive details about the Deployment, including its configuration, events, and related ReplicaSets and Pods.</li> </ul> </li> <li>Check ReplicaSet status: <code>kubectl get rs</code><ul> <li>This shows details for ReplicaSets, including <code>DESIRED</code>, <code>CURRENT</code>, and <code>READY</code> Pod counts.</li> </ul> </li> <li>Check Pod status: <code>kubectl get pods</code><ul> <li>This lists the Pods, their <code>STATUS</code>, <code>RESTARTS</code>, and <code>AGE</code>. Pod names created by a Deployment will typically start with the Deployment name followed by a random string.</li> </ul> </li> <li>View logs inside a container: <code>kubectl logs -f &lt;pod-name&gt;</code><ul> <li>This allows you to see the output of the command running inside a specific Pod's container.</li> </ul> </li> <li>Execute a command inside a container (e.g., to check OS release): <code>kubectl exec &lt;pod-name&gt; -- cat /etc/os-release</code></li> </ul>"},{"location":"devops/kube4/#4-scaling-deployments","title":"4. Scaling Deployments","text":"<p>You can easily scale your Deployment up or down:</p> <ul> <li>Scale up: <code>kubectl scale --replicas=4 deploy my-deployment</code> (changes the desired number of Pods to 4)</li> <li>Scale down: <code>kubectl scale --replicas=1 deploy my-deployment</code> (changes the desired number of Pods to 1)</li> </ul>"},{"location":"devops/kube4/#5-updating-and-rolling-back-deployments","title":"5. Updating and Rolling Back Deployments","text":"<p>To update your application, modify the <code>image</code> or <code>command</code> in your <code>my-deploy.yaml</code> file, then reapply it:</p> <ul> <li>Modify <code>my-deploy.yaml</code> (e.g., change <code>image: ubuntu</code> to <code>image: centos</code> or update the <code>command</code> output).</li> <li>Reapply the file: <code>kubectl apply -f my-deploy.yaml</code><ul> <li>This will create a new ReplicaSet for the updated version, and Pods will be transitioned.</li> </ul> </li> <li>Check rollout history: <code>kubectl rollout history deploy my-deployment</code><ul> <li>This command shows the revision history of your Deployment, indicating how many times you've made and applied changes.</li> </ul> </li> <li>Rollback to the previous version: <code>kubectl rollout undo deploy my-deployment</code><ul> <li>This command reverts the Deployment to the immediately preceding version.</li> </ul> </li> <li>Rollback to a specific version: <code>kubectl rollout undo deploy my-deployment --to-revision=2</code> (reverts to revision number 2).</li> </ul>"},{"location":"devops/kube4/#potential-deployment-failure-scenarios-interview-focus","title":"Potential Deployment Failure Scenarios (Interview Focus)","text":"<p>During an interview, you might be asked about reasons for Deployment failures. Here are some common scenarios:</p> <ul> <li>Insufficient Quota: Not enough resources (CPU, memory) are available in the cluster or on the nodes.</li> <li>Unhealthy Nodes: The Kubernetes nodes where Pods are scheduled are not in a healthy state or are down.</li> <li>Image Pull Errors: The specified container image cannot be pulled from the registry. This could be due to incorrect image name/tag, private registry authentication issues, or the image not existing.</li> <li>Insufficient Permissions: The service account or user deploying the application lacks the necessary permissions to create or manage Kubernetes resources.</li> <li>Limit Ranges: Resource limits defined (e.g., CPU or memory) are exceeded, preventing Pods from being scheduled or starting.</li> <li>Application Runtime Misconfiguration: Issues within the application code or its configuration prevent it from starting or running correctly within the container.</li> <li><code>kubectl</code> connection issues: Problems with the <code>kubectl</code> setup (e.g., issues accessing the cluster using <code>putty</code> due to incorrect key format) can prevent commands from executing.</li> </ul> <p>These are the main concepts and practical aspects of Kubernetes Deployments as discussed in the provided sources.</p>"},{"location":"devops/kube5/","title":"Kube5","text":"<p>Kubernetes networking is a fundamental and often advanced topic, critical for understanding how applications communicate within a cluster and with the outside world. It addresses several key challenges, including communication between containers within a Pod, communication between different Pods, exposing applications to external users, and enabling services to communicate internally within the cluster. All the practical examples discussed here can be performed using Minikube on an AWS EC2 instance.</p>"},{"location":"devops/kube5/#container-to-container-communication-within-a-pod","title":"Container-to-Container Communication Within a Pod","text":"<p>Within a single Kubernetes Pod, multiple containers can exist. While a container itself does not have its own IP address, the Pod does. If there are two or more containers inside the same Pod, they can communicate with each other using <code>localhost</code>. This is akin to members of the same household not needing a telephone to speak to each other; they are within the same \"house\" (the Pod). For example, if you have two containers, <code>c0</code> and <code>c01</code>, within a single Pod, and <code>c01</code> is running an Apache server, <code>c0</code> can access <code>c01</code> by making a request to <code>localhost</code> on the Apache server's port (e.g., <code>localhost:80</code>).</p> <p>To illustrate this, consider a Pod definition that creates two containers: one running Ubuntu (<code>c0</code>) and another running Apache HTTPD (<code>c01</code>). <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\nspec:\n  containers:\n  - name: c0\n    image: ubuntu\n    command: [\"/bin/bash\", \"-c\", \"while true; do sleep 5; done\"]\n  - name: c01\n    image: httpd # Apache HTTP Server\n    ports:\n    - containerPort: 80 # Exposing port 80 for Apache\n</code></pre> After applying this YAML with <code>kubectl apply -f &lt;filename&gt;.yaml</code>, you can check the Pod status using <code>kubectl get pods</code>. Once the Pod is running, you can execute a command inside the <code>c0</code> container to test communication with <code>c01</code>. You would typically install <code>curl</code> in <code>c0</code> first, then run <code>curl localhost:80</code>. A successful response like \"It works\" (the default Apache message) indicates that <code>c0</code> can communicate with <code>c01</code>.</p>"},{"location":"devops/kube5/#pod-to-pod-communication-within-the-same-node","title":"Pod-to-Pod Communication Within the Same Node","text":"<p>When different Pods need to communicate, even if they are on the same Node, they cannot use <code>localhost</code> because they are in different \"houses\" (different Pods). Instead, each Pod is assigned its own unique IP address. These Pod IPs enable communication between different Pods on the same Node. For instance, if you have <code>Pod1</code> running Nginx and <code>Pod2</code> running Apache on the same Node, <code>Pod1</code> can communicate with <code>Pod2</code> by using <code>Pod2</code>'s IP address and the port it exposes.</p> <p>The YAML for two separate Pods would look something like this: For an Nginx Pod (e.g., <code>pod-nginx.yaml</code>): <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n    ports:\n    - containerPort: 80\n</code></pre> For an Apache Pod (e.g., <code>pod-apache.yaml</code>): <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: apache-pod\nspec:\n  containers:\n  - name: apache-container\n    image: httpd\n    ports:\n    - containerPort: 80\n</code></pre> After applying both YAMLs, you can retrieve their IP addresses using <code>kubectl get pods -o wide</code>. Then, from within one Pod (e.g., <code>nginx-pod</code>), you can execute <code>curl &lt;apache-pod-ip&gt;:80</code> to confirm communication.</p> <p>It is important to remember that by default, these Pod IPs are not accessible from outside the Node. They are meant for internal cluster communication.</p>"},{"location":"devops/kube5/#the-challenge-of-ephemeral-pod-ips-and-the-role-of-services","title":"The Challenge of Ephemeral Pod IPs and the Role of Services","text":"<p>A significant challenge in Kubernetes is the ephemeral nature of Pod IPs. Pods are designed to be short-lived and can be terminated and recreated for various reasons, such as scaling operations, updates, or failures. When a Pod is recreated, it gets a new IP address. This poses a major problem for applications or users trying to access these Pods, as they cannot reliably keep track of constantly changing IP addresses. For example, if a frontend application needs to connect to a backend database Pod, and that database Pod's IP keeps changing, the frontend would lose its connection.</p> <p>To overcome this, Kubernetes introduces the Service object. A Service acts as a logical bridge or a stable abstraction layer in front of a set of Pods. It provides a static Virtual IP (VIP) and a DNS name that remains constant, regardless of whether the underlying Pods are terminated, recreated, or moved to different Nodes. When a client connects to the Service's VIP, the Service then redirects the traffic to the appropriate Pod(s). This redirection is dynamically maintained by a component called Kube-proxy, which continuously monitors the cluster and updates the mapping between the Service's VIP and the current Pod IPs. Kube-proxy queries the Kubernetes API server to learn about new Services and Pods in the cluster.</p> <p>Services use labels and selectors to determine which Pods they should target. You define specific labels on your Pods (e.g., <code>app: my-app</code>), and then the Service's selector is configured to match those labels. This ensures the Service only routes traffic to the intended Pods.</p> <p>There are four main types of Services: 1.  ClusterIP: Exposes the Service on a cluster-internal IP. This type is the default and only accessible from within the cluster. 2.  NodePort: Exposes the Service on a static port (the NodePort) on each Node in the cluster. This makes the Service accessible from outside the cluster using <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>. 3.  LoadBalancer: Exposes the Service externally using a cloud provider's load balancer. This type is specific to cloud environments. 4.  Headless: Used when you don't need a stable IP but want direct access to Pods. It doesn't allocate a ClusterIP and provides DNS records for each Pod directly.</p> <p>The source indicates a hierarchy where NodePort builds on ClusterIP, and LoadBalancer builds on NodePort.</p>"},{"location":"devops/kube5/#clusterip-service-example","title":"ClusterIP Service Example","text":"<p>A ClusterIP Service provides a virtual IP that is only visible and accessible from within the cluster. It is commonly used for internal communication between different components of a microservice architecture.</p> <p>Here is a conceptual YAML for a Deployment and a ClusterIP Service: First, a Deployment (e.g., <code>deploy-httpd.yaml</code>) to manage Apache Pods: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: my-apache-app # Label for Pods\n  template:\n    metadata:\n      labels:\n        app: my-apache-app # Labels applied to Pods\n    spec:\n      containers:\n      - name: apache-container\n        image: httpd\n        ports:\n        - containerPort: 80\n</code></pre> Then, a ClusterIP Service (e.g., <code>service-clusterip.yaml</code>) to expose the Deployment: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: demo-service\nspec:\n  selector:\n    app: my-apache-app # Matches the Pods' label\n  ports:\n    - protocol: TCP\n      port: 80 # Service port\n      targetPort: 80 # Container port\n  type: ClusterIP # Explicitly defined as ClusterIP\n</code></pre> After applying these files, <code>kubectl get svc</code> will show the Service and its assigned ClusterIP. You can then use <code>curl &lt;ClusterIP&gt;:80</code> from another Pod within the cluster to access the Apache application. If you then manually delete the Apache Pod (<code>kubectl delete pod &lt;pod-name&gt;</code>), the Deployment will automatically recreate it with a new IP, but the ClusterIP of the Service will remain the same, and you will still be able to access the new Pod using the original ClusterIP. This demonstrates the static nature of the Service's Virtual IP.</p>"},{"location":"devops/kube5/#nodeport-service-example","title":"NodePort Service Example","text":"<p>A NodePort Service enables external access to applications running in Pods. It does this by exposing the Service on a static port (the NodePort, typically in the range 30000-32767) on each Node in the cluster. External users can then access the application using the Node's IP address (or Public DNS) and this specific NodePort. The NodePort will forward the traffic to the Service's ClusterIP, which then routes it to the correct Pod.</p> <p>Using the same Deployment as before, here is a conceptual YAML for a NodePort Service (e.g., <code>service-nodeport.yaml</code>): <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: demo-service\nspec:\n  selector:\n    app: my-apache-app # Matches the Pods' label\n  ports:\n    - protocol: TCP\n      port: 80 # Service port\n      targetPort: 80 # Container port\n      nodePort: 31341 # Optional: specific NodePort, or Kubernetes assigns one\n  type: NodePort # Explicitly defined as NodePort\n</code></pre> After applying this, <code>kubectl get svc</code> will show the NodePort assigned to your Service (e.g., <code>31341:80/TCP</code>). To access the application from outside the cluster (e.g., from your web browser), you would use the Public DNS of your EC2 instance (Node) followed by the NodePort. For instance, <code>http://&lt;EC2-Public-DNS&gt;:31341</code>. It's crucial to ensure that the assigned NodePort is allowed in your cloud provider's security group (e.g., AWS Security Group) for inbound traffic.</p>"},{"location":"devops/kube5/#kubernetes-volumes-for-data-persistence","title":"Kubernetes Volumes for Data Persistence","text":"<p>Containers are designed to be stateless and ephemeral; any data stored directly within a container's filesystem is lost if the container crashes or is terminated. This presents a problem for applications that need to store persistent data. Kubernetes solves this using Volumes. A Volume in Kubernetes is essentially a directory that is accessible to the containers within a Pod. Unlike container-specific storage, a Volume's lifecycle is tied to the Pod, not individual containers.</p> <p>Important Volume behaviors: *   If a container within a Pod crashes or is restarted, the data in the Volume persists and the new or restarted container can access it. *   If the Pod itself is deleted or fails, the Volume associated with it is also deleted, and any data it contained is lost.</p> <p>Volumes can be shared between multiple containers within the same Pod. Kubernetes offers various types of Volumes, including local storage (like <code>emptyDir</code> and <code>hostPath</code>), network file systems (like NFS), and cloud-provider-specific storage solutions (like AWS EBS or Azure Disk). The video focuses on <code>emptyDir</code> and <code>hostPath</code> volumes.</p>"},{"location":"devops/kube5/#emptydir-volume-example","title":"EmptyDir Volume Example","text":"<p>An EmptyDir volume is created when a Pod is assigned to a Node and exists as long as that Pod is running on that Node. It starts empty, hence its name. Its primary purpose is to provide temporary, shared storage for multiple containers within the same Pod. Data in an EmptyDir volume is lost if the Pod is deleted.</p> <p>Consider a Pod definition with two containers (<code>c1</code> and <code>c2</code>) that share an EmptyDir volume: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-volume-emptydir\nspec:\n  containers:\n  - name: c1\n    image: ubuntu\n    command: [\"/bin/bash\", \"-c\", \"while true; do sleep 5; done\"]\n    volumeMounts:\n    - name: exchange # Volume name\n      mountPath: /tmp/exchange # Path inside container c1\n  - name: c2\n    image: ubuntu\n    command: [\"/bin/bash\", \"-c\", \"while true; do sleep 5; done\"]\n    volumeMounts:\n    - name: exchange # Same volume name\n      mountPath: /tmp/data # Path inside container c2\n  volumes:\n  - name: exchange # Defines the EmptyDir volume\n    emptyDir: {}\n</code></pre> After applying this YAML, you can verify data sharing. Execute a command in <code>c1</code> to create a file within its mounted path (<code>/tmp/exchange/</code>). For example, <code>kubectl exec -it my-volume-emptydir -c c1 -- /bin/bash</code> to enter the container, then <code>echo \"Technical Guftgu Zindabad\" &gt; /tmp/exchange/my-file.txt</code>. Then, exit <code>c1</code> and enter <code>c2</code> using <code>kubectl exec -it my-volume-emptydir -c c2 -- /bin/bash</code>. Navigate to <code>c2</code>'s mounted path (<code>/tmp/data/</code>) and list the contents. You should see <code>my-file.txt</code> created by <code>c1</code>. This confirms that both containers are accessing and sharing the same underlying volume.</p>"},{"location":"devops/kube5/#hostpath-volume-example","title":"HostPath Volume Example","text":"<p>A HostPath volume maps a file or directory from the host Node's filesystem into a Pod. This type of volume is useful when you want the data to persist even if the Pod is deleted, as the data resides on the underlying Node's disk. It also allows Pods to access specific files or directories on the host. However, it ties the Pod to a specific Node and is not suitable for multi-node clusters where Pods might move, or for highly available applications unless carefully managed [Outside source - I'm noting this is outside the source, as the source doesn't explicitly mention the limitations of HostPath in multi-node environments, but focuses on its persistence].</p> <p>Here is a conceptual YAML for a Pod using a HostPath volume: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-volume-hostpath\nspec:\n  containers:\n  - name: hostpath-container\n    image: ubuntu\n    command: [\"/bin/bash\", \"-c\", \"while true; do sleep 5; done\"]\n    volumeMounts:\n    - name: test-volume\n      mountPath: /tmp/hostpath-mount # Path inside the container\n  volumes:\n  - name: test-volume\n    hostPath:\n      path: /tmp/data # Path on the host Node's filesystem\n      type: DirectoryOrCreate # Ensures the directory exists or creates it\n</code></pre> After applying this YAML, you can execute a command inside the <code>hostpath-container</code> to create a file in its mounted path (<code>/tmp/hostpath-mount/</code>). For instance, <code>kubectl exec -it my-volume-hostpath -c hostpath-container -- /bin/bash</code>, then <code>echo \"I love Technical Guftgu\" &gt; /tmp/hostpath-mount/my-file.txt</code>. After creating the file, exit the container. Now, access the host Node's filesystem directly (e.g., via SSH into the EC2 instance) and navigate to <code>/tmp/data/</code>. You should find <code>my-file.txt</code> there, demonstrating that the data written by the container was persisted to the host machine. Similarly, if you create a file directly on the host in <code>/tmp/data/</code>, the container can also see and access that file.</p>"},{"location":"devops/kube5/#common-kubernetes-commands","title":"Common Kubernetes Commands","text":"<p>Throughout these operations, various <code>kubectl</code> commands are essential for managing and inspecting Kubernetes resources: *   <code>kubectl apply -f &lt;filename.yaml&gt;</code>: Applies a configuration defined in a YAML file. *   <code>kubectl get &lt;resource-type&gt;</code>: Lists resources (e.g., <code>kubectl get pods</code>, <code>kubectl get deploy</code>, <code>kubectl get svc</code>, <code>kubectl get rs</code>). *   <code>kubectl get &lt;resource-type&gt; -o wide</code>: Provides more detailed output, often including IP addresses. *   <code>kubectl describe &lt;resource-type&gt; &lt;resource-name&gt;</code>: Shows extensive details about a specific resource, including its configuration, events, and related objects. *   <code>kubectl logs -f &lt;pod-name&gt;</code>: Streams logs from a container in a Pod. *   <code>kubectl exec -it &lt;pod-name&gt; -c &lt;container-name&gt; -- &lt;command&gt;</code>: Executes a command inside a specific container, often used to get an interactive shell. *   <code>kubectl scale --replicas=&lt;count&gt; deploy &lt;deployment-name&gt;</code>: Scales a Deployment up or down by changing the desired number of Pod replicas [Previous conversation, not explicit in source, but implied by Deployment functionality]. *   <code>kubectl rollout history deploy &lt;deployment-name&gt;</code>: Shows the revision history of a Deployment [Previous conversation, not explicit in source, but implied by Deployment functionality]. *   <code>kubectl rollout undo deploy &lt;deployment-name&gt;</code>: Reverts a Deployment to its previous version [Previous conversation, not explicit in source, but implied by Deployment functionality]. *   <code>kubectl rollout undo deploy &lt;deployment-name&gt; --to-revision=&lt;number&gt;</code>: Reverts a Deployment to a specific historical revision [Previous conversation, not explicit in source, but implied by Deployment functionality]. *   <code>kubectl delete -f &lt;filename.yaml&gt;</code> or <code>kubectl delete &lt;resource-type&gt; &lt;resource-name&gt;</code>: Deletes a resource.</p>"},{"location":"devops/kube5/#potential-deployment-failure-scenarios-interview-focus","title":"Potential Deployment Failure Scenarios (Interview Focus)","text":"<p>During an interview, understanding why Kubernetes Deployments might fail is crucial. Common reasons include [Previous conversation, not explicit in source, but good for exam/interview]: *   Insufficient Quota: Lack of available CPU or memory resources in the cluster or on the Nodes. *   Unhealthy Nodes: The Kubernetes Nodes are in an unhealthy state or are down, preventing Pod scheduling. *   Image Pull Errors: The specified container image cannot be pulled from the registry due to an incorrect name/tag, authentication issues for private registries, or the image not existing. *   Insufficient Permissions: The user or service account deploying the application lacks the necessary permissions to create or manage Kubernetes resources. *   Limit Ranges: Resource limits defined (e.g., CPU or memory requests/limits) are exceeded, preventing Pods from being scheduled or starting correctly. *   Application Runtime Misconfiguration: Issues within the application code or its configuration prevent it from starting or running correctly inside the container. *   <code>kubectl</code> Connection Issues: Problems with the <code>kubectl</code> client's configuration or network connectivity to the Kubernetes API server.</p>"},{"location":"devops/kube6/","title":"Kube6","text":""},{"location":"devops/kube6/#persistent-volume-and-persistent-volume-claim","title":"Persistent Volume and Persistent Volume Claim","text":"<p>The video introduces the concept of Persistent Volumes (PV) and Persistent Volume Claims (PVC) in Kubernetes as a solution for managing storage persistently, especially in a distributed environment.</p> <p>Understanding the Need for Persistent Storage: Traditionally, in IT setups, organizations use centralized storage systems where multiple workstations can access and save data. This ensures that data is consistently available regardless of which machine is being used. In Kubernetes, a similar need arises because pods, which contain your applications, are ephemeral.</p> <p>Limitations of Previous Storage Concepts: *   EmptyDir: The video revisits the <code>EmptyDir</code> concept, explaining that it creates a temporary storage volume that is shared between containers within the same pod. However, if the pod itself is deleted, the <code>EmptyDir</code> and all its data are also deleted. When a new pod is created, new storage is also created, meaning data from the previous pod is lost. *   HostPath: Another concept discussed is <code>HostPath</code>, where a pod uses a directory directly on the worker node's local file system for storage. The significant problem with <code>HostPath</code> is that if a pod is deleted and then recreated on a different worker node within the cluster (which is a common occurrence in Kubernetes for load balancing and fault tolerance), the data stored on the original worker node's local machine will not be accessible to the new pod. This means data persistence is not guaranteed across different worker nodes.</p> <p>Introducing Persistent Volume (PV): To address the issues of ephemeral storage and data loss across pod recreations or migrations between worker nodes, Kubernetes provides Persistent Volumes (PV). A Persistent Volume is essentially an abstraction of a physical piece of storage that exists independently of a pod or even a specific worker node. It's a common, shared storage that remains available even if pods are deleted or moved. The video illustrates this with an example of an Elastic Block Store (EBS) volume from AWS, which can be connected to multiple worker nodes. If a pod is deleted from one worker node and recreated on another, it can still access the same shared EBS volume, ensuring data consistency. EBS is a block-level storage where data is stored in blocks, and it supports mounting to a single EC2 instance (worker node) at a time.</p> <p>The analogy used in the video explains that an administrator might acquire a large cloud storage (e.g., 100GB from AWS EBS). From this large pool, smaller, independent \"Persistent Volume objects\" are created (e.g., 30GB, 20GB, 50GB). These PVs are then made available for consumption by applications.</p> <p>Persistent Volume Claim (PVC): Pods do not directly access Persistent Volumes. Instead, they make a Persistent Volume Claim (PVC). A PVC is a request for a specific amount of storage with certain access modes. For example, a pod might request 1GB of storage. When a PVC is made, Kubernetes looks for an available Persistent Volume that matches the requested specifications (size, access mode) and then \"binds\" that PV to the PVC. Once bound, the pod can use the storage via its PVC. The video also explains that once the storage is used by a pod and its work is done, the data in the Persistent Volume can be \"reclaimed\" or \"recycled\" for future use by other pods if the original pod is deleted.</p> <p>Example YAML for a Persistent Volume (PV): The video demonstrates creating a PV named <code>my-pv-for-ebs</code> from an AWS EBS volume. This YAML specifies the capacity, access modes, reclaim policy, and the ID of the actual EBS volume. <pre><code># my-persistent-volume.yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv-for-ebs # Name of the Persistent Volume\nspec:\n  capacity:\n    storage: 1Gi # Defines the capacity of this PV (1 Gigabyte)\n  accessModes:\n    - ReadWriteOnce # Indicates that this volume can be mounted as read-write by a single node\n  persistentVolumeReclaimPolicy: Recycle # Specifies that the volume can be recycled after its claim is released\n  awsElasticBlockStore:\n    volumeID: &lt;YOUR_EBS_VOLUME_ID&gt; # The actual ID of your AWS EBS volume\n    fsType: ext4 # The file system type on the EBS volume\n</code></pre> This YAML defines a Persistent Volume named <code>my-pv-for-ebs</code> which is a 1GB slice of an existing AWS EBS volume. It's set to be <code>ReadWriteOnce</code>, meaning it can be mounted by only one node at a time in read-write mode, and its reclaim policy is <code>Recycle</code>, allowing the data to be reused.</p> <p>Example YAML for a Persistent Volume Claim (PVC): Next, a PVC is created to request storage from an available PV. <pre><code># my-pvc.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-ebs-vol-claim # Name of the Persistent Volume Claim\nspec:\n  accessModes:\n    - ReadWriteOnce # Must match the access mode of the PV it intends to bind to\n  resources:\n    requests:\n      storage: 1Gi # Requests 1 Gigabyte of storage\n</code></pre> This <code>my-ebs-vol-claim</code> PVC requests 1GB of storage with <code>ReadWriteOnce</code> access. Kubernetes will then match this claim to a suitable PV, such as <code>my-pv-for-ebs</code>, and bind them.</p> <p>Example YAML for a Deployment using PVC: Finally, a Deployment that utilizes this PVC is created, showing how a pod accesses the persistent storage. <pre><code># my-nginx-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx-container\n          image: nginx\n          ports:\n            - containerPort: 80\n          volumeMounts:\n            - name: my-ebs-volume # Name of the volume to mount\n              mountPath: /usr/share/nginx/html # Path inside the container where the volume will be mounted\n      volumes:\n        - name: my-ebs-volume # Definition of the volume\n          persistentVolumeClaim:\n            claimName: my-ebs-vol-claim # Reference to the PVC that provides the storage\n</code></pre> In this Deployment, an Nginx container is defined, and a <code>volumeMount</code> is created to attach the <code>my-ebs-volume</code> to <code>/usr/share/nginx/html</code> inside the container. The <code>my-ebs-volume</code> itself is defined to use the <code>my-ebs-vol-claim</code> PVC. This setup ensures that any data written to <code>/usr/share/nginx/html</code> within the Nginx container is stored on the persistent EBS volume, maintaining data integrity even if the Nginx pod is deleted and recreated on a different worker node.</p>"},{"location":"devops/kube6/#livenessprobe","title":"LivenessProbe","text":"<p>The video then transitions to the concept of LivenessProbe, explaining its crucial role in ensuring the continuous availability and health of applications running within Kubernetes.</p> <p>The Shortcoming of Basic Kubernetes Monitoring: Kubernetes inherently monitors if a pod is running and if the containers within that pod are running. It performs basic checks, such as sending a ping or running a simple test command, and if a response is received, it assumes the pod and container are healthy. However, this basic check does not guarantee that the application inside the container is actually functioning correctly. For instance, a web server might be running, but the specific application it hosts could be frozen, stuck in a loop, or unable to serve requests. In such a scenario, Kubernetes would still consider the container \"running,\" even though the application is effectively dead.</p> <p>Purpose of LivenessProbe: LivenessProbe addresses this limitation by actively checking the health of the application itself, inside the container. It ensures that the application is not just running, but is also responsive and performing its intended function.</p> <p>How LivenessProbe Works: A LivenessProbe is configured to execute a specific command, make an HTTP GET request, or check a TCP socket at predefined intervals. *   Success: If the probe's check (e.g., a command) executes successfully and returns an exit code of 0, the application is considered healthy. *   Failure: If the check returns a non-zero exit code, times out, or fails to connect, it indicates that the application is unhealthy or stuck. *   Action on Failure: If the LivenessProbe fails a specified number of times consecutively (defined by <code>failureThreshold</code>), Kubernetes will terminate the unhealthy container. Once terminated, Kubernetes will then create a new container to replace it, aiming to bring the application back to a healthy state. This mechanism helps maintain the reliability and availability of your services, especially when combined with load balancers, as it ensures that only healthy application instances are serving traffic.</p> <p>Example YAML for LivenessProbe: The video provides a YAML example for a Deployment that includes a LivenessProbe. <pre><code># my-liveness-probe-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-liveness-probe-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      containers:\n        - name: my-container\n          image: busybox\n          command: [\"/bin/sh\", \"-c\", \"echo 'Hello from LivenessProbe' &gt; /tmp/healthy; sleep 300\"] # Command to run on container startup\n          livenessProbe:\n            exec:\n              command: [\"cat\", \"/tmp/healthy\"] # The command to execute for the probe\n            initialDelaySeconds: 5 # Initial delay before the first probe is performed (in seconds)\n            periodSeconds: 5 # How often the probe is performed (in seconds)\n            timeoutSeconds: 1 # Timeout for the probe command (in seconds)\n            failureThreshold: 3 # Number of consecutive failures before Kubernetes restarts the container\n</code></pre> Explanation of LivenessProbe Parameters: *   The container uses <code>busybox</code> and is configured to create a file <code>/tmp/healthy</code> immediately upon startup, then sleeps for 300 seconds. This setup simulates an application that is initially healthy. *   The <code>livenessProbe</code> is defined using an <code>exec</code> action, meaning it will run a command to check the application's health. The command chosen is <code>cat /tmp/healthy</code>, which attempts to read the content of the <code>healthy</code> file. If the file exists and is readable, <code>cat</code> will return a 0 exit code (success); otherwise, it will return a non-zero exit code (failure). *   <code>initialDelaySeconds: 5</code>: This parameter tells Kubernetes to wait for 5 seconds after the container starts before executing the first LivenessProbe check. This is important to give the application sufficient time to initialize and become ready to serve requests before being probed. *   <code>periodSeconds: 5</code>: After the initial delay, the probe will run the <code>cat /tmp/healthy</code> command every 5 seconds. *   <code>timeoutSeconds: 1</code>: If the <code>cat /tmp/healthy</code> command does not complete and return an output within 1 second, the probe will consider it a failure. *   <code>failureThreshold: 3</code>: This specifies that if the probe fails 3 consecutive times, Kubernetes will consider the container unhealthy, terminate it, and then recreate a new one. This prevents transient issues from immediately causing a container restart.</p>"},{"location":"devops/kube7/","title":"Kube7","text":"<p>This response provides detailed notes from the video, focusing on Kubernetes Namespaces and Resource Quotas, with examples and explanations to aid your study for exams and interviews.</p>"},{"location":"devops/kube7/#kubernetes-namespaces","title":"Kubernetes Namespaces","text":"<p>The video introduces Namespaces as a fundamental concept in Kubernetes for organizing and managing resources within a cluster. It addresses the challenge of managing multiple projects or applications from different companies within a single Kubernetes cluster, which can become complicated if all resources exist in one flat space. The speaker draws an analogy to Git, where initially all work might be done on a <code>master</code> branch, but then branches are created to separate development efforts; Namespaces serve a similar purpose in Kubernetes by providing boundaries or logical separation within a cluster. You can think of a Namespace as a dedicated area or a \"folder\" for a specific project or team, where all related Kubernetes objects like pods, services, and deployments reside.</p> <p>The primary purpose of Namespaces is to make managing multiple projects easier by creating these isolated environments. For example, if a cluster hosts applications for companies A, B, C, and D, putting all their pods and resources in one place would be difficult to manage. By creating a separate Namespace for each project or company, such as a \"Project A\" Namespace or a \"Project B\" Namespace, you establish a clear boundary for their resources. This helps avoid confusion, especially if similar applications are running for different entities. A project manager, for instance, can quickly understand what is running by just looking at the Namespace name, which can be named after the project, team, area, or location.</p> <p>Namespaces also allow for the attachment of authorization and policy rules to specific sections of the cluster. This means you can define which users or teams have access to which Namespace, and what actions they can perform within it. Furthermore, Namespaces are critical for allocating resources effectively, as they allow you to define Resource Quotas (discussed next) for each isolated section. This enables you to give different amounts of resources to different projects based on their needs or the budget allocated to them.</p> <p>By default, when you start a Kubernetes cluster and haven't created any custom Namespaces, all your work and resources are placed in the <code>default</code> Namespace. When you run a command like <code>kubectl get pods</code> without specifying a Namespace, Kubernetes will automatically search for pods within this <code>default</code> Namespace. The video demonstrates this by showing \"No resources found in default namespace\" when no pods are present and no custom Namespace is specified.</p> <p>To view all existing Namespaces in your cluster, you can use the command <code>kubectl get namespaces</code>. Typically, you'll see <code>default</code>, <code>kube-system</code> (used by Kubernetes itself), <code>kube-public</code>, and <code>kube-node-lease</code>.</p> <p>Creating and Managing Namespaces:</p> <p>The video shows how to create your own Namespace using a simple YAML file. For instance, to create a Namespace named <code>dev</code>: <pre><code># namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev # The name of your Namespace\n</code></pre> Once the YAML file is created, you apply it using <code>kubectl apply -f namespace.yaml</code>. After application, running <code>kubectl get namespaces</code> will show your newly created <code>dev</code> Namespace as active.</p> <p>When creating resources like pods within a specific Namespace, you must explicitly mention the Namespace. For example, to create a pod from <code>pod.yaml</code> in the <code>dev</code> Namespace: <code>kubectl apply -f pod.yaml -n dev</code>. If you then try to list pods using <code>kubectl get pods</code> (which defaults to the <code>default</code> Namespace), you will see \"No resources found in default namespace\" because your pod was created in <code>dev</code>. To see pods in your custom Namespace, you must specify it: <code>kubectl get pods -n dev</code>.</p> <p>Similarly, when deleting resources from a specific Namespace, you must include the <code>-n</code> flag: <code>kubectl delete -f pod.yaml -n dev</code>. If you attempt to delete without specifying the Namespace, Kubernetes will look in <code>default</code> and report that the resource is not found there.</p> <p>For convenience, you can change your current Kubernetes context to default to a specific Namespace, so you don't have to use <code>-n</code> every time. This is done with the command: <code>kubectl config set-context --current --namespace=dev</code>. After running this, subsequent <code>kubectl get pods</code> commands will automatically check the <code>dev</code> Namespace. To verify which Namespace your current context is set to, you can use <code>kubectl config view --minify | grep namespace</code>.</p> <p>It's important to remember that most Kubernetes resources like pods, services, and deployments are Namespace-scoped. However, some lower-level or cluster-wide resources, such as Nodes and Persistent Volumes, are not Namespace-scoped. This means Persistent Volumes and Nodes operate at the cluster level and are not tied to any specific Namespace, which is a crucial point for understanding Kubernetes architecture and potentially for interview questions.</p> <p>Namespaces are particularly beneficial for environments with many users or multiple teams sharing a large Kubernetes cluster. For smaller setups with only a few applications and users, Namespaces might not be strictly necessary.</p>"},{"location":"devops/kube7/#resource-quota","title":"Resource Quota","text":"<p>Following the discussion of Namespaces, the video delves into Resource Quota, a mechanism for managing resource consumption within a Namespace. Without Resource Quotas, pods can potentially consume all available CPU and memory on a cluster, leading to resource starvation for other applications.</p> <p>The Need for Resource Quota:</p> <p>By default, containers running on Kubernetes operate without explicit CPU or memory limits. This means an application can scale its resource usage indefinitely if not restrained. While this might seem beneficial for a single application, in a shared cluster, it can lead to one application monopolizing resources, thereby impacting the performance or even availability of other applications. The analogy used here is that of a buffet versus a fixed-menu restaurant: without limits, a container might consume as much as it wants from an available pool, but with limits, it can only take what's allocated.</p> <p>Purpose of Resource Quota:</p> <p>Resource Quotas allow you to define and enforce limits on the total amount of compute resources (like CPU and memory) or object count (like number of pods or services) that a Namespace can consume. This ensures fair resource allocation among different teams or projects sharing a cluster, preventing any single Namespace from overwhelming the system.</p> <p>Key Concepts: Request and Limit</p> <p>Within the context of Resource Quotas, two critical parameters for defining resource allocation for individual containers are:</p> <ol> <li> <p>Request: This is the minimum guaranteed amount of resources (CPU and memory) that a container needs to function. Kubernetes guarantees that this amount will always be available to the container. The Kubernetes scheduler uses the <code>request</code> value to decide which worker node to place a pod on; it will only place a pod on a node that has enough available resources to satisfy its <code>request</code>.</p> <ul> <li>CPU <code>request</code>: Specified in units of cores or millicores (m). One CPU core is equivalent to 1000 millicores. So, 0.5 CPU is 500m.</li> <li>Memory <code>request</code>: Specified in units of bytes, such as megabytes (Mi) or gigabytes (Gi).</li> </ul> </li> <li> <p>Limit: This is the maximum amount of resources (CPU and memory) that a container is allowed to consume. If a container attempts to exceed its CPU limit, it will be throttled. If it exceeds its memory limit, it will be terminated by Kubernetes.</p> <ul> <li>The <code>limit</code> for CPU and memory must always be greater than or equal to its corresponding <code>request</code>. You cannot request more than your limit.</li> </ul> </li> </ol> <p>Example YAML for defining Requests and Limits within a Pod:</p> <p>Here's an example from the video of a pod definition that includes <code>requests</code> and <code>limits</code> for its container: <pre><code># pod-resources.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-resource-pod\nspec:\n  containers:\n  - name: my-container\n    image: busybox\n    command: [\"sh\", \"-c\", \"echo 'Hello'; sleep 3600\"]\n    resources:\n      requests:\n        memory: \"64Mi\" # Request 64 MiB of memory\n        cpu: \"100m\"   # Request 100 millicores of CPU\n      limits:\n        memory: \"128Mi\" # Limit to 128 MiB of memory\n        cpu: \"200m\"   # Limit to 200 millicores of CPU\n</code></pre> This YAML specifies that the <code>my-container</code> needs at least <code>64Mi</code> memory and <code>100m</code> CPU, but will not be allowed to use more than <code>128Mi</code> memory or <code>200m</code> CPU. After creating this pod using <code>kubectl apply -f pod-resources.yaml -n dev</code> (assuming you are in the <code>dev</code> Namespace), you can inspect its resource allocation with <code>kubectl describe pod my-resource-pod</code> to see the <code>Requests</code> and <code>Limits</code> section.</p> <p>Relationship Between ResourceQuota and Container Resources:</p> <p>A ResourceQuota object is typically applied to a Namespace to set aggregate limits on CPU, memory, and storage that all pods within that Namespace can consume collectively.</p> <p>Example YAML for a Resource Quota: <pre><code># my-quota.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: my-quota\nspec:\n  hard:\n    cpu: \"400m\"  # Total CPU limit for the Namespace\n    memory: \"40Mi\" # Total memory limit for the Namespace\n</code></pre> This Resource Quota limits the <code>dev</code> Namespace to a total of 400 millicores of CPU and 40 megabytes of memory.</p> <p>A crucial point demonstrated in the video is that the total sum of <code>requests</code> and <code>limits</code> of all containers within a Namespace must not exceed the <code>hard</code> limits defined in the Resource Quota for that Namespace. The video illustrates a scenario where a Resource Quota was set for <code>400m</code> CPU, but a deployment attempted to create three replicas, each requesting <code>200m</code> CPU. This would total <code>600m</code> CPU, which exceeds the Namespace's <code>400m</code> quota. In such a case, the deployment will fail, and inspecting it with <code>kubectl describe deployment &lt;deployment-name&gt;</code> will show <code>FailedQuota</code> errors, indicating that the requested resources exceed the available quota.</p> <p>Defaulting Behavior of Request and Limit:</p> <p>The video highlights important defaulting behaviors when <code>request</code> and <code>limit</code> are not explicitly defined:</p> <ol> <li> <p>If <code>request</code> is NOT defined, but <code>limit</code> IS defined: In this scenario, the <code>request</code> for the container will automatically be set equal to its <code>limit</code>. This means if you only specify <code>limits: {cpu: \"100m\"}</code>, the container will implicitly have <code>requests: {cpu: \"100m\"}</code> as well. The video demonstrates this by creating a <code>LimitRange</code> with a 1 CPU limit and no request, then a pod gets 1 CPU for both request and limit.</p> </li> <li> <p>If <code>request</code> IS defined, but <code>limit</code> is NOT defined: In this case, the <code>limit</code> for the container will NOT be set equal to its <code>request</code>. Instead, it will be set to the default limit defined for the cluster or Namespace, if one exists. If no such default exists (e.g., in a <code>LimitRange</code> object), the limit might effectively be considered unlimited (0), meaning it can use as much as available. The video demonstrates this by defining a pod with a <code>750m</code> CPU request but no limit. When described, the pod's limit appears as <code>1 CPU</code> (which is <code>1000m</code>), indicating it picked up the default limit, not the request value.</p> </li> <li> <p>If NEITHER <code>request</code> nor <code>limit</code> are defined: By default, containers will have no CPU or memory limits, and can consume resources as needed. The video also explains that in this case, <code>request</code> effectively becomes equal to <code>limit</code>, both being unlimited (or constrained only by the cluster's overall capacity).</p> </li> </ol> <p>These interactions are crucial for understanding how resource allocation behaves in Kubernetes, especially for interview scenarios.</p> <p>The video concludes by reiterating the importance of Namespaces and Resource Quotas for managing complex Kubernetes environments, ensuring proper resource allocation, and preventing resource contention among different projects or teams. It encourages hands-on practice through the provided lab steps to solidify understanding.</p>"},{"location":"devops/kube8/","title":"Kube8","text":"<p>Kubernetes offers powerful capabilities for managing resources and automatically scaling applications to meet demand. This detailed overview will cover Resource Quota for CPU and Memory, and Horizontal Pod Autoscaling (HPA), complete with practical considerations for exams and interviews.</p>"},{"location":"devops/kube8/#kubernetes-resource-quota-cpu-and-memory-management","title":"Kubernetes Resource Quota: CPU and Memory Management","text":"<p>Resource Quotas in Kubernetes allow you to define the minimum and maximum resource allocations for containers within a pod. This is crucial for efficient resource utilization and preventing a single container from consuming excessive resources.</p> <p>Default Resource Ranges for a Container: When defining resource limits for a container, Kubernetes has default ranges that must be adhered to. These ranges specify the minimum and maximum values for CPU and memory that a container can request or be limited to. *   CPU: A container's minimum CPU request cannot be less than 0.5 CPU, and its maximum CPU limit cannot exceed 1 CPU. *   Memory: A container's minimum memory request cannot be less than 500MB, and its maximum memory limit cannot exceed 1GB.</p> <p>Request and Limit Behavior: Understanding the interaction between <code>requests</code> and <code>limits</code> is vital. *   When <code>limit</code> is mentioned but <code>request</code> is not: If you specify a <code>limit</code> for CPU, for instance, 500 milliCPU, but do not mention the <code>request</code>, then the <code>request</code> will automatically be set equal to the <code>limit</code>. This means the container will be allocated 500 milliCPU as its request.     *   Example Scenario: <pre><code># Example part of a Pod YAML definition\nresources:\n  limits:\n    cpu: \"500m\" # Limit is 500 milliCPU\n  # requests: # Request is not mentioned\n</code></pre>         In this case, the effective request will also be 500 milliCPU. *   When <code>request</code> is mentioned but <code>limit</code> is not: If you specify a <code>request</code> for CPU, for example, 600 milliCPU, but do not mention the <code>limit</code>, then the <code>limit</code> will not automatically equal the <code>request</code>. Instead, the <code>limit</code> will default to its maximum allowed value, which for CPU is 1 CPU.     *   Example Scenario: <pre><code># Example part of a Pod YAML definition\nresources:\n  requests:\n    cpu: \"600m\" # Request is 600 milliCPU\n  # limits: # Limit is not mentioned\n</code></pre>         In this case, the effective limit will be 1 CPU. *   When only one resource (e.g., Memory) is mentioned: If you define <code>request</code> and <code>limit</code> only for memory, for instance, and do not mention CPU resources, then CPU will automatically pick up its default values. This means the CPU request will be 0.5 CPU and the CPU limit will be 1 CPU.</p> <p>Memory Resource Quota Examples and Error Scenarios: When defining memory resources, it's critical to stay within the default ranges. *   Setting a Limit Range (e.g., using a <code>LimitRange</code> object):     You can define a <code>LimitRange</code> object to set default minimum and maximum memory for containers within a namespace.     *   Example YAML structure for <code>LimitRange</code>: <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: mem-default-limit-range\nspec:\n  limits:\n  - default:\n      memory: 1Gi # Default limit for memory\n    defaultRequest:\n      memory: 500Mi # Default request for memory\n    type: Container\n</code></pre>         This example sets a maximum memory of 1GB and a minimum request of 500MB for any container within the specified namespace. *   Valid Memory Allocation: If you set a <code>request</code> of 600MB and a <code>limit</code> of 800MB for a container, this is valid because 600MB is above the 500MB minimum request and 800MB is below the 1GB maximum limit.     *   Example YAML part: <pre><code>resources:\n  requests:\n    memory: \"600Mi\"\n  limits:\n    memory: \"800Mi\"\n</code></pre>         When applied, this pod will be created successfully, and <code>kubectl describe pod &lt;pod-name&gt;</code> will show both the requested 600MB and the limited 800MB. *   Error Scenario: Limit Exceeds Maximum: If you try to set a memory limit greater than 1GB (e.g., 1200MB or 1800MB), Kubernetes will throw an error. The error message will explicitly state that the maximum memory for your container is 1GB.     *   Example YAML part leading to error: <pre><code>resources:\n  limits:\n    memory: \"1200Mi\" # This will cause an error\n</code></pre>         The command <code>kubectl apply -f mem-too-high-limit.yaml</code> would result in an error similar to: \"The maximum memory for your container is 1GB and you have increased the limit to 1200MB, which is not possible\". *   Error Scenario: Request Below Minimum: If you attempt to set a memory request less than 500MB (e.g., 300MB), it will also result in an error. The error message will indicate that the minimum memory usage per container is 500MB.     *   Example YAML part leading to error: <pre><code>resources:\n  requests:\n    memory: \"300Mi\" # This will cause an error\n</code></pre>         The command <code>kubectl apply -f mem-too-low-request.yaml</code> would result in an error similar to: \"The minimum memory usage per container is 500MB, but you have requested 300MB which is not possible\".</p> <p>These resource quota configurations ensure that containers operate within defined boundaries, preventing resource hogging and promoting cluster stability. It is important to note that these default limits apply to containers within a specific namespace when you explicitly define it, not necessarily to the default namespace.</p>"},{"location":"devops/kube8/#horizontal-pod-autoscaling-hpa","title":"Horizontal Pod Autoscaling (HPA)","text":"<p>Horizontal Pod Autoscaling (HPA) is a Kubernetes feature that automatically scales the number of pods in a deployment or replica set based on observed CPU utilization or other custom metrics. It's a critical component for achieving automation in modern DevOps environments, as manual scaling is inefficient and impractical for fluctuating loads.</p> <p>Concept and Need for HPA: Imagine an application like Hotstar during a major cricket match, experiencing over 1.2 crore (12 million) viewers. Without auto-scaling, handling such sudden and massive increases in user traffic would require manual intervention, leading to potential service disruptions. HPA addresses this by allowing your application to automatically create new pods (horizontal scaling) when the load increases, and reduce pods when the load decreases.</p> <p>Core Components of HPA: HPA relies on several key components to function effectively: 1.  Scalable Object: HPA scales objects like Deployments, ReplicaSets, or Replication Controllers. It cannot scale every Kubernetes object, such as Services. 2.  Horizontal Pod Autoscaler (HPA) Object: This is a dedicated Kubernetes API resource and a controller that defines the auto-scaling behavior. It monitors the metrics and makes decisions to scale up or down. 3.  Metric Server: This is a crucial add-on that needs to be installed in your cluster. Its primary role is to collect resource metrics (like CPU utilization, memory utilization, storage utilization) from all running pods. The HPA controller constantly queries the Metric Server for this data.</p> <p>How HPA Makes Scaling Decisions: *   Metric Collection: The Metric Server continuously collects data on CPU utilization (and other configured metrics) for all running pods. *   Average Utilization: HPA evaluates the average CPU utilization across all pods of a given scalable object (e.g., a Deployment). For example, if you have two containers, one with 40% load and another with 20% load, the average would be (40+20)/2 = 30%. *   Target Comparison: The HPA configuration includes a target CPU utilization percentage (e.g., 20% or 30%). If the calculated average utilization exceeds this target, HPA will trigger a scale-up event. *   Pod Creation/Deletion: Based on the average utilization relative to the target, HPA calculates how many new pods are needed to bring the average utilization down to the target. It then instructs the Deployment or ReplicaSet to create (or delete) pods accordingly. For instance, if one pod has 80% CPU utilization and the target is 30%, HPA might create two more pods, so the 80% load can be distributed, aiming for an average closer to the 30% target.</p> <p>Scaling Parameters: When configuring HPA, you define key parameters: *   Target CPU Percentage: The desired average CPU utilization for your pods (e.g., <code>20%</code> or <code>30%</code>). *   Minimum Replicas: The minimum number of pods that should always be running, even if the load is very low (e.g., <code>1</code> pod). HPA will not scale down below this number. *   Maximum Replicas: The maximum number of pods HPA can create (e.g., <code>10</code> pods). This prevents uncontrolled scaling and resource exhaustion.</p> <p>Downscaling Behavior and Cooling Period: While HPA scales up quickly when load increases, it has a more cautious approach to scaling down. *   Cooling Period: To prevent rapid, unnecessary scaling fluctuations (known as \"thrashing\"), HPA has a default cooling period of 5 minutes before it will delete pods. This means that even if the load drops significantly, HPA will wait for 5 minutes to confirm that the low load persists before downscaling. If the load increases again within this 5-minute window, it avoids deleting pods unnecessarily. *   Check Frequency: The HPA controller checks the resource metrics from the Metric Server and re-evaluates scaling decisions every 30 seconds.</p> <p>Horizontal vs. Vertical Autoscaling: It is important to distinguish between these two types of scaling: *   Horizontal Scaling: Involves creating new pods to distribute the load across more instances. This is like adding more rooms to a house when more family members arrive. *   Vertical Scaling: Involves increasing the capacity of existing pods (e.g., allocating more CPU or memory to an already running pod). This is like expanding an existing room to accommodate more people. The video uses an analogy of increasing the capacity of an existing 50GB storage to 100GB (vertical) versus adding a completely new 50GB storage (horizontal). HPA focuses on horizontal scaling.</p>"},{"location":"devops/kube8/#practical-implementation-steps-for-hpa","title":"Practical Implementation Steps for HPA","text":"<p>To demonstrate HPA, you need to set up the Metric Server, create a sample application Deployment, and then configure the HPA object.</p> <ol> <li> <p>Setting up the Metric Server:     The Metric Server is not installed by default in Kubernetes and is essential for HPA to work.</p> <ul> <li>Download the Metric Server YAML: You typically download it from its GitHub repository, which is an open-source project.     Command: <code>wget https://raw.githubusercontent.com/kubernetes-sigs/metrics-server/master/deploy/1.8%2B/metrics-server-deployment.yaml</code> (Note: The exact URL might vary slightly depending on the Kubernetes version and Metric Server release. The source indicates a GitHub link for download).</li> <li>Modify the Metric Server YAML: After downloading, you need to edit the <code>metrics-server-deployment.yaml</code> file. Inside the <code>Deployment</code> section, find the <code>args</code> (arguments) for the <code>metrics-server</code> container. You must add the argument <code>--kubelet-insecure-tls</code> to bypass certificate validation, as you typically won't have self-signed certificates set up in a basic lab environment.<ul> <li>YAML modification example (inside <code>containers[].args</code> for <code>metrics-server</code>):     <pre><code># ... (other parts of metrics-server-deployment.yaml)\ncontainers:\n- name: metrics-server\n  image: k8s.gcr.io/metrics-server/metrics-server:v0.5.0 # Example image\n  args:\n    - --cert-dir=/tmp\n    - --secure-port=4443\n    - --kubelet-insecure-tls # Add this line\n    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n    - --kubelet-use-node-status-port\n# ...\n</code></pre></li> </ul> </li> <li>Apply the Metric Server YAML: After modification, apply the YAML file to deploy the Metric Server.     Command: <code>kubectl apply -f metrics-server-deployment.yaml</code>.</li> <li>Verify Metric Server: You can check the pods in the <code>kube-system</code> namespace to ensure the Metric Server pod is running. You can also view its logs to confirm it's collecting metrics.     Commands:<ul> <li><code>kubectl get pods -n kube-system</code></li> <li><code>kubectl logs -f &lt;metrics-server-pod-name&gt; -n kube-system</code> (Look for \"Generated self-signed certificate\" to confirm proper setup).</li> </ul> </li> </ul> </li> <li> <p>Creating a Sample Deployment:     Next, create a sample application deployment that HPA will manage. This deployment will typically include resource requests and limits.</p> <ul> <li>Deployment YAML structure example:     <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deploy # Name of your deployment\nspec:\n  replicas: 1 # Start with 1 replica for demonstration\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      containers:\n      - name: my-container # Name of your container\n        image: httpd:latest # Example image (Apache server)\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: \"200m\" # Request 200 milliCPU\n          limits:\n            cpu: \"500m\" # Limit 500 milliCPU\n</code></pre>     This YAML defines a deployment named <code>my-deploy</code> with an Apache server running in a container, requesting 200m CPU and limited to 500m CPU. It starts with one replica.</li> <li>Apply the Deployment YAML:     Command: <code>kubectl apply -f my-deployment.yaml</code>.</li> <li>Verify Deployment and Pod:     Command: <code>kubectl get all</code> (You should see your deployment, replica set, and one running pod).</li> </ul> </li> <li> <p>Configuring the HPA Object:     Now, create the HPA object that will monitor and scale your deployment.</p> <ul> <li>HPA creation command:     Command: <code>kubectl autoscale deployment my-deploy --cpu-percent=20 --min=1 --max=10</code>.<ul> <li><code>deployment my-deploy</code>: Specifies that HPA should scale the <code>my-deploy</code> deployment.</li> <li><code>--cpu-percent=20</code>: Sets the target average CPU utilization to 20%.</li> <li><code>--min=1</code>: Sets the minimum number of pods to 1.</li> <li><code>--max=10</code>: Sets the maximum number of pods to 10.</li> </ul> </li> <li>Verify HPA: After running the command, you can check the HPA status.     Command: <code>kubectl get hpa</code> (You will see the HPA named <code>my-deploy</code>, target CPU, min/max pods, and current replicas). Initially, the target CPU might show <code>0%</code> or a low value if no load is present.</li> </ul> </li> <li> <p>Demonstrating Autoscaling:     To see HPA in action, you need to simulate a load increase on your application.</p> <ul> <li>Open Two Terminals: Keep one terminal for monitoring and another for generating load.</li> <li>Monitoring Terminal: In the first terminal, continuously monitor your pods and HPA.     Command: <code>watch kubectl get all</code> (This command will refresh every 2 seconds, showing changes in pod counts and HPA metrics). You will observe the <code>TARGET</code> column for your HPA.</li> <li>Load Generation Terminal: In the second terminal, get a shell into your running application pod.     Command: <code>kubectl exec -it &lt;my-deploy-pod-name&gt; -- /bin/bash</code>.</li> <li>Generate Load: Inside the pod's shell, run a command that consumes CPU, such as an <code>apt update</code> or a simple infinite loop.     Command (example for CPU load): <code>while true; do true; done</code> (This command will make the CPU usage of the container spike). Alternatively, if your image has <code>apt</code>, you can run <code>apt update</code> repeatedly.</li> <li>Observe Scaling Up: As you run the load generation command, watch the monitoring terminal. You will see the <code>TARGET</code> CPU percentage of your HPA increase from <code>0%</code> (or its initial low value). Once it consistently exceeds <code>20%</code>, you will observe new pods being created automatically. The <code>REPLICAS</code> count in <code>kubectl get all</code> will increase, and new pod entries will appear as <code>Running</code>.</li> <li>Observe Scaling Down: Once you stop the load generation (e.g., by pressing <code>Ctrl+C</code> in the load generation terminal), the CPU utilization will drop. Due to the 5-minute cooling period, the extra pods will not be deleted immediately. After approximately 5 minutes, if the load remains low, HPA will automatically delete the excess pods, returning the replica count closer to the minimum defined value (e.g., 1).</li> </ul> </li> </ol> <p>This practical demonstration highlights how Kubernetes, through HPA and Metric Server, can automatically manage application scaling based on real-time resource utilization, making your applications resilient and efficient in handling varying loads.</p>"},{"location":"devops/kubernetesarchitecture/","title":"Kubernetes Architecture Explained","text":"<p>The architecture diagram depicted in the image shows two basic components: Master and Worker (also referred to as Node). These can be considered nodes. In the example discussed, the diagram shows a master and two nodes, indicating the use of two EC2 instances for the lab. A node signifies a server that manages containers or pods. The master is the component that controls these nodes. This structure follows a client-server model, similar to systems like Chef. For a lab setup, three EC2 instances would be created, and one would be designated as the master.</p> <p>The diagram also illustrates how users interact with the cluster. Users communicate with the cluster through the API Server. Users can create manifests, which are configuration files written in YAML or JSON format, similar to recipes in Chef. These manifests describe the desired state, such as creating a pod with a specific number of containers. The API Server reads the manifest, understands the desired state, and then communicates with the Controller Manager to initiate the process.</p>"},{"location":"devops/kubernetesarchitecture/#components-of-control-plane","title":"Components of Control Plane","text":""},{"location":"devops/kubernetesarchitecture/#1-kube-api-server","title":"1. Kube API Server","text":"<p>This is described as the most important component, acting as the point of contact for all communication. All other components communicate through the API Server. User requests and requests from nodes always come to the API Server. It acts like a receptionist in a bank, receiving requests and forwarding them to the appropriate components; it doesn't solve the requests itself. Nodes communicate with the API Server, not directly with other master components. The Kubelet component on the nodes communicates with the API Server. It is meant to scale automatically as per load. It is the front end of the control plane.</p>"},{"location":"devops/kubernetesarchitecture/#2-etcd","title":"2. ETCD","text":"<p>This is described as a database or storage, similar to Chef's data bag, which maintains the current state. It stores information about the cluster's state, including the number of containers in a pod, pod IP addresses, and other details. ETCD is an external component, not a fundamental part of Kubernetes itself, but Kubernetes cannot work without it. It acts as a ledger of all activities. Importantly, only the API Server can access ETCD directly; no other component like the Controller Manager or Scheduler can. It stores data as key-value pairs.</p> <p>ETCD has the following features:</p> <ul> <li>Fully replicated \u2013 entire state is available on every node in the cluster</li> <li>Secure \u2013 implements automatic TLS with optional client certificate authentication</li> <li>Fast \u2013 benchmarked at 10,000 writes per second</li> </ul>"},{"location":"devops/kubernetesarchitecture/#3-kube-scheduler","title":"3. Kube Scheduler","text":"<p>This component performs the actions to make the actual state equal to the desired state. If a pod should have four containers but only has three, the Controller Manager notes the mismatch and tells the Scheduler to create the additional container. The Scheduler is the one that actually performs the work, like creating pods. It decides on which node to create a pod based on factors like available resources.</p>"},{"location":"devops/kubernetesarchitecture/#4-controller-manager","title":"4. Controller Manager","text":"<p>This component is responsible for maintaining the balance between the actual state and the desired state of the cluster. It ensures that the requested number of containers for a pod are available. Using a bank analogy, it's like the person who verifies a withdrawal request against the account balance before allowing the cashiers to dispense money. It guarantees that what was requested (desired state) matches what is actually running (actual state).</p> <p>Components on master that run controller: - Node Controller: For checking the cloud provider to determine if a node has been detected in the cloud after it stops responding - Route Controller: Responsible for setting up network routes on your cloud - Service Controller: Responsible for load balancer on your cloud against service of type <code>LoadBalancer</code> - Volume Controller: For creating, attaching, and mounting volumes and interacting with cloud provider to orchestrate volumes</p>"},{"location":"devops/kubernetesarchitecture/#components-of-worker-plane","title":"Components of Worker Plane","text":"<p>The Worker Nodes are depicted separately from the Master. They are also referred to as workers or minions. A cluster can have one master and one node, one master and multiple nodes, or even multiple masters and multiple nodes.</p> <p>Each Worker Node contains three basic components:</p>"},{"location":"devops/kubernetesarchitecture/#1-kubelet","title":"1. Kubelet","text":"<p>This is an agent that runs on each node. It communicates with the API Server and reports the state of the node. It receives requests from the API Server and is responsible for managing pods on the node. Its general task is to create and manage multiple pods. It reports the success or failure status back to the master. Uses port 10255.</p>"},{"location":"devops/kubernetesarchitecture/#2-kube-proxy","title":"2. Kube-Proxy","text":"<p>This component handles the networking for the pods on the node. Its basic job is to assign IP addresses to pods. It runs on each node and ensures that each pod gets its own unique IP address.</p>"},{"location":"devops/kubernetesarchitecture/#3-container-engine","title":"3. Container Engine","text":"<p>This is the software that runs the containers. It is recommended to refer to it as a \"Container Engine\" rather than specifically \"Docker,\" as Kubernetes can work with various container engines like Docker, Rkt, or Containerd. The Container Engine is not a part of Kubernetes itself; it needs to be installed on each node. It works with the Kubelet to pull images, create and run containers, and expose containers on specified ports.</p>"},{"location":"devops/kubernetesarchitecture/#pod","title":"Pod","text":"<p>The diagram also features Pods as the basic unit within the nodes. A Pod is the smallest or atomic unit in Kubernetes. While direct containers can be created with a container engine like Docker, Kubernetes introduces the concept of Pods as a logical unit. Kubernetes talks to Pods, not directly to containers. A Pod is a logical envelope or wrapper for containers. It typically contains one or more containers. In Kubernetes, the control unit is called a Pod, not containers.</p>"},{"location":"flink/iq/","title":"FLINK_IQ","text":"<p>1. Briefly introduce Flink?</p> <p>Apache Flink is an open-source stream processing and batch processing framework designed for big data processing and analytics. It provides fault tolerance, high throughput, and low-latency processing of large-scale data streams</p> <p>2. What are the differences between Flink and Spark Streaming?</p> <ol> <li>The design ideas are different. Flink considers batch to be a kind of streaming, and spark considers a streaming batch.</li> <li>The architecture model is different. Spark has Driver, Master, Worker, and Executor. Flink has the concepts of TaskManager, JobManager, Task, SubTask, and Slot</li> <li>Flink's streaming data processing is much stronger than spark, for example, time supports three kinds of time There are more windows than spark</li> <li>In the case of out-of-order data, Flink is stronger than spark, because flink has watermark. In fact, the calculation method when running is the time of the last data-if watermaker is greater than the end of the window, execute</li> <li>For fault tolerance, flink is also better than spark. For example, flink supports two-stage transactions to ensure that data after program crashes will not be re-consumed. Spark also has checkpoints, but it only ensures that data is not lost, and it cannot be repeated. consumption.</li> </ol> <p>3. What are the roles of Flink cluster? What are the functions?</p> <p>Flink programs mainly have three roles: TaskManager, JobManager, and Client when they are running.</p> <p>JobManager In the role of a manager in the cluster, it is the coordinator of the entire cluster. It is responsible for receiving the execution of Flink Job, coordinating checkpoints, recovering from failures, and managing Task Manager.</p> <p>TaskManager It is responsible for the resource information on the node where the manager is located, such as memory, disk, and network. It will report the resource to the JobManager when it is started.</p> <p>Client It is the client submitted by the Flink program. When a user submits a Flink program, a Client is first created. Then the program submitted by the user will be preprocessed and submitted to the cluster for processing.</p> <p>4. What is TaskSlot?</p> <p>In Flink's architecture, TaskManager is the working node that is actually used to execute our program. TaskManager is a JVM process. In fact, in order to achieve the concept of resource isolation and parallel execution, the concept of TaskSlot was proposed at this time, which is actually In order to control how many Tasks the TaskManager can receive, the TaskManager is controlled by taskslot, that is, if we have a source that specifies three parallelism, then he will use three slots, and the other one needs to be mainly parallel as an operator When the degree is the same, and there is no change in the degree of parallelism, or there is no shuffle, they will be together at this time. This is an optimized concept.</p> <p>5. What are the commonly used operators in Flink?</p> <p>Map operator</p> <p>Filter operator</p> <p>KeyBy operator</p> <p>Window window</p> <p>6. What is the parallelism of Flink and What is the parallelism setting of Flink?</p> <p>The parallelism of Flink is well understood. For example, kafkaSource, its parallelism is the number of partitions by default. The degree of parallelism is this operator, and how many taskslot are needed, we should know that is the advantage of parallel computing. Generally, the degree of parallelism is set according to the amount of data. It is best to keep the source and map operators without shuffle, because the pressure on the source and map operators is not very large, but when our data table is widened, It is better to set it larger.</p> <p>7. What is the relationship between Flink's Slot and parallelism?</p> <p>slot is a concept in TaskManager. Parallelism is a concept in the program, that is, the concept of execution level. In fact, slot specifies how many slots this TaskManager has and how much parallelism can be supported, but the parallelism developed by the program uses slots That is slot, that is, TaskManager is the provider and the program is the user</p> <p>8. What if Flink encounters an abnormal restart of the program?</p> <p>Flink has some restart strategies, and as long as the checkpoint is done, it can be done at least once. Of course, it may not be accurate once, but some components can be done. The restart strategy generally set is a fixed delay restart strategy. The restart does not delete the checkpoint. Generally, the number of restarts set by our company is 4 times. If it stops, we will send a nail warning and start from the checkpoint when it starts.</p> <p>9. Flink's distributed cache</p> <p>The distributed cache implemented by Flink is similar to Hadoop. The purpose is to read the file locally and put it in the taskmanager node to prevent the task from repeatedly pulling data and reduce performance.</p> <p>10. Broadcast variables in Flink</p> <p>We know that Flink is parallel, and the calculation process may not be performed in a Slot. Then there is a situation: when we need to access the same data. Then the broadcast variable in Flink is to solve this situation. We can understand the broadcast variable as a public shared variable. We can broadcast a dataset, and then different tasks can be obtained on the node. There will only be one copy of this data on each node.</p> <p>11. Do you know what windows in Flink are?</p> <p>Flink supports two ways to divide windows, according to time and count. session is also a kind of time</p> <p>Tumbing Count Window\uff1a Perform calculation when reaching a certain number, no folding</p> <p>Sliding Time Window\uff1a When a certain period of time is reached, roll over, there can be overlap, generally used to calculate the recent demand, such as nearly 5 minutes.</p> <p>Tubing time Window\uff1a When a certain period of time is reached, the slide is carried out, which can be thought of as the Nokia slide phone used before. This is actually a micro batch</p> <p>Sliding Count Window\uff1a Slide when it reaches a certain number</p> <p>Session Window: The window data has no fixed size, it is divided according to the parameters passed in by the user, and the window data does not overlap. It is similar to calculating the user's previous actions when the user logs out.</p> <p>12. Flink's state storage?</p> <p>Flink often needs to store intermediate states during calculations to avoid data loss and recover from abnormal states. Choosing a different state storage strategy will affect the state interaction between JobManager and Subtask, that is, JobManager will interact with State to store state.</p> <p>Flink provides three state storage: MemoryStateBackend</p> <p>FsSateBackend</p> <p>RocksDBStateBackend</p> <p>13. What kind of time are there in Flink?</p> <p>Event time: the time when the event actually occurred</p> <p>Intake time: time to enter flink</p> <p>Processing time: the time to enter the flink operator</p> <p>14. What is Watermark in Flink?</p> <p>Watermark is an operation used by Flink to deal with out-of-order time. In fact, in Flink, if we use event time and take kafka's source, then the window execution time at this time is the smallest among the partitions Partitions are used for triggering, and each partition must be triggered to perform calculations. Why is this? In fact, it is because the partitions of Kafka are disordered. Orderly in the zone. The execution time is the maximum time minus the watermark&gt;window end time, and the calculation will be executed at this time. Watermarks are used in Apache Flink to track the progress of event time. They represent a threshold for event times and indicate that no events with timestamps earlier than the watermark should arrive any longer. They help define when window computations should be considered complete.</p> <p>15. What is Unbounded streams in Apache Flink?</p> <p>Any type of data is produced as a stream of events. Data can be processed as unbounded or bounded streams. Unbounded streams have a beginning but no end. They do not end and continue to provide data as it is produced. Unbounded streams should be processed continuously, i.e., events should be handled as soon as they are consumed. Since the input is unbounded and will not be complete at any point in time, it is not possible to wait for all of the data to arrive. Processing unbounded data sometimes requires that events are consuming in a specific order, such as the order in which events arrives, to be able to reason about result completeness.</p> <p>16. What is Bounded streams in Apache Flink?</p> <p>Bounded streams have a beginning and an end point. Bounded streams could be processed by consuming all data before doing any computations. Ordered ingestion is not needed to process bounded streams since a bounded data set could always be sorted. Processing of bounded streams is also called as batch processing.</p> <p>17. What is Dataset API in Apache Flink?</p> <p>The Apache Flink Dataset API is used to do batch operations on data over time. This API is available in Java, Scala, and Python. It may perform various transformations on datasets such as filtering, mapping, aggregating, joining, and grouping.</p> <p>DataSet API helps us in enabling the client to actualize activities like a guide, channel, gathering and so on.It is utilized for appropriated preparing, it is an uncommon instance of stream preparing where we have a limited information source.They are regular programs that implement transformation on data sets like filtering, mapping, etc.</p> <p>Data sets are created from sources like reading files, local collections, etc.All the results are returned through sinks, the execution can happen in a local JVM or on clusters of many machines.</p> <p>DataSet&gt; wordCounts = text .flatMap(new LineSplitter()) .groupBy(0) .sum(1); <p>18. What is DataStream API in Apache Flink?</p> <p>The Apache Flink DataStream API is used to handle data in a continuous stream. On the stream data, you can perform operations such as filtering, routing, windowing, and aggregation. On this data stream, there are different sources such as message queues, files, and socket streams, and the resulting data can be written to different sinks such as command line terminals. This API is supported by the Java and Scala programming languages.</p> <p>DataStream&gt; dataStream = env .socketTextStream(\"localhost\", 9091) .flatMap(new Splitter()) .keyBy(0) .timeWindow(Time.seconds(7)) .sum(1); <p>19. What is Apache Flink Table API?</p> <p>Table API is a relational API with an expression language similar to SQL. This API is capable of batch and stream processing. It is compatible with the Java and Scala Dataset and Datastream APIs. Tables can be generated from internal Datasets and Datastreams as well as from external data sources. You can use this relational API to perform operations such as join, select, aggregate, and filter. The semantics of the query are the same if the input is batch or stream. val tableEnvironment = TableEnvironment.getTableEnvironment(env) // register a Table tableEnvironment.registerTable(\"TestTable1\", ...); // create a new Table from a Table API query val newTable2 = tableEnvironment.scan(TestTable1).select(...);</p> <p>20. What is Apache Flink FlinkML?</p> <p>FlinkML is the Flink Machine Learning (ML) library. It is a new initiative in the Flink community, with an expanding list of algorithms and contributors. FlinkML aims to include scalable ML algorithms, an easy-to-use API, and tools to help reduce glue code in end-to-end ML systems. Note: Flink Community has planned to delete/deprecate the legacy flink-libraries/flink-ml package in Flink1.9, and replace it with the new flink-ml interface proposed in FLIP39 and FLINK-12470.</p> <p>21. Explain the Apache Flink Job Execution Architecture?</p> <p>Program: It is a piece of code that is executed on the Flink Cluster.</p> <p>Client: It is in charge of taking code from the given programm and creating a job dataflow graph, which is then passed to JobManager. It also retrieves the Job data.</p> <p>JobManager: It is responsible for generating the execution graph after obtaining the Job Dataflow Graph from the Client. It assigns the job to TaskManagers in the cluster and monitors its execution.</p> <p>TaskManager:It is in charge of executing all of the tasks assigned to it by JobManager. Both TaskManagers execute the tasks in their respective slots in the specified parallelism. It is in charge of informing JobManager about the status of the tasks.</p> <p>22. What are the features of Apache Flink?</p> <p>One of the key features of Apache Flink is its ability to process data in real-time with low-latency and high throughput. It supports event time processing, which means it can handle out-of-order events and provide correct results based on event timestamps. Flink also provides extensive windowing operations for aggregating data within time intervals, such as tumbling windows, sliding windows, and session windows.</p> <p>Another important feature of Flink is its support for fault-tolerance. It achieves fault-tolerance through a mechanism called \"exactly-once\" processing, which guarantees that each event is processed exactly once, even in the presence of failures. This is crucial for applications where data correctness is paramount.</p> <p>Apache Flink can process all data in the form of streams at any point in time. Apache Flink does not give a burden on users' shoulders for tunning or managing the physical execution concepts. The memory management is done by the Apache Flink itself and not by the user. Apache Flink optimizer chooses the best plan to execute the user's program hence very little intervention is required in terms of tunning. Apache Flink can be deployed on various cluster frameworks. It is capable to support various types of the file system. Apache Flink can be integrated with Hadoop YARN in a very good way.</p> <p>23. What are the Apache Flink domain-specific libraries?</p> <p>The following is the list of Apache Flink domain-specific libraries. FlinkML: It is used for machine learning. Table: It is used to perform the relational operation. Gelly: It is used to perform the Graph operation. CEP: It is used for complex event processing.</p> <p>24. What is the programing model of Apache Flink?</p> <p>The Apache Flink Datastream and the Dataset work as a programming model of flink and its other layers of architecture. The Datastream programming model is useful in real-time stream processing whereas the Dataset programming model is useful in batch processing.</p> <p>25. What are the use cases of Apache Flink?</p> <p>Many real-world industries are using Apache Flink and their use cases are as mentioned below.</p> <p>Financial Services Financial industries are using Flink to perform fraud detection in real-time and send mobile notifications.</p> <p>Healthcare The hospitals are using Apache Flink to collect data from devices such as MRI, IVs for real-time issue detection and analysis.</p> <p>Ad Tech Ads companies are using Flink to find out the real-time customer preference.</p> <p>Oil Gas The Oil and Gas industries are using Flink for real-time monitoring of pumping and issue detection.</p> <p>Telecommunications The telecom companies are using Apache Flink to provide the best services to the users such as real-time view of billing and payment, the optimization of antenna per-user location, mobile offers, and so on.</p> <p>26. What is bounded and unbounded data in Apache Flink?</p> <p>Apache Flink processes data in the form of bounded and unbounded streams. The bounded data will have a start point and an endpoint and the computation starts once all data has arrived. It is also called batch processing. The unbounded data will have a start point but no endpoint because it is streaming of data. The processing of unbounded data is continous and doesn't wait for complete data. As soon the data is generated the processing will start.</p> <p>27. How Apache Flink handles the fault-tolerance?</p> <p>Apache Flink manages the fault-tolerance of stream applications by capturing the snapshot of the streaming dataflow state so in case of failure those snapshots will be used for recovery. For batch processing, Flink uses the program's sequence of transformations for recovery.</p> <p>To achieve fault tolerance, Apache Flink employs a combination of techniques such as data replication, checkpointing, and exactly-once processing semantics. The framework allows users to define fault-tolerant data streams, which are resilient to failures and can effectively recover from possible errors.</p> <ol> <li> <p>Checkpointing: Apache Flink periodically captures the state of executing jobs by taking checkpoints. Checkpoints consist of the in-memory state of all operators and the metadata necessary for restoring the state, such as the offset of each stream source. Users can configure the frequency of checkpoints to strike a balance between reliability and performance</p> </li> <li> <p>State Backends: Apache Flink supports different state backends (e.g., in-memory, RocksDB) to persist checkpointed state. The chosen backend determines how and where the state is stored, allowing for fault tolerance and efficient recovery.</p> </li> <li> <p>Exactly-once Processing: Flink's checkpointing, along with its transactional processing capabilities, enables exactly-once processing semantics. It ensures that each record is processed exactly once, even in the presence of failures or system restarts. This guarantees consistency and correctness in data processing.</p> </li> <li> <p>Failure Handling: In case of failures, Flink automatically reverts the system to the latest successful checkpoint. It replays the data from that point onwards, resuming processing from a consistent state.</p> </li> </ol> <p>28. What is the responsibility of JobManager in Apache Flink Cluster?</p> <p>The Job Manager is responsible for managing and coordinating with distributed processing of a program. It assigns the task to node managers, handles the failures for recovery, and performs the checkpointing. It has three components namely ResourceManager, Dispatcher, and JobMaster.</p> <p>29. What is the responsibility of TaskManager in Apache Flink Cluster?</p> <p>The Task Manager is responsible for executing the dataflow task and return the result to JobManager. It executes the task in the form of a slot hence the number of slots shows the number of process execution.</p> <p>30. What is the difference between stream processing and batch processing?</p> <p>In Batch processing, the data is a bounded set of the stream that has a start point and the endpoint, so once the entire data is ingested then only processing starts in batch processing mode. In-stream processing the nature of data is unbounded which means the processing will continue as the data will be received.</p> <p>Flink How to ensure accurate one-time consumption Flink There are two ways to ensure accurate one-time consumption Flink Mechanism 1\u3001Checkpoint Mechanism 2\u3001 Two stage submission mechanism</p> <p>Checkpoint Mechanism:Mainly when Flink Turn on Checkpoint When , Will turn out for the Source Insert a barrir, And then this barrir As the data flows all the time , When it comes to an operator , This operator starts to make checkpoint, It's made from barrir The state of the current operator when it comes to the previous time , Write the state to the state backend . And then barrir Flow down , When it flows to keyby perhaps shuffle Operator time , For example, when the data of an operator , Depending on multiple streams , There will be barrir alignment , That is, when all barrir All come to this operator to make checkpoint, Flow in turn , When it flows to sink Operator time , also sink The operator is also finished checkpoint Will send to jobmanager The report checkpoint n Production complete .</p> <p>Two stage submission mechanism: Flink Provides CheckpointedFunction And CheckpointListener These two interfaces ,CheckpointedFunction There is snapshotState Method , Every time checkpoint Trigger execution method , The cache data is usually put into the State , You can think of it as one hook, This method can be used to achieve pre submission ,CheckpointListyener There is notifyCheckpointComplete Method ,checkpoint Notification method after completion , There are some extra operations that can be done here . for example FLinkKafkaConumerBase Use this to do Kafka offset Submission of , In this method, you can implement the submit operation . stay 2PC If the corresponding process, such as a checkpoint Failure words , that checkpoint It will roll back , No impact on data consistency , So if you're informing checkpoint Success followed by failure , Then it will be in initalizeSate Method to complete the transaction commit , This ensures data consistency . It's mainly based on checkpoint The state file to judge .</p> <p>flink and spark difference: flink It's a similar spark Of \" Open source technology stack \", Because it also provides batch processing , Flow computation , Figure calculation , Interactive query , Machine learning, etc .flink It's also memory computing , similar spark, But the difference is ,spark The calculation model of is based on RDD, Consider streaming as a special batch process , His DStream In fact, or RDD. and flink Consider batch processing as a special stream computing , But there are two engines in the layer of batch processing and streaming computing , Abstract the DataSet and DataStream.flink It's also very good in performance , Streaming delay ratio spark Less , Can do real flow computing , and spark It can only be a quasi flow calculation . And in batch processing , When the number of iterations gets more ,flink Faster than spark faster , So if flink Come out earlier , Maybe more than what we have now Spark More fire .</p> <p>31. Flink watermark Transmission mechanism.</p> <p>Flink Medium watermark Mechanism is used to deal with disorder ,flink It has to be event time , A simple example is , If the window is 5 second ,watermark yes 2 second , that All in all 7 second , When will calculation be triggered at this time , Suppose the initial time of the data is 1000, Then wait until 6999 It will trigger 5999 The calculation of windows , So the next one is 13999 Is triggered when 10999 The window of In fact, this is watermark The mechanism of , In multi parallelism , For example, in kafka The window will not be triggered until all partitions are reached</p> <p>32. Flink window join:</p> <p>1\u3001window join, That is, according to the specified fields and scrolling sliding window and session window inner join</p> <p>2\u3001 yes coGoup In fact, that is left join and right join,</p> <p>3\u3001interval join That is to say In the window join There are some problems , Because some of the data really came after the meeting , It's still a long time , Then there will be interval join But it has to be the time of the event , And also specify watermark And water level and getting event timestamps . And set it up Offset interval , because join I can't wait all the time .</p> <p>33. keyedProcessFunction How it works</p> <p>keyedProcessFunction There is one ontime Operation of the , If so event In time that The time to call is to look at ,event Of watermark Is it greater than trigger time Time for , If it is greater than, calculate it , No, just wait , If it is kafka Words , Then the default is to trigger the partition key in the shortest time .</p> <p>34. How to deal with offline data such as the association with offline data?</p> <p>1\u3001async io 2\u3001broadcast 3\u3001async io + cache 4\u3001open Method , Then the thread is refreshed at a fixed time , Cache updates are deleted first , Then write another one, and then write to the cache</p> <p>35. What if there is a data skew?</p> <p>Flink How to view data skew \uff1a stay flink Of web ui You can see the data skew in , It's every one subtask The amount of data processed varies greatly , For example, some have only one M yes , we have 100M This is a serious data skew . KafkaSource Data skew at the end For example, upstream kafka It was specified when it was sent key There are data hotspots , So just after the access , Do a load balancing \uff08 The premise is not keyby\uff09. Aggregation class operator data skew Pre aggregation plus global aggregation</p> <p>36. FlinkTopN And offline TopN The difference between?</p> <p>topn It is a common function in both offline and real-time computing , It's different from... In offline computing topn, Real time data is continuous , This will give topn It's very difficult to calculate , Because it's going to keep a... In memory topn Data structure of , When new data comes , Update this data structure</p> <p>37. Sparkstreaming and flink in checkpoint?</p> <p>sparkstreaming Of checkpoint It will lead to repeated consumption of data however flink Of checkpoint Sure Make sure it's accurate one time , At the same time, it can be incremental , fast checkpoint Of , There are three states ,memery\u3001rocksdb\u3001hdfs</p> <p>38. A brief introduction cep State programming:</p> <p>Complex Event Processing\uff08CEP\uff09\uff1a FLink Cep Is in FLink Complex time processing library implemented in ,CEP Allows event patterns to be detected in an endless stream of time , Give us a chance to grasp the important parts of the data , One or more time streams composed of simple events are matched by certain rules , Then output the data the user wants , That is, complex events that satisfy the rules .</p> <p>Flink Data aggregation in , How to aggregate without windows valueState Used to save a single value ListState Used to hold list Elements MapState Used to save a set of key value pairs ReducingState Provided with ListState Same method , Return to one ReducingFunction The aggregated value . AggregatingState and ReducingState similar , Return to one AggregatingState The value after internal aggregation</p> <p>39. How to deal with abnormal data in Flink.</p> <p>Abnormal data in our scenario , It is generally divided into missing fields and outlier data . outliers \uff1a For example, data on the age of the baby , For example, for the maternal and infant industry , The age of a baby is a crucial data , It's the most important , Because the baby is bigger than 3 At the age of 20, you hardly buy things from mothers and babies . There are days like ours \u3001 Unknown \u3001 And for a long time . This is an exception field , We will show the data to store managers and regional managers , Let them know how many ages are not allowed . If we have to deal with it , It can be corrected in real time according to the time of purchase , For example, maternity clothing \u3001 The rank of milk powder \u3001 The size of a diaper , As well as pacifiers, some can distinguish the age group to carry on the processing . We don't process the data in real time , We're going to have a low-level strategy task, night dimension, to run , Run once a week . Missing field \uff1a For example, some fields are really missing , If you can fix it, you can fix it . Give up if you can't fix it , It's like the news recommendation filter in the last company .</p> <p>40. Is there any possibility of data loss in Flink?</p> <p>Flink There are three kinds of data consumption semantics \uff1a At Most Once One consumption at most In case of failure, it may be lost At Least Once At least once If there is a fault, it may be repeated Exactly-Once Exactly once If something goes wrong , It can also ensure that the data will not be lost or repeated . flink The new version is no longer available At-Most-Once semantics .</p> <p>Flink interval join Can you write it simply DataStream keyed1 = ds1.keyBy(o -&gt; o.getString(\"key\")) DataStream keyed2 = ds2.keyBy(o -&gt; o.getString(\"key\")) // Time stamp on the right -5s&lt;= Stream timestamp on the left &lt;= Time stamp on the right -1s keyed1.intervalJoin(keyed2).between(Time.milliseconds(-5), Time.milliseconds(5)) <p>41. How to maintain Checkpoint?</p> <p>By default , If set Checkpoint Options ,Flink Only the most recently generated 1 individual Checkpoint. When Flink When the program fails , From the nearest one Checkpoint To recover . however , If we want to keep more than one Checkpoint, And can choose one of them to recover according to the actual needs , It's more flexible .Flink Support to keep multiple Checkpoint, Need to be in Flink Configuration file for conf/flink-conf.yaml in , Add the following configuration to specify that at most Checkpoint The number of . For small files, please refer to The death of Daedalus - Solutions to the problem of small files in the field of big data .</p> <p>42. What's the difference at Spark and Flink Serialization?</p> <p>Spark The default is Java Serialization mechanism , At the same time, there is an optimization mechanism , That is to say kryo Flink It's a self implemented serialization mechanism , That is to say TypeInformation</p> <p>43. How to deal with late data?</p> <p>In Flink, late data refers to events that arrive after the watermark has progressed past their event time. This typically happens due to network delays, out-of-order arrival, or slow event sources.</p> <p>To handle late data, Flink provides the following mechanisms:</p> <p>Allowed Lateness You can configure how long Flink should wait for late events using the allowedLateness() method.</p> <p>java Copy Edit .window(TumblingEventTimeWindows.of(Time.minutes(5))) .allowedLateness(Time.minutes(2)) This keeps the window open for 2 extra minutes to accept late events.</p> <p>Late elements within this period will update the window and trigger re-evaluation.</p> <p>Side Output for Late Data Events that arrive after the allowed lateness can be redirected to a side output for separate handling.</p> <p>java Copy Edit OutputTag lateOutputTag = new OutputTag(\"late-data\"){}; windowedStream     .sideOutputLateData(lateOutputTag); You can process or store this late data elsewhere for analysis or alerting. <p>Watermarks Configuration</p> <p>Watermarks indicate the progress of event time. You can use bounded out-of-orderness watermarks to tolerate out-of-order events.</p> <p>java Copy Edit env.assignTimestampsAndWatermarks(     WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofMinutes(2)) ); <p>44. When to use aggregate perhaps process:</p> <p>aggregate\uff1a Incremental aggregation process\uff1a Total polymerization When calculating the accumulation operation, you can use aggregate operation . When calculating the full amount of data in the window, use process, For example, sorting and other operations.</p> <p>45. How does Flink handle stateful computations efficiently:</p> <p>Flink is designed to handle stateful computations efficiently by using a distributed and fault-tolerant mechanism called StateBackend. It provides several options for managing state, including in-memory state and state that can be stored on disk or in an external system like Apache Hadoop or Amazon S3.</p> <p>Flink's StateBackend allows users to choose between three options: MemoryStateBackend, FsStateBackend, and RocksDBStateBackend. These options differ in terms of their trade-offs between performance and fault tolerance. For example, the MemoryStateBackend provides high performance and low latency but does not recover state after a failure, while the FsStateBackend provides fault tolerance by storing state on a distributed file system.</p> <p>46. What are the important factors to consider when tuning the performance of Apache Flink applications:</p> <p>When tuning the performance of Apache Flink applications, several important factors need to be considered. These factors range from resource allocation to algorithm design and configuration settings. Here are some key aspects to focus on:</p> <ol> <li> <p>Resource Allocation: Efficiently allocating resources is crucial for optimal performance. This includes tuning the number of Task Managers and slots, Memory, and CPU resources. Understanding the data and workload patterns can help determine the right resource allocation strategy.</p> </li> <li> <p>Data Serialization and Deserialization: Choosing the appropriate serialization format can greatly impact performance. Flink provides multiple serialization formats, such as Avro, JSON, and Protobuf. Assessing the size and complexity of data structures can help decide the most suitable serialization method.</p> </li> <li> <p>Memory Management: Flink employs managed memory which consists of both heap and off-heap memory. Configuring the sizes of managed memory components, such as network buffers and managed memory fractions, can significantly impact performance</p> </li> <li> <p>Parallelism: Parallelism influences the throughput and resource utilization of Flink applications. Setting an appropriate degree of parallelism, considering the available resources and input characteristics, is important.</p> </li> <li> <p>Operators' Chaining and State Size: Operator chaining can optimize performance by reducing serialization and deserialization costs. Additionally, Flink provides various state backends, such as MemoryStateBackend and RocksDBStateBackend, that allow selecting different state storage options based on the state size and access patterns.</p> </li> </ol> <p>47. How does Flink handle exactly-once semantics and end-to-end consistency in data processing?:</p> <p>Apache Flink provides built-in mechanisms to handle exactly-once semantics and ensure end-to-end consistency in data processing pipelines. This is essential in scenarios where duplicate or lost data cannot be tolerated, such as financial transactions, data pipelines, or event-driven applications.</p> <p>Flink achieves exactly-once semantics through a combination of checkpointing, state management, and transactional sinks. Checkpointing is a mechanism that periodically takes a snapshot of the application's state, including the operator's internal state and the position in the input streams. By storing these checkpoints persistently, Flink can recover the state and precisely revert to a previous consistent state when failures occur. The state managed by operators includes both user-defined operator state and Flink's internal bookkeeping state.</p> <p>To enable exactly-once semantics, it is important to ensure that the output of the pipeline is also processed atomically and deterministically. Flink achieves this through transactional sinks, which are responsible for writing the output of a stream into an external system (e.g., a database). When a failure occurs, these sinks coordinate with Flink's checkpointing to guarantee that the data is only committed if the checkpoint is successful. This ensures that the output of the pipeline is consistent and non-duplicative</p> <p>48. How does Flink handle windowing in stream processing?:</p> <p>Flink supports various windowing operations, such as tumbling windows, sliding windows, and session windows. Windowing in Flink allows you to group and process events based on time or count constraints. It provides flexibility in defining window sizes and slide intervals.</p> <p>49. If everything is a stream, why are there a DataStream and a DataSet API in Flink?:</p> <p>Bounded streams are often more efficient to process than unbounded streams. Processing unbounded streams of events in (near) real-time requires the system to be able to immediately act on events and to produce intermediate results (often with low latency). Processing bounded streams usually does not require producing low latency results, because the data is a while old anyway (in relative terms). That allows Flink to process the data in a simple and more efficient way.</p> <p>The DataStream API captures the continuous processing of unbounded and bounded streams, with a model that supports low latency results and flexible reaction to events and time (including event time). The DataSet API has techniques that often speed up the processing of bounded data streams. In the future, the community plans to combine these optimizations with the techniques in the DataStream API.</p> <p>50. What are windowing strategies in Flink?</p> <p>Windowing assigns unbounded streams into finite-sized windows for aggregation. Common strategies include: Time windows (fixed or sliding), Count windows (fixed number of events), and Session windows (gaps between events define windows).</p> <p>51. What are the different types of operators in Flink?:</p> <p>Flink operators include map, flatmap, filter, keyBy, window, reduce, aggregate, join, etc., offering a rich set of transformations for data processing.</p> <p>52. What is the difference between a keyed and non-keyed stream in Flink?:</p> <p>Keyed streams partition data based on a key, enabling operations like windowing and stateful aggregations on specific keys. Non-keyed streams process data without partitioning.</p> <p>53. Explain the role of the process function in Flink.</p> <p>Process functions are low-level operators that provide fine-grained control over event processing and state management. They offer advanced features like timers and custom state management logic.</p> <p>54. What are the different windowing techniques in Apache Flink?:</p> <p>Apache Flink supports a variety of windowing techniques, including:</p> <p>Tumbling windows: These windows are fixed in size and slide across the stream.</p> <p>Sliding windows: These windows are variable in size and slide across the stream.</p> <p>Session windows: These windows are based on the arrival time of events.</p> <p>Count windows: These windows are based on the number of events that arrive within a given time interval.</p> <p>55. What is the difference between bounded and unbounded streams?:</p> <p>Bounded streams are datasets that have a defined start and end. Unbounded streams are datasets that have a defined start but no defined end. Bounded streams can be processed by ingesting the complete data and them preforming any computations. Unbounded streams have to be processed continuously as new data comes in. In most cases, unbounded streams have to be process in the order in which messages are received. Bounded messages can be processed in any order since the messages can be sorted as needed.</p> <p>56. What features does flink framework provide to handle state?</p> <p>Flink framework provides the following features to handle state. Data structure specific state primitives - Flink framework provides specific state primitives for different data structures such as lists and maps. Pluggable state storages - Flink supports multiple pluggable state storage systems that store state in-memory or on disc. Exactly-once state consistency - Flink framework has checkpoint and recovery algorithms, which guarantee the consistency of state in case of failures. Store large state data - Flink has the ability to store very large application state data, of several terabytes, due to its asynchronous and incremental checkpoint algorithm. Scalable Applications - Flink applications are highly scalable since the application state data can be distributed across containers.</p> <p>57. What features does flink framework provide to handle time?:</p> <p>Flink framework provides the following features to handle time. Event-time mode - Flink framework supports applications that process steams based on event-time mode, i.e. applications that process streams based on timestamp of events. Processing-time mode - Flink framework also supports applications that process streams based on processing-time mode, i.e. applications that process streams based on the clock time of the processing machine. Watermark support Flink framework provides watermark support in the processing of streams based on event-time mode. Late data processing Flink framework supports the processing of events that arrive late, after a related computation has already been performed.</p>"},{"location":"hadoop/Bigdata/","title":"Overview","text":"<p>Big Data is a collection of data that is huge in volume, yet growing exponentially with time. It is a data with so large size and complexity that none of traditional data management tools can store it or process it efficiently.</p>"},{"location":"hadoop/Bigdata/#5vs-of-bigdata","title":"5V's of Bigdata","text":"<ul> <li> <p>Volume</p> </li> <li> <p>Velocity</p> </li> <li> <p>Variety</p> </li> <li> <p>Veracity</p> </li> <li> <p>Value</p> </li> </ul>"},{"location":"hadoop/Bigdata/#examples-of-bigdata","title":"Examples Of BigData","text":"<ul> <li> <p>Social Media</p> </li> <li> <p>Healthcare</p> </li> <li> <p>Finance</p> </li> <li> <p>Telecommunications</p> </li> <li> <p>E-commerce</p> </li> <li> <p>IoT</p> </li> <li> <p>Transport</p> </li> </ul>"},{"location":"hadoop/Bigdata/#types-of-data","title":"Types of Data","text":"<ul> <li> <p>Structured Data: This is data that adheres to a model and is easily searchable. Examples include data stored in relational databases and spreadsheets.</p> </li> <li> <p>Unstructured Data: This type of data does not have a predefined model or is not organized in a pre-defined manner. Examples include text files, images, videos, emails, web pages, and social media posts.</p> </li> <li> <p>Semi-Structured Data: This is a hybrid of structured and unstructured data. While it does not conform to the formal structure of data models, it contains tags or other markers to enforce hierarchy and order. Examples include JSON, XML, and email messages with both defined fields and free-form text.</p> </li> </ul>"},{"location":"hadoop/Bigdata/#what-is-a-cluster","title":"What is a cluster","text":"<p>A cluster, in the context of computing, refers to a group of computers or servers that work together and can be viewed as a single system. These computers, known as nodes, interact with each other to accomplish a common goal. This setup is used to improve performance and availability over that provided by a single computer, while typically being much more cost-effective and scalable than a single computer of comparable speed or availability. </p>"},{"location":"hadoop/Bigdata/#vertical-scaling","title":"Vertical Scaling","text":"<p>Vertical Scaling, also known as scaling up, involves increasing the capacity of a single server, such as using a more powerful CPU, adding more RAM, or increasing disk space.</p>"},{"location":"hadoop/Bigdata/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>Horizontal Scaling, also known as scaling out, involves adding more servers to a system and distributing the load across multiple servers. </p>"},{"location":"hadoop/HDFS/","title":"HDFS","text":""},{"location":"hadoop/HDFS/#hadoop","title":"Hadoop","text":"<p>Hadoop is an open-source software framework for storing and processing big data in a distributed fashion on large clusters of commodity hardware. Essentially, it accomplishes two tasks: massive data storage and faster processing. It was developed by the Apache Software Foundation and is based on two main components:</p> <ul> <li> <p>Hadoop Distributed File System (HDFS): This is the storage component of Hadoop, designed to hold large amounts of data, potentially in the range of petabytes or even exabytes. The data is distributed across multiple nodes in the cluster, providing high availability and fault tolerance.</p> </li> <li> <p>Map-Reduce: This is the processing component of Hadoop, which provides a software framework for writing applications that process large amounts of data in parallel. MapReduce operations are divided into two stages: the Map stage, which sorts and filters the data, and the Reduce stage, which summarizes the data.</p> </li> <li> <p>Yet Another Resource Negotiator (YARN): This is the resource management layer in Hadoop. Introduced in Hadoop 2.0, YARN decouples the programming model from the resource management infrastructure, and it oversees and manages the compute resources in the clusters.</p> </li> </ul>"},{"location":"hadoop/HDFS/#properties-of-hadoop","title":"Properties of Hadoop","text":"<ul> <li> <p>Scalability: Can store and distribute large data sets across many servers.</p> </li> <li> <p>Cost-effectiveness: Designed to run on inexpensive, commodity hardware.</p> </li> <li> <p>Flexibility: Can handle any type of data, structured or unstructured.</p> </li> <li> <p>Fault Tolerance: Data is automatically replicated to other nodes in the cluster.</p> </li> <li> <p>Data Locality: Processes data on or near the node where it's stored, reducing network I/O.</p> </li> <li> <p>Simplicity: Provides a simple programming model (MapReduce) for processing data.</p> </li> <li> <p>Open-source: Freely available to use and modify with a large community of contributors.</p> </li> </ul>"},{"location":"hadoop/HDFS/#hdfs-architecture-and-core-concepts","title":"HDFS Architecture and Core Concepts","text":"<p>HDFS (Hadoop Distributed File System) is a distributed file system. It is designed using a master/slave architecture.</p> <p>A Hadoop cluster consists of multiple computers networked together.</p> <p>A rack is a physical enclosure where multiple computers are fixed.Each rack typically has its individual power supply and a dedicated network switch. The importance of racks lies in the possibility of an entire rack failing if its switch or power supply goes out of network, affecting all computers within it. Multiple racks are connected, with their switches linked to a core switch, forming the Hadoop cluster.</p> <p>-- Master/Slave Architecture: NameNode and DataNode</p> <p>In HDFS, there is one master and multiple slaves.</p> <ul> <li> <p>NameNode (Master Node): The Hadoop master is called the NameNode. It is called NameNode because it stores and manages the names of directories and files within the HDFS namespace.</p> <p>Responsibilities:</p> <ol> <li>Manages the file system namespace. </li> <li>Regulates access to files by clients (e.g., checking access permissions, user quotas). </li> <li>Maintains an image of the entire HDFS namespace in memory, known as in-memory FS image (File System Image). This allows it to perform checks quickly. </li> <li>Does not store actual file data. </li> <li>Assigns DataNodes for block storage based on free disk space information from DataNodes. </li> <li>Maintains the mapping of blocks to files, their order, and all other metadata.</li> </ol> </li> <li> <p>DataNode (Slave Node): The Hadoop slaves are called DataNodes. They are called DataNodes because they store and manage the actual data of the files.</p> <p>Responsibilities:</p> <ol> <li>Stores file data in the form of blocks. </li> <li>Periodically sends a heartbeat to the NameNode to signal that it is alive. This heartbeat also includes resource capacity information that helps the NameNode in making decisions. </li> <li>Sends a block report to the NameNode, which is health information about all the blocks maintained by that DataNode.</li> </ol> </li> </ul> <p>--- Key terminologies and Components of HDFS</p> <ul> <li> <p>Block</p> <p></p> <p>Block is nothing but the smallest unit of storage on a computer system. It is the smallest contiguous storage allocated to a file. In Hadoop, we have a default block size of 128MB or 256MB.</p> <p>Note</p> <p>If you have a file of 50 MB and the HDFS block size is set to 128 MB, the file will only use 50 MB of one block. The remaining 78 MB in that block will remain unused, as HDFS blocks are allocated on a per-file basis. It's important to note that this is one of the reasons why HDFS is not well-suited to handling a large number of small files. Since each file is allocated its own blocks, if you have a lot of files that are much smaller than the block size, then a lot of space can be wasted. This is also why block size in HDFS is considerably larger than it is in other file systems (default of 128 MB, as opposed to a few KBs or MBs in other systems).</p> <p>Larger block sizes mean fewer blocks for the same amount of data, leading to less metadata to manage, less communication between the NameNode and DataNodes, and better performance for large, streaming reads of data.</p> </li> <li> <p>Replication Management</p> <p>To provide fault tolerance HDFS uses a replication technique. In that, it makes copies of the blocks and stores in on different DataNodes. Replication factor decides how many copies of the blocks get stored. It is 3 by default but we can configure to any value.</p> </li> <li> <p>Rack Awareness</p> <p>A rack contains many DataNode machines and there are several such racks in the production. HDFS follows a rack awareness algorithm to place the replicas of the blocks in a distributed fashion. This rack awareness algorithm provides for low latency and fault tolerance. Suppose the replication factor configured is 3. Now rack awareness algorithm will place the first block on a local rack. It will keep the other two blocks on a different rack. It does not store more than two blocks in the same rack if possible.</p> </li> <li> <p>Secondary Namenode</p> <p>The Secondary NameNode in Hadoop HDFS is a specially dedicated node in the Hadoop cluster that serves as a helper to the primary NameNode, but not as a standby NameNode. Its main roles are to take checkpoints of the filesystem metadata and help in keeping the filesystem metadata size within a reasonable limit.</p> <p>Here is what it does:</p> <ol> <li> <p>Checkpointing: The Secondary NameNode periodically creates checkpoints of the namespace by merging the fsimage file and the edits log file from the NameNode. The new fsimage file is then transferred back to the NameNode. These checkpoints help reduce startup time of the NameNode</p> </li> <li> <p>Size management: The Secondary NameNode helps in reducing the size of the edits log file on the NameNode. By creating regular checkpoints, the edits log file can be purged occasionally, ensuring it does not grow too large.</p> </li> </ol> <p>A common misconception is that the Secondary NameNode is a failover option for the primary NameNode. However, this is not the case; the Secondary NameNode cannot substitute for the primary NameNode in the event of a failure. For that, Hadoop 2 introduces the concept of Standby NameNode.</p> </li> <li> <p>Standby Namenode </p> <p>In Hadoop, the Standby NameNode is part of the High Availability (HA) feature of HDFS that was introduced with Hadoop 2.x. This feature addresses one of the main drawbacks of the earlier versions of Hadoop: the single point of failure in the system, which was the NameNode.</p> <ol> <li> <p>The Standby NameNode is essentially a hot backup for the Active NameNode. The Standby NameNode and Active NameNode are in constant synchronization with each other. When the Active NameNode updates its state, it records the changes to the edit log, and the Standby NameNode applies these changes to its own state, keeping both NameNodes in sync.</p> </li> <li> <p>The Standby NameNode maintains a copy of the namespace image in memory, just like the Active NameNode. This means it can quickly take over the duties of the Active NameNode in case of a failure, providing minimal downtime and disruption.</p> </li> <li> <p>Unlike the Secondary NameNode, the Standby NameNode is capable of taking over the role of the Active NameNode immediately without any data loss, thus ensuring the High Availability of the HDFS system.</p> </li> </ol> <p></p> <p>Hadoop incorporates robust features for fault tolerance and high availability to ensure the reliability and continuous operation of the cluster. These two concepts, while related to system resilience, address different aspects of failure within the Hadoop ecosystem.</p> </li> </ul>"},{"location":"hadoop/HDFS/#hadoop-fault-tolerance","title":"Hadoop Fault Tolerance","text":"<p>Fault tolerance in Hadoop primarily addresses what happens when a data node fails. If a data file is broken into blocks and stored across various data nodes, the failure of one such node could lead to the loss of a part of the file, making it unreadable. Hadoop's solution to this fundamental problem is replication. It involves creating backup copies of each data block and storing them on different data nodes. This mechanism ensures that if one copy becomes unavailable, the data can still be read from another copy.</p> <p>The number of copies made for each block is determined by the replication factor, which can be configured on a file-by-file basis and even modified after a file has been created in HDFS. For instance, if a file's replication factor is set to two, HDFS automatically creates two copies of each block for that file, ensuring they are placed on different machines. Typically, the replication factor is set to three, which is considered a reasonably good level of protection, though it can be increased for files deemed super critical.</p> <p>Beyond individual node failures, Hadoop also provides protection against entire rack failures through rack awareness. Without rack awareness, if all three copies of a file's block were on nodes within the same rack, an entire rack failure would lead to the loss of all copies. By configuring Hadoop for rack awareness, it ensures that at least one copy of a block is placed in a different rack, thereby protecting against such widespread failures.</p> <p>The Name Node plays a crucial role in maintaining the desired replication factor. Each data node sends periodic heartbeats to the Name Node. If a data node stops sending heartbeats, the Name Node identifies it as failed. In response, the Name Node initiates the re-replication of affected blocks to restore the number of replicas to the configured factor, for example, back to three. The Name Node constantly monitors and tracks the replication factor of every block and triggers replication whenever necessary. Reasons for re-replication can include a data node becoming unavailable, a replica getting corrupted, a hard disk failing on a data node, or even a user increasing the replication factor of a file.</p> <p>While replication offers robust protection against failures, it comes with a cost: increased storage consumption. Making three copies of a file effectively reduces the cluster's usable storage capacity to one-third of its raw capacity, leading to higher costs. To mitigate this, Hadoop 2.x introduced storage policies, and Hadoop 3.x offers Erasure Coding as an alternative to traditional replication. Despite these alternatives, replication remains the conventional method for fault avoidance, and its costs are generally manageable because disks are relatively inexpensive.</p>"},{"location":"hadoop/HDFS/#hadoop-high-availability","title":"Hadoop High Availability","text":"<p>High availability refers to the uptime of a system, representing the percentage of time a service is operational. Enterprises typically aim for extremely high uptime, such as 99.999%, for their critical systems. It's important to distinguish high availability from fault tolerance: while data node, disk, or even rack failures, as discussed in fault tolerance, do not typically bring down the entire Hadoop cluster, high availability specifically addresses faults that would render the entire system unusable. The cluster, as a whole, usually remains available during data-related faults, with replication handling the underlying data protection.</p> <p>The primary single point of failure in a Hadoop cluster is the Name Node. The Name Node is responsible for maintaining the file system namespace, including the list of directories and files, and managing the mapping of files to their blocks. Every client interaction with the Hadoop cluster begins with the Name Node. Consequently, if the Name Node fails, the entire Hadoop cluster becomes unusable, preventing any read or write operations. Therefore, protecting against Name Node failures is essential to achieve high availability for a Hadoop cluster.</p> <p>The fundamental solution to protect against any failure is a backup. For the Name Node, this involves two key aspects: backing up all the HDFS namespace information that the Name Node maintains and having a standby Name Node machine readily available to take over its role quickly.</p> <p>The Name Node maintains the complete file system in its memory as an in-memory FS image. Additionally, it keeps an edit log on its local disk, which records every change made to the file system like a journal. Because the in-memory FS image can be reconstructed from the edit log, backing up the Name Node's edit log is crucial. The recommended solution for backing up the edit log in Hadoop 2.x is the Quorum Journal Manager (QJM). The QJM consists of at least three machines, each running a lightweight Journal Node daemon. Instead of writing edit log entries to its local disk, the Name Node is configured to write them to the QJM. Utilizing three Journal Nodes provides double protection for the critical edit log, and for even higher protection, a QJM can consist of five or seven nodes.</p> <p>A separate machine is added to the cluster and configured as a Standby Name Node. This Standby Name Node is set up to continuously read the edit log from the QJM, ensuring it stays updated with the latest file system changes. This configuration enables the Standby Name Node to assume the active Name Node role within a few seconds. Furthermore, all data nodes are configured to send their block reports (health information for blocks) to both the Active and Standby Name Nodes.</p> <p>The mechanism by which the Standby Name Node determines that the Active Name Node has failed and should take over is managed by Zookeeper and Failover Controllers. A Failover Controller runs on each Name Node. The Failover Controller on the active Name Node maintains a lock in Zookeeper, while the Standby Name Node's Failover Controller continuously attempts to acquire this lock. If the Active Name Node fails or crashes, the lock it held in Zookeeper expires. As soon as the Standby Name Node successfully acquires the lock, it recognizes that the active Name Node has failed and proceeds to transition from its standby state to the active role.</p>"},{"location":"hadoop/HDFS/#secondary-name-node","title":"Secondary Name Node","text":"<p>It is common to confuse the Secondary Name Node with the Standby Name Node, but they serve distinct purposes. As explained, a Standby Name Node acts as a direct backup for the Name Node in case of failure. The Secondary Name Node, however, addresses a different operational concern.</p> <p>When the Name Node restarts, for example, due to maintenance, it loses its in-memory FS image. It then reconstructs this image by reading the edit log. The challenge arises because the edit log grows continuously, and its size directly impacts the Name Node's restart time. A very large edit log could cause the Name Node to take an hour or more to start, which is undesirable.</p> <p>The Secondary Name Node solves this problem by performing a checkpoint activity periodically, typically every hour. During a checkpoint, the Secondary Name Node performs the following steps:</p> <ol> <li> <p>It reads the current edit log.</p> </li> <li> <p>It then creates the latest file system state, which is an exact copy of the in-memory FS image.</p> </li> <li> <p>This state is then saved to disk as an on-disk FS image.</p> </li> <li> <p>Once the new on-disk FS image is created, the Secondary Name Node truncates (clears) the edit log, as all the changes have been applied to the new on-disk image.</p> </li> <li> <p>For subsequent checkpoints, the Secondary Name Node reads the previous on-disk FS image and applies only the new changes accumulated in the edit log during the last hour. It then replaces the old on-disk FS image with the new one and truncates the edit log again.</p> </li> </ol> <p>This checkpointing process is essentially a merging of an on-disk FS image and the edit log. It is a quick process because it only deals with a limited amount of new edit logs (e.g., from the last hour), and the FS image itself is smaller compared to the cumulative edit log. The primary benefit is that when the Name Node eventually restarts, it also performs this quick checkpoint activity, reading a much smaller edit log (only from the last checkpoint) and applying it to the latest on-disk FS image, thus minimizing the restart time.</p> <p>It is important to note that when a Hadoop High Availability configuration is implemented with a Standby Name Node, the Standby Name Node also performs this checkpoint activity. Consequently, in a high availability setup, a separate Secondary Name Node service is no longer required.</p>"},{"location":"hadoop/HDFS/#write-operation-in-hdfs","title":"Write Operation in HDFS","text":"<p>When a client intends to write a file into HDFS, the very first step involves the client interacting with the NameNode. The NameNode is considered the \"centerpiece\" of the HDFS cluster because it stores all the metadata and possesses complete information about the entire cluster's data and its slave nodes (DataNodes). Therefore, any write operation must begin with a create request sent from the client to the File System API, which then forwards it to the NameNode. The NameNode's initial role is to check for access rights to ensure that the specific user or users have permission to write to the requested path.</p> <p>Once access rights are verified, the NameNode's crucial function is to provide the address of the slave (DataNode) where the client should begin writing the data directly. It's a significant point to understand that the client sends only one copy of the data. This is a key design choice because if the client were to send multiple copies (e.g., three copies for a replication factor of three), it would create significant network overhead. For instance, writing 10 TB of data would necessitate sending 30 TB over the network, which is inefficient.</p> <p>After the client starts writing the data directly to the initial DataNode via an FS Data Output Stream, the replication process begins among the DataNodes themselves, not initiated by the client. This process follows a data write pipeline: once the first DataNode has received and started writing a block, it immediately starts copying that block to another DataNode. This second DataNode, upon receiving the block, in turn starts copying it to a third DataNode, and so on, until the required replication level is achieved. For example, if the replication factor is three, the block will be written to DataNode 1, then DataNode 1 will copy to DataNode 3, and DataNode 3 will copy to DataNode 7. All decisions regarding which DataNodes handle the replication are taken care of by the NameNode (the master). DataNodes are in constant communication with the NameNode and report block information.</p> <p>Once the required replicas are created, an acknowledgement process takes place in reverse order. The last DataNode in the pipeline sends an acknowledgement to the second-to-last DataNode, which then sends it to the first DataNode. Finally, the first DataNode sends the ultimate acknowledgement back to the client.</p> <p>An important aspect of HDFS write operations is that they occur in parallel. It's not a serialized process where block one is fully written before block two begins. While write operations are generally costly, HDFS's parallel writing prevents them from being prohibitively slow, ensuring that writing terabytes of data doesn't take days. Furthermore, HDFS is designed to automatically handle failures during writing. If any DataNode goes down while data is being written, the NameNode will immediately provide the address of another DataNode where the data can be copied, ensuring data integrity and availability.</p> <p>A common question is who divides the large file into smaller blocks. The responsibility for dividing the file into smaller blocks falls to the HDFS client itself (also referred to as the Hadoop client). The user or their specific machine does not need to manually divide or specify any logic for splitting the file. The Hadoop setup itself acts as the Hadoop client, containing all the necessary APIs to perform this division automatically. You, as the user, are the client, but the actual HDFS setup on your machine acts as the 'Hadoop client' that performs the block division.</p>"},{"location":"hadoop/HDFS/#read-operation-in-hdfs","title":"Read Operation in HDFS","text":"<p>When a client wishes to read a file from HDFS, the initial and crucial step involves the client interacting with the NameNode. The NameNode is recognized as the \"centerpiece\" of the HDFS cluster because it is responsible for storing all the metadata related to files and their block locations across the various DataNodes. Before providing any information, the NameNode first checks for access rights to ensure that the particular user or client is authorized to read the requested file.</p> <p>Once access rights are verified, the NameNode's primary role shifts to providing the specific locations of the data blocks. It will furnish the client with the addresses of the DataNodes where each block of the file is actually stored. For example, it might instruct the client to \"go on slave two to read block one,\" \"go on slave ten to read block two,\" and so on. A very important distinction in HDFS is that the client then directly interacts with these respective DataNodes to read the file, and the NameNode is not involved in the actual data transfer during the read process.</p> <p>The reason for this direct client-DataNode interaction, bypassing the NameNode for data flow, is critical: the NameNode would become a severe bottleneck if all data had to pass through it. Considering that HDFS deals with data in the range of petabytes and operates across thousands of nodes, routing all read operations through a single NameNode would make it incredibly inefficient and slow. Therefore, the read operation itself is distributed and performed in parallel directly from the DataNodes, which significantly enhances efficiency. For instance, one client might be reading a block from DataNode One, while another client simultaneously reads a different block from DataNode Three, or even the same client reads different blocks from different DataNodes in parallel.</p> <p>To ensure security during this direct interaction, the NameNode doesn't just give out DataNode addresses freely. It also provides a token to the client. This token acts like an \"ID card\"; the client must show this token to the DataNode for authentication before the DataNode grants access to the data. This mechanism prevents unauthorized access to the data stored on the DataNodes.</p> <p>At a deeper, API level, when an end-user initiates a read operation on their client machine, a Java Virtual Machine (JVM) starts. The very first API class to come into play is the <code>HDFS client class</code>. This class sends an \"open request\" to the <code>Distributed File System API</code>. This <code>File System API</code> then interacts directly with the NameNode to request the block locations. After the NameNode performs its authorization and authentication checks, it provides the necessary block locations. Following this, the client uses an <code>FS Data Input Stream</code> \u2013 a standard Java API specifically adapted for Hadoop \u2013 to start directly reading the data from the DataNodes.</p> <p>Finally, HDFS is designed with robust fault tolerance during read operations. If, at any point while reading data, a DataNode goes down (e.g., due to a server failure or power loss), there are no issues. The client will immediately complain to the NameNode about the problem. The NameNode, which constantly monitors its DataNodes through heartbeats, will detect that the particular slave has stopped responding. In such a scenario, the NameNode will then provide the location of another DataNode where the same block of data is available. This ensures that the client can continue reading the file seamlessly without data loss or interruption.</p>"},{"location":"hadoop/hadoopiq/","title":"Hadoop","text":"<p>What is the difference between Nodes in HDFS?</p> <p>The differences between NameNode, BackupNode and Checkpoint NameNode are as follows:</p> <p>NameNode: NameNode is at the heart of the HDFS file system that manages the metadata i.e. the data of the files is not stored on the NameNode but rather it has the directory tree of all the files present in the HDFS file system on a Hadoop cluster. NameNode uses two files for the namespace: fsimage file: This file keeps track of the latest checkpoint of the namespace. edits file: This is a log of changes made to the namespace since checkpoint.</p> <p>Checkpoint Node:    Checkpoint Node keeps track of the latest checkpoint in a directory that has the same structure as that of NameNode\u2019s directory. Checkpoint node creates checkpoints for the namespace at regular intervals by downloading the edits and fsimage file from the NameNode and merging it locally. The new image is then again updated back to the active NameNode.</p> <p>BackupNode: This node also provides check pointing functionality like that of the Checkpoint node but it also maintains its up-to-date in-memory copy of the file system namespace that is in sync with the active NameNode.</p> <p>How will you disable a Block Scanner on HDFS DataNode?</p> <p>In HDFS, there is a configuration dfs.datanode.scan.period.hours in hdfs-site.xml to set the number of hours interval at which Block Scanner should run. We can set dfs.datanode.scan.period.hours=0 to disable the Block Scanner. It means it will not run on HDFS DataNode.</p> <p>How will you get the distance between two nodes in Apache Hadoop?</p> <p>In Apache Hadoop we can use the NetworkTopology.getDistance() method to get the distance between two nodes. Distance from a node to its parent is considered as 1.</p> <p>How does inter cluster data copying work in Hadoop?</p> <p>In Hadoop, there is a utility called DistCP (Distributed Copy) to perform large inter/intra-cluster copying of data. This utility is also based on MapReduce. It creates Map tasks for files given as input. After every copy using DistCP, it is recommended to run cross checks to confirm that there is no data corruption and the copy is complete.</p> <p>What is the Replication factor in HDFS?</p> <p>Replication factor in HDFS is the number of copies of a file in a file system. A Hadoop application can specify the number of replicas of a file it wants HDFS to maintain. This information is stored in NameNode. We can set the replication factor in following ways:</p> <p>We can use Hadoop fs shell, to specify the replication factor for a file. Command as follows: $hadoop fs \u2013setrep \u2013w 5 /file_name In the above command, the replication factor of file_name file is set as 5.</p> <p>We can also use Hadoop fs shell, to specify the replication factor of all the files in a directory. $hadoop fs \u2013setrep \u2013w 2 /dir_name In the above command, the replication factor of all the files under directory dir_name is set as 2.</p> <p>What are the two messages that NameNode receives from DataNode?</p> <p>NameNode receives following two messages from every DataNode:</p> <pre><code>Heartbeat: This message signals that DataNode is still alive. Periodic receipt of Heartbeat is very important for NameNode to decide whether to use a DataNode or not.\n\nBlock Report: This is a list of all the data blocks hosted on a DataNode. This report is also very useful for the functioning of NameNode. With this report, NameNode gets information about what data is stored on a specific DataNode.\n</code></pre> <p>How does indexing work in Hadoop?</p> <p>Indexing in Hadoop has two different levels. Index based on File URI: In this case data is indexed based on different files. When we search for data, the index will return the files that contain the data. Index based on InputSplit: In this case, data is indexed based on locations where input split is located.</p> <p>What data is stored in a HDFS NameNode?</p> <p>NameNode is the central node of an HDFS system. It does not store any actual data on which MapReduce operations have to be done. But it has all the metadata about the data stored in HDFS DataNodes. NameNode has the directory tree of all the files in the HDFS filesystem. Using this metadata it manages all the data stored in different DataNodes.</p> <p>What would happen if NameNode crashes in a HDFS cluster?</p> <p>There is only one NameNode in a HDFS cluster. This node maintains metadata about DataNodes. Since there is only one NameNode, it is the single point of failure in a HDFS cluster. When NameNode crashes, the system may become unavailable. We can specify a secondary NameNode in the HDFS cluster. The secondary NameNode takes the periodic checkpoints of the file system in HDFS. But it is not the backup of NameNode. We can use it to recreate the NameNode and restart it in case of a crash.</p> <p>What are the main functions of Secondary NameNode?</p> <p>Main functions of Secondary NameNode are as follows:</p> <p>FsImage: It stores a copy of the FsImage file and EditLog.</p> <p>NameNode crash: In case NameNode crashes, we can use Secondary NameNode's FsImage to recreate the NameNode.</p> <p>Checkpoint: Secondary NameNode runs Checkpoint to confirm that data is not corrupt in HDFS.</p> <p>Update: It periodically applies the updates from EditLog to the FsImage file. In this way the FsImage file on Secondary NameNode is kept up to date. This helps in saving time during NameNode restart.</p> <p>What happens if an HDFS file is set with a replication factor of 1 and DataNode crashes?</p> <p>Replication factor is the same as the number of copies of a file on HDFS. If we set the replication factor of 1, it means there is only 1 copy of the file. In case, DataNode that has this copy of file crashes, the data is lost. There is no way to recover it. It is essential to keep a replication factor of more than 1 for any business critical data.</p> <p>What is the meaning of Rack Awareness in Hadoop?</p> <p>In Hadoop, most of the components like NameNode, DataNode etc are rack- aware. It means they have the information about the rack on which they exist. The main use of rack awareness is in implementing fault-tolerance. Any communication between nodes on the same rack is much faster than the communication between nodes on two different racks. In Hadoop, NameNode maintains information about the rack of each DataNode. While reading/writing data, NameNode tries to choose the DataNodes that are closer to each other. Due to performance reasons, it is recommended to use close data nodes for any operation. So Rack Awareness is an important concept for high performance and fault- tolerance in Hadoop. If we set Replication factor 3 for a file, does it mean any computation will also take place 3 times?\" No. Replication factor of 3 means that there are 3 copies of a file. But computation takes place only one copy of the file. If the node on which the first copy exists does not respond then computation will be done on the second copy.</p> <p>How will you check if a file exists in HDFS?</p> <p>In Hadoop, we can run hadoop fs command with option e to check the existence of a file in HDFS. This is generally used for testing purposes. Command will be as follows: + %&gt;hadoop fs -test -ezd file_uri e is for checking the existence of file z is for checking non-zero size of File d is for checking if the path is directory</p> <p>Why do we use the fsck command in HDFS?</p> <p>fsck command is used for getting the details of files and directories in HDFS. Main uses of fsck command in HDFS are as follows: delete: We use this option to delete files in HDFS.</p> <p>move: This option is for moving corrupt files to lost/found.</p> <p>locations: This option prints all the locations of a block in HDFS.</p> <p>racks: This option gives the network topology of data-node locations.</p> <p>blocks: This option gives the report of blocks in HDFS.</p> <p>How does partitioning work in Hadoop?</p> <p>Partitioning is the phase between Map phase and Reduce phase in Hadoop workflow. Since the partitioner gives output to Reducer, the number of partitions is the same as the number of Reducers. Partitioner will partition the output from Map phase into distinct partitions by using a user-defined condition. Partitions can be like Hash based buckets. E.g. If we have to find the student with the maximum marks in each gender in each subject. We can first use the Map function to map the keys with each gender. Once mapping is done, the result is passed to the Partitioner. Partitioner will partition each row with gender on the basis of subject. For each subject there will be a different Reducer. Reducer will take input from each partition and find the student with the highest marks.</p> <p>Why does HDFS store data in Block structure?</p> <p>HDFS stores all the data in terms of Blocks. With Block structure there are some benefits that HDFS gets. Some of these are as follows: Fault Tolerance: With Block structure, HDFS implements replication. By replicating the same block in multiple locations, fault tolerance of the system increases. Even if some copy is not accessible, we can get the data from another copy. Large Files: We can store very large files that cannot be even stored on one disk, in HDFS by using Block structure. We just divide the data of the file in multiple Blocks. Each Block can be stored on the same or different machines. Storage management: With Block storage it is easier for Hadoop nodes to calculate the data storage as well as perform optimization in the algorithm to minimise data transfer across the network.</p> <p>How will you create a custom Partitioner in a Hadoop job?</p> <p>Partition phase runs between Map and Reduce phase. It is an optional phase. We can create a custom partitioner by extending the org.apache.hadoop.mapreduce.Partitio class in Hadoop. In this class, we have to override the getPartition(KEY key, VALUE value, int numPartitions) method. This method takes three inputs. In this method, numPartitions is the same as the number of reducers in our job. We pass key and value to get the partition number to which this key,value record will be assigned. There will be a reducer corresponding to that partition. The reducer will further handle summarizing of the data.Once the custom Partitioner class is ready, we have to set it in the Hadoop job. We can use following method to set it: job.setPartitionerClass(CustomPartitioner)</p> <p>What is a Checkpoint node in HDFS?</p> <p>A Checkpoint node in HDFS periodically fetches fsimage and edits from NameNode, and merges them. This merge result is called a Checkpoint. Once a Checkpoint is created, Checkpoint Node uploads the Checkpoint to NameNode. Secondary nodes also take a Checkpoint similar to Checkpoint Node. But it does not upload the Checkpoint to NameNode. Main benefit of Checkpoint Node is in case of any failure on NameNode. A NameNode does not merge its edits to fsimage automatically during the runtime. If we have a long running task, the edits will become huge. When we restart NameNode, it will take much longer time, because it will first merge the edits. In such a scenario, a Checkpoint node helps for a long running task. Checkpoint nodes performs the task of merging the edits with fsimage and then uploads these to NameNode. This saves time during the restart of NameNode.</p> <p>What is a Backup Node in HDFS?</p> <p>Backup Node in HDFS is similar to Checkpoint Node. It takes the stream of edits from NameNode. It keeps these edits in memory and also writes these to storage to create a new checkpoint. At any point of time, Backup Node is in sync with the Name Node. The difference between Checkpoint Node and Backup Node is that Backup Node does not upload any checkpoints to Name Node. Also Backup node takes a stream instead of periodic reading of edits from Name Node.</p> <p>What is the meaning of the term Data Locality in Hadoop?</p> <p>In a Big Data system, the size of data is huge. So it does not make sense to move data across the network. In such a scenario, Hadoop tries to move computation closer to data. So the Data remains local to the location wherever it was stored. But the computation tasks will be moved to data nodes that hold the data locally. Hadoop follows following rules for Data Locality optimization: Hadoop first tries to schedule the task on a node that has an HDFS file on a local disk. If it cannot be done, then Hadoop will try to schedule the task on a node on the same rack as the node that has data. If this also cannot be done, Hadoop will schedule the task on the node with the same data on a different rack. The above method works well, when we work with the default replication factor of 3 in Hadoop.</p> <p>What is a Balancer in HDFS?</p> <p>In HDFS, data is stored in blocks on a DataNode. There can be a situation when data is not uniformly spread into blocks on a DataNode. When we add a new DataNode to a cluster, we can face such a situation. In such a case, HDFS provides a useful tool Balancer to analyze the placement of blocks on a DataNode. Some people call it a Rebalancer also. This is an administrative tool used by admin staff. We can use this tool to spread the blocks in a uniform manner on a DataNode.</p> <p>What are the important points a NameNode considers before selecting the DataNode for placing a data block?</p> <p>Some of the important points for selecting a DataNode by NameNode are as follows: NameNode tries to keep at least one replica of a Block on the same node that is writing the block. It tries to spread the different replicas of the same block on different racks, so that in case of one rack failure, another rack has the data. One replica will be kept on a node on the same node as the one that is writing it. It is different from point 1. In Point 1, a block is written to the same node. At this point the block is written on a different node on the same rack. This is important for minimizing the network I/O. NameNode also tries to spread the blocks uniformly among all the DataNodes in a cluster.</p> <p>What is Safemode in HDFS?</p> <p>Safemode is considered as the read-only mode of NameNode in a cluster. During the startup of NameNode, it was in SafeMode. It does not allow writing to the file-system in Safemode. At this time, it collects data and statistics from all the DataNodes. Once it has all the data on blocks, it leaves Safemode. The main reason for Safemode is to avoid the situation when NameNode starts replicating data in DataNodes before collecting all the information from DataNodes. It may erroneously assume that a block is not replicated well enough, whereas, the issue is that NameNode does not know about the whereabouts of all the replicas of a block. Therefore, in Safemode, NameNode first collects the information about how many replicas exist in a cluster and then tries to create replicas wherever the number of replicas is less than the policy.</p> <p>How will you replace HDFS data volume before shutting down a DataNode?</p> <p>In HDFS, DataNode supports hot swappable drives. With a swappable drive we can add or replace HDFS data volumes while the DataNode is still running. The procedure for replacing a hot swappable drive is as follows: First we format and mount the new drive. We update the DataNode configuration dfs.datanode.data.dir to reflect the data volume directories. Run the \"dfsadmin -reconfig datanode HOST:PORT start\" command to start the reconfiguration process Once the reconfiguration is complete, we just unmount the old data volume After unmount we can physically remove the old disks.</p> <p>Why do we need Serialization in Hadoop map reduce methods?</p> <p>In Hadoop, there are multiple data nodes that hold data. During the processing of map and reduce methods data may transfer from one node to another node. Hadoop uses serialization to convert the data from Object structure to Binary format. With serialization, data can be converted to binary format and with de-serialization data can be converted back to Object format with reliability.</p> <p>What is the use of Distributed Cache in Hadoop?</p> <p>Hadoop provides a utility called Distributed Cache to improve the performance of jobs by caching the files used by applications. An application can specify which file it wants to cache by using JobConf configuration. Hadoop framework copies these files to the nodes one which a task has to be executed. This is done before the start of execution of a task. DistributedCache supports distribution of simple read only text files as well as complex files like jars, zips etc.</p> <p>What are the important steps when you are partitioning a table?</p> <p>Don\u2019t over partition the data with too small partitions, it\u2019s overhead to the namenode. if dynamic partition, at least one static partition should exist and set to strict mode by using given commands. SET hive.exec.dynamic.partition = true; SET hive.exec.dynamic.partition.mode = nonstrict;</p> <p>first load data into nonpartitioned table, then load such data into a partitioned table. It\u2019s not possible to load data from local to partitioned tables. insert overwrite table table_name partition(year) select * from nonpartitiontable;</p> <p>What is the difference between block And split?</p> <p>Block: How much chunk data is stored in the memory called block. Split: how much data to process the data called split.</p> <p>Why Hadoop Framework reads a file parallel, why not sequential?</p> <p>To retrieve data faster, Hadoop reads data parallel, the main reason it can access data faster. While, writes in sequence, but not parallel, the main reason it might result is that one node can be overwritten by another and where the second node. Parallel processing is independent, so there is no relation between two nodes. If you write data in parallel, it\u2019s not possible where the next chunk of data has. For example 100 MB data write parallel, 64 MB one block another block 36, if data writes parallel the first block doesn\u2019t know where the remaining data is. So Hadoop reads parallel and writes sequentially.</p> <p>If I change block size from 64 to 128?</p> <p>Even if you have changed block size, it does not affect existing data. After changing the block size, every file chunked after 128 MB of block size. It means old data is in 64 MB chunks, but new data stored in 128 MB blocks.</p> <p>How much Hadoop allows maximum block size and minimum block size?</p> <p>Minimum: 512 bytes. It\u2019s local OS file system block size. No one can decrease fewer than block size. Maximum: Depends on the environment. There is no upper bound.</p> <p>What is speculative execution?</p> <p>Hadoop runs the process in commodity hardware, so it\u2019s possible to fail if the system also has low memory. So if system failed, process also failed, it\u2019s not recommendable.Speculative execution is a process performance optimization technique.Computation/logic distribute to the multiple systems and execute which system execute quickly. By default this value is true. Now even if the system crashes, not a problem, the framework chooses logic from other systems. Eg: logic distributed on A, B, C, D systems, completed within a time.</p> <p>System A, System B, System C, System D systems executed 10 min, 8 mins, 9 mins 12 mins simultaneously. So consider system B and kill remaining system processes, framework take care to kill the other system process.</p> <p>What are the setup and clean up methods?</p> <p>If you don\u2019t know the starting and ending points/lines, it\u2019s much more difficult to solve those problems. Setup and clean up can resolve it. N number of blocks, by default 1 mapper called to each split. Each split has one start and clean up methods. N number of methods, number of lines. Setup is initialising job resources. The purpose of cleaning up is to close the job resources. The map processes the data. Once the last map is completed, cleanup is initialized. It Improves the data transfer performance. All these block size comparisons can be done in reducer as well. If you have any key and value, compare one key value to another key value. If you compare record levels, use these setup and cleanup. It opens once and processes many times and closes once. So it saves a lot of network wastage during the process.</p> <p>How does a NameNode handle the failure of the Data Nodes?</p> <p>HDFS has master/slave architecture. An HDFS cluster consists of a single NameNode, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of DataNodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on. The NameNode and DataNode are pieces of software designed to run on commodity machines. NameNode periodically receives a Heartbeat and a Block report from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode. When NameNode notices that it has not received a heartbeat message from a data node after a certain amount of time, the data node is marked as dead. Since blocks will be under replication the system begins replicating the blocks that were stored on the dead DataNode. The NameNode Orchestrates the replication of data blocks from one DataNode to another. The replication data transfer happens directly between DataNode and the data never passes through the NameNode.</p> <p>How is HDFS different from traditional File Systems?</p> <p>HDFS, the Hadoop Distributed File System, is responsible for storing huge data on the cluster. This is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS is designed to support very large files. Applications that are compatible with HDFS are those that deal with large data sets. These applications write their data only once but they read it one or more times and re !!!- info \"Quire these reads to be satisfied at streaming speeds. HDFS supports write-once-read-many semantics on files.</p> <p>What is Hdfs block size and how is it different from Traditional File System block size?</p> <p>In HDFS data is split into blocks and distributed across multiple nodes in the cluster. Each block is typically 64Mb or 128Mb in size. Each block is replicated multiple times. Default is to replicate each block three times. Replicas are stored on different nodes. HDFS utilizes the local file system to store each HDFS block as a separate file. HDFS Block size can not be compared with the traditional file system block size.</p> <p>What is a NameNode and how many instances of NameNode run on a Hadoop Cluster?</p> <p>The NameNode is the centrepiece of an HDFS file system. It keeps the directory tree of all files in the file system, and tracks where across the cluster the file data is kept. It does not store the data of these files itself. There is only One NameNode process run on any hadoop cluster. NameNode runs on its own JVM process. In a typical production cluster it runs on a separate machine. The NameNode is a Single Point of Failure for the HDFS Cluster. When the NameNode goes down, the file system goes offline. Client applications talk to the NameNode whenever they wish to locate a file, or when they want to add /copy /move /delete a file. The NameNode responds to successful requests by returning a list of relevant DataNode servers where the data lives.</p> <p>How does the client communicate with Hdfs?</p> <p>The Client communication to HDFS happens using Hadoop HDFS API. Client applications talk to the NameNode whenever they wish to locate a file, or when they want to add/copy/move/delete a file on HDFS. The NameNode responds the successful requests by returning a list of relevant DataNode servers where the data lives. Client applications can talk directly to a DataNode, once the NameNode has provided the location of the data.</p> <p>How the Hdfs blocks are replicated?</p> <p>HDFS is designed to reliably store very large files across machines in a large cluster. It stores each file as a sequence of blocks; all blocks in a file except the last block are the same size. The blocks of a file are replicated for fault tolerance. The block size and replication factor are configurable per file. An application can specify the number of replicas of a file. The replication factor can be specified at file creation time and can be changed later. Files in HDFS are write-once and have strictly one writer at any time. The NameNode makes all decisions regarding replication of blocks. HDFS uses a rack-aware replica placement policy. In default configuration there are a total 3 copies of a data block on HDFS, 2 copies are stored on datanodes on the same rack and 3rd copy on a different rack.</p> <p>Can you give some examples of Big Data?</p> <p>There are many real life examples of Big Data! Facebook is generating 500+ terabytes of data per day, NYSE (New York Stock Exchange) generates about 1 terabyte of new trade data per day, a jet airline collects 10 terabytes of censor data for every 30 minutes of flying time. All these are day to day examples of Big Data!</p> <p>What is structured and unstructured Data?</p> <p>Structured data is the data that is easily identifiable as it is organized in a structure. The most common form of structured data is a database where specific information is stored in tables, that is, rows and columns. Unstructured data refers to any data that cannot be identified easily. It could be in the form of images, videos, documents, email, logs and random text. It is not in the form of rows and columns.</p> <p>Since the data is replicated thrice in Hdfs, does it mean that any calculation done on One Node will also be replicated on the other Two?</p> <p>Since there are 3 nodes, when we send the MapReduce programs, calculations will be done only on the original data. The master node will know which node exactly has that particular data. In case, if one of the nodes is not responding, it is assumed to be failed. Only then, the re !!!- info \"Quired calculation will be done on the second replica.</p> <p>What is throughput and how does Hdfs get a good throughput?</p> <p>Throughput is the amount of work done in a unit time. It describes how fast the data is getting accessed from the system and it is usually used to measure performance of the system. In HDFS, when we want to perform a task or an action, then the work is divided and shared among different systems. So all the systems will be executing the tasks assigned to them independently and in parallel. So the work will be completed in a very short period of time. In this way, the HDFS gives good throughput. By reading data in parallel, we decrease the actual time to read data tremendously.</p> <p>What is streaming access?</p> <p>As HDFS works on the principle of \u2018Write Once, Read Many\u2018, the feature of streaming access is extremely important in HDFS. HDFS focuses not so much on storing the data but how to retrieve it at the fastest possible speed, especially while analyzing logs. In HDFS, reading the complete data is more important than the time taken to fetch a single record from the data.</p> <p>How indexing is done in Hdfs?</p> <p>Hadoop has its own way of indexing. Depending upon the block size, once the data is stored, HDFS will keep on storing the last part of the data which will say where the next part of the data will be. In fact, this is the base of HDFS.</p> <p>What is a Rack?</p> <p>Rack is a storage area with all the datanodes put together. These data nodes can be physically located at different places. Rack is a physical collection of datanodes which are stored at a single location. There can be multiple racks in a single location.</p> <p>On what basis Data will be stored on a Rack?</p> <p>When the client is ready to load a file into the cluster, the content of the file will be divided into blocks. Now the client consults the Namenode and gets 3 datanodes for every block of the file which indicates where the block should be stored. While placing the datanodes, the key rule followed is \u201cfor every block of data, two copies will exist in one rack, and a third copy in a different rack\u201c. This rule is known as \u201cReplica Placement Policy\u201c.</p> <p>Do we need to place 2nd and 3rd Data in Rack 2 only?</p> <p>Yes, this is to avoid datanode failure.</p> <p>What if Rack 2 and DataNode fails?</p> <p>If both rack2 and datanode present in rack 1 fails then there is no chance of getting data from it. In order to avoid such situations, we need to replicate that data more number of times instead of replicating only thrice. This can be done by changing the value in the replication factor which is set to 3 by default.</p> <p>What is the difference between Gen1 and Gen2 Hadoop with regards to the NameNode?</p> <p>In Gen 1 Hadoop, Namenode is the single point of failure. In Gen 2 Hadoop, we have what is known as Active and Passive Namenodes kind of a structure. If the active Namenode fails, the passive Namenode takes over the charge.</p> <p>Which are the two types of writes In Hdfs?</p> <p>There are two types of writes in HDFS: Posted and non-posted write. Posted Write is when we write it and forget about it, without worrying about the acknowledgement.</p> <p>It is similar to our traditional Indian post.</p> <p>Non-posted Write, we wait for the acknowledgement. It is similar to today's courier services. Naturally, non-posted write is more expensive than the posted write. It is much more expensive, though both writes are asynchronous.</p> <p>Why reading is done in parallel and writing is not in Hdfs?</p> <p>Reading is done in parallel because by doing so we can access the data fast. But we do not perform the write operation in parallel. The reason is that if we perform the write operation in parallel, then it might result in data inconsistency. For example, you have a file and two nodes are trying to write data into the file in parallel, then the first node does not know what the second node has written and vice-versa. So, this makes it confusing which data to be stored and accessed.</p> <p>What is the configuration of a typical Slave Node on a Hadoop Cluster and how many Jvms run on a Slave Node?</p> <p>Single instance of a Task Tracker is run on each Slave node. Task tracker is run as a separate JVM process. Single instance of a DataNode daemon is run on each Slave node. DataNode daemon is run as a separate JVM process. One or Multiple instances of Task Instance is run on each slave node. Each task instance is run as a separate JVM process. The number of Task instances can be controlled by configuration. Typically a high end machine is configured to run more task instances.</p> <p>Explain in brief the three Modes in which Hadoop can be run?</p> <p>The three modes in which Hadoop can be run are:</p> <p>Standalone (local) mode - No Hadoop daemons running, everything runs on a single Java Virtual machine only.</p> <p>Pseudo-distributed mode - Daemons run on the local machine, thereby simulating a cluster on a smaller scale.</p> <p>Fully distributed mode - Runs on a cluster of machines.</p> <p>Explain what are the features of Standalone local Mode?</p> <p>In stand-alone or local mode there are no Hadoop daemons running, and everything runs on a single Java process. Hence, we don't get the benefit of distributing the code across a cluster of machines. Since it has no DFS, it utilizes the local file system. This mode is suitable only for running MapReduce programs by developers during various stages of development. It's the best environment for learning and good for debugging purposes.</p> <p>What are the features of fully distributed mode?</p> <p>In Fully Distributed mode, the clusters range from a few nodes to 'n' number of nodes. It is used in production environments, where we have thousands of machines in the Hadoop cluster. The daemons of Hadoop run on these clusters. We have to configure separate masters and separate slaves in this distribution, the implementation of which is quite complex. In this configuration, Namenode and Datanode run on different hosts and there are nodes on which the task tracker runs. The root of the distribution is referred to as HADOOP_HOME.</p> <p>Explain what are the main features Of pseudo mode?</p> <p>In Pseudo-distributed mode, each Hadoop daemon runs in a separate Java process, as such it simulates a cluster though on a small scale. This mode is used both for development and QA environments. Here, we need to do the configuration changes.</p> <p>Which are the three main Hdfs site.xml properties?</p> <p>The three main hdfs-site.xml properties are:</p> <p>Dfs.name.dir which gives you the location on which metadata will be stored and where DFS is located \u2013 on disk or onto the remote.</p> <p>Dfs.data.dir which gives you the location where the data is going to be stored.</p> <p>Fs.checkpoint.dir which is for secondary Namenode.</p> <p>What does etc.init.d do?</p> <p>/etc /init.d specifies where daemons (services) are placed or to see the status of these daemons. It is very LINUX specific, and has nothing to do with Hadoop.</p> <p>What is the function Of Hadoop-env.sh and where is it present?</p> <p>This file contains some environment variable settings used by Hadoop; it provides the environment for Hadoop to run. The path of JAVA_HOME is set here for it to run properly. Hadoop-env.sh file is present in the conf/hadoop-env.sh location. You can also create your own custom configuration file conf/hadoop-user-env.sh, which will allow you to override the default Hadoop settings.</p>"},{"location":"hadoop/kerberos/","title":"Kerberos","text":"<p>Kerberos is a protocol designed to provide secure authentication to services over an insecure network. It ensures that passwords are never sent across the network and encryption keys are never directly exchanged. Furthermore, you and the application can mutually authenticate each other. Many organizations utilize Kerberos as the foundation for single sign-on capabilities. The name \"Kerberos\" originates from Greek mythology, specifically from Cerberus, the three-headed dog that guards the gates to the underworld, symbolizing its role in guarding access to applications.</p> <p>Kerberos was selected over other options like SSL certificates due to its better performance and simpler user management; for instance, removing a user in Kerberos involves a simple deletion, whereas revoking an SSL certificate is a more complicated process. </p> <p>A key benefit of Kerberos is that it eliminates the need for passwords to be transmitted across the network, thereby removing the potential threat of attackers \"sniffing\" or intercepting passwords.</p> <p>To understand Kerberos, several key terms are essential:</p> <ul> <li> <p>A Kerberos realm is the domain or group of systems where Kerberos has the authority to authenticate users to services. Multiple realms can exist and be interconnected.</p> </li> <li> <p>Database: This part stores user and service identities, which are known as principles. It also holds other information like encryption keys, ticket validity durations, and expiration dates.</p> </li> <li> <p>A principle is a unique identity, which can be either a user or a service (like an application).</p> </li> <li> <p>A client is a process that accesses a service on behalf of a user, essentially the user wanting to access something.</p> </li> <li> <p>A service is a resource provided to a client, such as a file server or an application.</p> </li> <li> <p>The Key Distribution Center (KDC) is the central component of Kerberos, responsible for supplying tickets and generating temporary session keys that enable secure user authentication to a service. The KDC stores all the secret symmetric keys for users and services. It comprises two main servers: the Authentication Server (AS) and the Ticket Granting Server (TGS).</p> </li> <li> <p>The Authentication Server (AS) confirms that a known user is making an access request and issues a Ticket Granting Ticket (TGT).</p> </li> <li> <p>The Ticket Granting Server (TGS) confirms that a user is requesting access to a known service and issues service tickets.</p> </li> <li> <p>Authenticators are records that contain information provably generated recently using a session key known only to the client and the server, facilitating mutual authentication.</p> </li> <li> <p>Tickets contain crucial information such as the client's identity, service ID, session keys, timestamps, and time to live, all encrypted with a server's secret key.</p> </li> </ul> <p>The high-level communication process for a user to access a service involves a series of messages exchanged between the user, the Authentication Server (AS), the Ticket Granting Server (TGS), and the service itself, with at least two messages sent at almost every step, some in plaintext and some encrypted with a symmetric key.</p>"},{"location":"hadoop/kerberos/#explaination-part-1","title":"Explaination part 1","text":"<p>Here's a detailed breakdown of the Kerberos authentication process:</p> <ul> <li> <p>Initial Request to Authentication Server (AS): </p> <p>The user initiates the process by sending an unencrypted message to the AS. This message includes the user's ID (e.g., Rob), the ID of the service they wish to access (e.g., a CRM application), the user's IP address (which can be single, multiple, or none, depending on configuration), and the requested lifetime for the TGT. While users might desire an infinite lifetime for convenience, security considerations often lead Kerberos to override such requests.</p> </li> <li> <p>AS Processing and Response:</p> <p>Upon receiving the user's message, the AS first verifies the user ID against its list of known users and their secret keys, retrieving the user's secret client key if found. The AS then generates two messages to send back to the user. The first message contains the ID of the TGS, the message creation timestamp, and its lifetime. The second message is the Ticket Granting Ticket (TGT), which includes the user's ID, the TGS's ID, a timestamp, the user's IP address, and the TGT's lifetime (which may differ from the user's request). The AS also generates a randomly generated symmetric TGS session key and adds it to both messages. The first message is encrypted with the user's secret key, and the TGT is encrypted with the TGS's secret key. These two encrypted messages are then sent to the user.</p> </li> <li> <p>User Processing AS Response: </p> <p>The user must decrypt the first message to proceed. This is done by the user generating their secret key, which involves entering their password. Kerberos adds a salt (typically the user's username at realm name) and a key version number (KDN) to the password. The salted password is then processed through a hashing algorithm (specifically, string-to-key) to generate the user's secret key. This key is used to decrypt the first message, and this decryption step also serves to validate the user's password; an incorrect password would result in a decryption failure. If successful, the user gains access to the TGS's ID and the TGS session key. Importantly, the user cannot decrypt the TGT because they do not possess the TGS's secret key.</p> </li> <li> <p>User Request to Ticket Granting Server (TGS):</p> <p>The user then prepares two new messages. The first is a simple plaintext message indicating the desired service and its requested ticket lifetime. The second is a user Authenticator, containing the user ID and a creation timestamp, encrypted with the TGS session key. These new messages, along with the still-encrypted TGT, are sent to the TGS.</p> </li> <li> <p>TGS Processing User Request:</p> <p>The TGS starts by examining the plaintext service ID and verifying it against its list of known services in the KDC, retrieving the service's secret key if found. The TGS then decrypts the TGT using its own secret key, which reveals the TGS session key. This session key is then used to decrypt the user Authenticator. With both the TGT and Authenticator decrypted, the TGS performs several validations: ensuring user IDs match between the TGT and Authenticator, comparing timestamps (tolerating up to a two-minute difference), comparing the TGT's IP address to the user's received IP address (if applicable), and checking if the TGT has expired. For replay protection, the TGS maintains a cache of recently received authenticators and checks if the current Authenticator is already in the cache; if not, it adds it.</p> </li> <li> <p>TGS Response to User: </p> <p>If all validations pass, the TGS creates two messages for the user. The first message includes the service ID, timestamp, and message lifetime. The second message is the service ticket, containing the user ID, the service ID, a timestamp, the user's IP address, and the service ticket's lifetime. The TGS generates a random symmetric service session key and adds it to both messages. The first message is encrypted with the TGS session key, and the service ticket is encrypted with the service secret key. These two messages are then sent to the user.</p> </li> <li> <p>User Processing TGS Response: </p> <p>The user decrypts the first message using the TGS session key (which they received from the AS), gaining access to the service session key. The user then creates a new Authenticator message with their ID and a timestamp, encrypting it with the service session key. The user cannot decrypt the service ticket because it's encrypted with the service's secret key, so they simply forward the encrypted service ticket along with the newly created Authenticator message to the service.</p> </li> <li> <p>Service Processing User Request: </p> <p>The service decrypts the service ticket using its own secret key, which reveals the service session key. This service session key is then used to decrypt the user Authenticator message. Similar to the TGS, the service performs validations: matching user IDs, comparing timestamps (tolerating less than two minutes difference), checking IP addresses, and verifying ticket expiration. For replay protection, the service also maintains a cache of recently received authenticators and adds the current one if it's new.</p> </li> <li> <p>Service Response and Mutual Authentication:</p> <p>If validations are successful, the service creates its own Authenticator message, including its service ID and a timestamp, encrypted with the service session key. This service Authenticator is sent back to the user. The user decrypts this message using the same symmetric service session key. The user then verifies that the service name in the Authenticator matches the expected service, completing the mutual authentication. The user also checks the timestamp to ensure the Authenticator was recently created. Finally, the user caches a copy of the encrypted service ticket for future use with that service, given the mutual authentication. This entire exchange securely distributes a symmetric service session key that allows the user and service to communicate authentication information securely.</p> </li> </ul>"},{"location":"hadoop/kerberos/#explaination-part-2","title":"Explaination part 2","text":"<p>Here's a detailed breakdown of how Kerberos authenticates a user attempting to access a service in a Hadoop cluster, such as listing a directory from HDFS:</p> <ul> <li> <p>Initial Authentication (User to KDC - AS):</p> <p>The process begins on a Linux machine where the user executes the <code>kinit</code> tool. The <code>kinit</code> program prompts for the user's password and then sends an authentication request to the Kerberos Authentication Server (AS). Upon successful authentication, the AS responds by providing a Ticket Granting Ticket (TGT). The <code>kinit</code> tool then stores this TGT in the user's credentials cache. At this point, the user has been authenticated and is ready to execute a Hadoop command.</p> </li> <li> <p>Requesting a Service Ticket (Hadoop Client to KDC - TGS): </p> <p>When a user runs a Hadoop command (e.g., <code>hadoop fs -ls /</code>), the Hadoop client uses the cached TGT to contact the Ticket Granting Server (TGS). The client approaches the TGS to request a service ticket for the specific Hadoop service it intends to access, such as the NameNode service. The TGS grants the requested service ticket, and the Hadoop client caches it.</p> </li> <li> <p>Accessing the Service (Hadoop Client to Service): </p> <p>With the service ticket in hand, the Hadoop client can now communicate with the target service (e.g., the NameNode). The Hadoop RPC (Remote Procedure Call) mechanism uses this service ticket to reach out to the NameNode. A mutual exchange of tickets occurs between the client and the NameNode. The client's service ticket proves its identity, and the NameNode's ticket confirms its own identity, ensuring that both parties are certain they are communicating with an authenticated entity. This two-way verification is known as mutual authentication.</p> </li> <li> <p>Authorization: </p> <p>After authentication is complete, the system proceeds to authorization. This is a separate step where the NameNode checks if the authenticated user has the necessary permissions to perform the requested action, such as listing the root directory. If permissions are granted, the NameNode returns the results.</p> </li> </ul>"},{"location":"hadoop/mapreduce/","title":"MapReduce","text":""},{"location":"hadoop/mapreduce/#mapreduce","title":"MapReduce","text":"<p>MapReduce is a programming framework that allows us to perform distributed and parallel processing on large data sets in a distributed environment.</p> <p>MapReduce consists of two distinct tasks \u2014 Map and Reduce.</p> <p>As the name MapReduce suggests, reducer phase takes place after the mapper phase has been completed.</p> <p>So, the first is the map job, where a block of data is read and processed to produce key-value pairs as intermediate outputs.</p> <p>The output of a Mapper or map job (key-value pairs) is input to the Reducer.</p> <p>The reducer receives the key-value pair from multiple map jobs.</p> <p>Then, the reducer aggregates those intermediate data tuples (intermediate key-value pair) into a smaller set of tuples or key-value pairs which is the final output.</p>"},{"location":"hadoop/mapreduce/#advantages-of-mapreduce","title":"Advantages of MapReduce","text":"<ul> <li> <p>Parallel Processing: In MapReduce, we are dividing the job among multiple nodes and each node works with a part of the job simultaneously. So, MapReduce is based on Divide and Conquer paradigm which helps us to process the data using different machines very quickly.</p> </li> <li> <p>Data Locality: Instead of moving data to the processing unit, we are moving the processing unit to the data in the MapReduce Framework.  In the traditional system, we used to bring data to the processing unit and process it. But, as the data grew and became very huge, bringing this huge amount of data to the processing unit posed the following issues:</p> </li> </ul> <p>Moving huge data to processing is costly and deteriorates the network performance. Processing takes time as the data is processed by a single unit which becomes the bottleneck. The master node can get over-burdened and may fail.</p> <p>Now, MapReduce allows us to overcome the above issues by bringing the processing unit to the data. This allows us to have the following advantages:</p> <ol> <li> <p>It is very cost-effective to move processing unit to the data.</p> </li> <li> <p>The processing time is reduced as all the nodes are working with their part of the data in parallel.</p> </li> <li> <p>Every node gets a part of the data to process and therefore, there is no chance of a node getting overburdened.</p> </li> </ol>"},{"location":"hadoop/mapreduce/#mapreduce-data-flow","title":"MapReduce Data Flow","text":"<ul> <li> <p>Input Files: The data for a MapReduce task is stored in input files, and input files typically lives in HDFS.</p> </li> <li> <p>InputFormat: Now, InputFormat defines how these input files are split and read. It selects the files or other objects that are used for input.</p> </li> <li> <p>InputSplits: It is created by InputFormat, logically represent the data which will be processed by an individual Mapper. One map task is created for each split; thus the number of map tasks will be equal to the number of InputSplits.</p> </li> <li> <p>RecordReader: It communicates with the InputSplit in Hadoop MapReduce and converts the data into key-value pairs suitable for reading by the mapper.</p> </li> <li> <p>Mapper: It processes each input record (from RecordReader) and generates new key-value pair, and this key-value pair generated by Mapper is completely different from the input pair. The output of Mapper is also known as intermediate output which is written to the local disk. The output of the Mapper is not stored on HDFS as this is temporary data and writing on HDFS will create unnecessary copies.</p> </li> <li> <p>Combiner: The combiner is also known as \u2018Mini-reducer\u2019. Hadoop MapReduce Combiner performs local aggregation on the mappers\u2019 output, which helps to minimize the data transfer between mapper and reducer.</p> </li> <li> <p>Partitioner: Hadoop MapReduce, Partitioner comes into the picture if we are working on more than one reducer (for one reducer partitioner is not used). Partitioner takes the output from combiners and performs partitioning. Partitioning of output takes place on the basis of the key and then sorted. By hash function, key (or a subset of the key) is used to derive the partition. According to the key value in MapReduce, each combiner output is partitioned, and a record having the same key value goes into the same partition, and then each partition is sent to a reducer.</p> </li> <li> <p>Shuffling and Sorting: Now, the output is Shuffled to the reduce node (which is a normal slave node but reduce phase will run here hence called as reducer node). The shuffling is the physical movement of the data which is done over the network. Once all the mappers are finished and their output is shuffled on the reducer nodes, then this intermediate output is merged and sorted, which is then provided as input to reduce phase.</p> </li> <li> <p>Reducer: It takes the set of intermediate key-value pairs produced by the mappers as the input and then runs a reducer function on each of them to generate the output. The output of the reducer is the final output, which is stored in HDFS.</p> </li> <li> <p>RecordWriter: It writes these output key-value pair from the Reducer phase to the output files.</p> </li> <li> <p>OutputFormat: The way these output key-value pairs are written in output files by RecordWriter is determined by the OutputFormat.</p> </li> </ul>"},{"location":"hadoop/yarn/","title":"YARN","text":"<p>Apache Hadoop YARN (Yet Another Resource Negotiator) is a resource management layer in Hadoop. YARN came into the picture with the introduction of Hadoop 2.x. It allows various data processing engines such as interactive processing, graph processing, batch processing, and stream processing to run and process data stored in HDFS (Hadoop Distributed File System).</p> <p>In essence, YARN is responsible for managing cluster resources, which include CPU, memory, disk I/O, and network bandwidth. It provides APIs that computation engines use to request and work with Hadoop cluster resources for their task scheduling and resource requirements. It's important to note that YARN APIs are not designed for individual application developers; instead, they target teams creating new computation engines. </p> <p>The ambition of YARN is to consolidate all types of distributed computation capabilities into a single cluster, thereby eliminating the need for multiple clusters and the associated pain of data movement or duplication. Popular execution engines that operate on top of YARN include Apache Spark, Apache Storm, Apache Solr, and Apache Tez. While some NoSQL databases like Cassandra are not yet YARN-enabled, an incubating Apache project called Slider aims to integrate them into a YARN-managed Hadoop cluster, including existing implementations for HBase and Accumulo, without requiring changes to those systems themselves. Apache Slider also seeks to introduce on-demand scale-up and scale-down capabilities, offering elasticity similar to cloud providers.</p>"},{"location":"hadoop/yarn/#components-of-yarn","title":"Components of YARN","text":"<ul> <li> <p>Resource Manager </p> <p>Resource Manager is the master daemon of YARN. It is responsible for managing several other applications, along with the global assignments of resources such as CPU and memory. It is used for job scheduling.</p> <p>Resource Manager has two components:</p> <pre><code>1. Scheduler: Schedulers\u2019 task is to distribute resources to the running applications. It only deals with the scheduling of tasks and hence it performs no tracking and no monitoring of applications.\n\n2. Application Manager: The application Manager manages applications running in the cluster. Tasks, such as the starting of Application Master or monitoring, are done by the Application Manager.\n</code></pre> </li> </ul>"},{"location":"hadoop/yarn/#node-manager","title":"Node Manager","text":"<p>Node Manager is the slave daemon of YARN. </p> <p>It has the following responsibilities:</p> <ul> <li> <p>Node Manager has to monitor the container\u2019s resource usage, along with reporting it to the Resource Manager.</p> </li> <li> <p>The health of the node on which YARN is running is tracked by the Node Manager.</p> </li> <li> <p>It takes care of each node in the cluster while managing the workflow, along with user jobs on a particular node.</p> </li> <li> <p>It keeps the data in the Resource Manager updated</p> </li> <li> <p>Node Manager can also destroy or kill the container if it gets an order from the Resource Manager to do so.</p> </li> </ul>"},{"location":"hadoop/yarn/#application-master","title":"Application Master","text":"<p>Every job submitted to the framework is an application, and every application has a specific Application Master associated with it. </p> <p>Application Master performs the following tasks:</p> <ul> <li> <p>It coordinates the execution of the application in the cluster, along with managing the faults.</p> </li> <li> <p>It negotiates resources from the Resource Manager.</p> </li> <li> <p>It works with the Node Manager for executing and monitoring other components\u2019 tasks.</p> </li> <li> <p>At regular intervals, heartbeats are sent to the Resource Manager for checking its health, along with updating records according to its resource demands.</p> </li> </ul> <p>Now, we will step forward with the fourth component of Apache Hadoop YARN.</p>"},{"location":"hadoop/yarn/#container","title":"Container","text":"<p>A container is a set of physical resources (CPU cores, RAM, disks, etc.) on a single node.</p> <p>The tasks of a container are listed below:</p> <ul> <li>It grants the right to an application to use a specific amount of resources (memory, CPU, etc.) on a specific host.</li> <li>YARN containers are particularly managed by a Container Launch context which is Container Life Cycle(CLC).This record contains a map of environment variables, dependencies stored in remotely accessible storage, security tokens, the payload for Node Manager services, and the command necessary to create the process.</li> </ul>"},{"location":"hadoop/yarn/#running-and-application-through-yarn","title":"Running and application through YARN","text":"<p>When an application is submitted to YARN, the request goes to the Resource Manager, which then instructs a Node Manager to launch the first container for that application, known as the Application Master. The Application Master then assumes responsibility for executing and monitoring the entire job, with its specific functionality varying depending on the application framework (e.g., MapReduce Application Master functions differently than a Spark Application Master).</p> <p>For a MapReduce application, the Application Master requests more containers from the Resource Manager to initiate map and reduce tasks. Once these containers are allocated, the Application Master directs the Node Managers to launch the containers and execute the tasks. Tasks directly report their status and progress back to the Application Master. Upon completion of all tasks, all containers, including the Application Master, perform necessary cleanup and terminate.</p> <ul> <li> <p>Application Submission: The RM accepts the application, causing the creation of an ApplicationMaster (AM) instance. The AM is responsible for negotiating resources from the RM and working with the Node Managers (NMs) to execute and monitor the tasks.</p> </li> <li> <p>Resource Request: The AM starts by requesting resources from the RM. It specifies what resources are needed, in which locations, and other constraints. These resources are encapsulated in terms of \"Resource Containers\" which include specifications like memory size, CPU cores, etc.</p> </li> <li> <p>Resource Allocation: The Scheduler in the RM, based on the current system load and capacity, as well as policies (e.g., capacity, fairness), allocates resources to the applications by granting containers. The specific strategy depends on the scheduler type (e.g., FIFO, Capacity Scheduler).</p> </li> <li> <p>Container Launching: Post-allocation, the RM communicates with relevant NMs to launch the containers. The Node Manager sets up the container's environment, then starts the container by executing the specified commands.</p> </li> <li> <p>Task Execution: Each container then runs the task assigned by the ApplicationMaster. These are actual data processing tasks, specific to the application's purpose.</p> </li> <li> <p>Monitoring and Fault Tolerance: The AM monitors the progress of each task. If a container fails, the AM requests a new container from the RM and retries the task, ensuring fault tolerance in the execution phase.</p> </li> <li> <p>Completion and Release of Resources: Upon task completion, the AM releases the allocated containers, freeing up resources. After all tasks are complete, the AM itself is terminated, and its resources are also released.</p> </li> <li> <p>Finalization: The client then polls the RM or receives a notification to know the status of the application. Once informed of the completion, the client retrieves the result and finishes the process.</p> </li> </ul>"},{"location":"hive/overview/","title":"Hive","text":"<p>Apache Hive is an open source data warehouse system built on top of Hadoop HA used for querying and analyzing large datasets stored in Hadoop files.</p> <p>Initially, you have to write complex Map-Reduce jobs, but now with the help of the Hive, you just need to submit merely SQL queries. Hive is mainly targeted towards users who are comfortable with SQL. Hive use language called HiveQL (HQL), which is similar to SQL. HiveQL automatically translates SQL-like queries into MapReduce jobs.</p> <p>Hive abstracts the complexity of Hadoop. The main thing to notice is that there is no need to learn java for Hive. The Hive generally runs on your workstation and converts your SQL query into a series of jobs for execution on a Hadoop cluster. Apache Hive organizes data into tables. This provides a means for attaching the structure to data stored in HDFS.</p>"},{"location":"hive/overview/#hive-architecture","title":"Hive Architecture","text":"<p>The major components of Apache Hive are:</p> <p>--- Hive Client</p> <ul> <li> <p>Thrift Client: The Hive server is based on Apache Thrift so that it can serve the request from a thrift client.</p> </li> <li> <p>JDBC Client: Hive allows for the Java applications to connect to it using the JDBC driver. JDBC driver uses Thrift to communicate with the Hive Server.</p> </li> <li> <p>ODBC Client: Hive ODBC driver allows applications based on the ODBC protocol to connect to Hive. Similar to the JDBC driver, the ODBC driver uses Thrift to communicate with the Hive Server.</p> </li> </ul>"},{"location":"hive/overview/#hive-services","title":"Hive Services","text":"<ul> <li> <p>Beeline: The Beeline is a command shell supported by HiveServer2, where the user can submit its queries and command to the system. It is a JDBC client that is based on SQLLINE CLI (pure Java-console-based utility for connecting with relational databases and executing SQL queries).</p> </li> <li> <p>Hive Server 2 : HiveServer2 is the successor of HiveServer1. HiveServer2 enables clients to execute queries against the Hive. It allows multiple clients to submit requests to Hive and retrieve the final results. It is basically designed to provide the best support for open API clients like JDBC and ODBC. Note: Hive server1, also called a Thrift server, is built on Apache Thrift protocol to handle the cross-platform communication with Hive. It allows different client applications to submit requests to Hive and retrieve the final results. It does not handle concurrent requests from more than one client due to which it was replaced by HiveServer2.</p> </li> <li> <p>Hive Driver: The Hive driver receives the HiveQL statements submitted by the user through the command shell. It creates the session handles for the query and sends the query to the compiler.</p> </li> <li> <p>Hive Compiler: Hive compiler parses the query. It performs semantic analysis and type-checking on the different query blocks and query expressions by using the metadata stored in metastore and generates an execution plan. The execution plan created by the compiler is the DAG(Directed Acyclic Graph), where each stage is a map/reduce job, operation on HDFS, a metadata operation.</p> </li> <li> <p>Optimizer: Optimizer performs the transformation operations on the execution plan and splits the task to improve efficiency and scalability.</p> </li> <li> <p>Execution Engine: Execution engine, after the compilation and optimization steps, executes the execution plan created by the compiler in order of their dependencies using Hadoop.</p> </li> <li> <p>Metastore: Metastore is a central repository that stores the metadata information about the structure of tables and partitions, including column and column type information. It also stores information of serializer and deserializer, required for the read/write operation, and HDFS files where data is stored. This metastore is generally a relational database. Metastore provides a Thrift interface for querying and manipulating Hive metadata.</p> <p>We can configure metastore in any of the two modes:</p> <ol> <li> <p>Remote: In remote mode, metastore is a Thrift service and is useful for non-Java applications.</p> </li> <li> <p>Embedded: In embedded mode, the client can directly interact with the metastore using JDBC.</p> </li> </ol> </li> </ul>"},{"location":"hive/overview/#hive-query-flow","title":"Hive Query Flow","text":"<ul> <li> <p>execututeQuery: Command Line or Web UI sends the query to the Driver (any Hive interface like database driver JDBC, ODBC, etc.) to execute the query.</p> </li> <li> <p>getPlan: The driver takes the help of the query compiler which parses the query to check the syntax and the query plan or the requirement of the query.</p> </li> <li> <p>getMetaData: The compiler sends a metadata request to the Metastore (any database).</p> </li> <li> <p>sendMetaData: Metastore sends the metadata to the compiler in response.</p> </li> <li> <p>sendPlan: The compiler checks the requirement and resends the plan to the driver. The parsing and compiling of a query is complete.</p> </li> <li> <p>executePlan: The driver sends the executing plan to the execution engine.</p> <ol> <li> <p>metaDataOps (On Hive): Meanwhile in execution, the execution engine can execute metadata operations with Metastore.</p> </li> <li> <p>executeJob (on Hadoop): Internally, the process of execution job is a MapReduce job. The execution engine sends the job to JobTracker, which is in the Name node and it assigns this job to TaskTracker, which is in the Data node. Here, the query executes MapReduce job.</p> </li> <li> <p>job done: After the map-reduce process in Hadoop is finished, it sends a message that the process is finished here</p> </li> <li> <p>dfsOperations: DFS operations using the client\u2019s reported user and group permissions and worked between Execution Engine and NameNode</p> </li> </ol> </li> <li> <p>fetchResults: The driver sends the results to Hive Interfaces.</p> </li> <li> <p>sendResults: The execution engine sends those resultant values to the driver.</p> </li> <li> <p>fetchResults: The driver sends the results to HDFS.</p> </li> </ul>"},{"location":"hive/overview/#data-storage","title":"Data Storage","text":"<p>--- Plain Text Storage</p> <p>In plain text data storage, data is stored as readable characters. For example, consider the number 12345. In plain text, it's stored as the characters '1', '2', '3', '4', and '5'. Each character typically uses 1 byte of memory (in ASCII), or 2 bytes (in UTF-16), so this number would use 5 to 10 bytes of memory. The advantage of plain text storage is that it's human-readable and easy to interpret without any conversion. The disadvantage is that it's not space-efficient. Larger numbers or data types other than integers (like floating-point numbers) will use more space.</p> <p>--- Byte (Binary) Data Storage</p> <p>In byte (or binary) data storage, data is stored as binary values, not as readable characters. Each byte consists of 8 bits, and each bit can be either 0 or 1. Using our previous example, the number 12345 can be represented in binary format as 11000000111001, which is 14 bits or 2 bytes (with 6 unused bits). In a more memory-optimized format, it could use only the necessary 14 bits. The advantage of binary storage is that it's very space-efficient. Each type of data (integer, float, etc.) has a standard size, regardless of its value. The disadvantage is that binary data is not human-readable. You need to know the type of data and how it's encoded to convert it back to a readable format.</p>"},{"location":"hive/overview/#serde-in-hive","title":"SerDe in Hive","text":"<p>SerDe stands for Serializer/Deserializer. It's a crucial component of Hive used for IO operations, specifically for reading and writing data. It helps Hive to read data in custom formats and translate it into a format Hive can process (deserialization) and vice versa (serialization).</p> <p>Role of SerDe in Hive:</p> <ol> <li> <p>When reading data from a table (input to Hive), deserialization is performed by the SerDe.</p> </li> <li> <p>When writing data to a table (output from Hive), serialization is performed by the SerDe.</p> </li> </ol> <p>Serialization - Process of converting an object in memory into bytes that can be stored in a file or transmitted over a network.</p> <p>Deserialization - Process of converting the bytes back into an object in memory.</p> <p>\u201cA select statement creates deserialized data(columns) that is understood by Hive. An insert statement creates serialized data(files) that can be stored into an external storage like HDFS\u201d.</p> <p>In any table definition, there are two important sections. The \u201cRow Format\u201d describes the libraries used to convert a given row into columns. The \u201cStored as\u201d describes the InputFormat and OutputFormat libraries used by map-reduce to read and write to HDFS files.</p> <p></p>"},{"location":"hive/overview/#file-formats-in-hive-with-serde-library","title":"File formats in Hive with SerDe Library","text":"<ul> <li> <p>Textfile </p> <p>This is the default file format. Each line in the text file is a record. Hive uses the LazySimpleSerDe for serialization and deserialization. It's easy to use but doesn't provide good compression or the ability to skip over not-needed columns during read.</p> </li> <li> <p>JSON</p> <p>Hive supports reading and writing JSON format data. Note that JSON files typically do not have a splittable structure, which can affect performance as only one mapper can read the data when not splittable.</p> </li> <li> <p>CSV</p> <p>CSV is a simple, human-readable file format used to store tabular data. Columns are separated by commas, and rows by new lines. CSV files can be read and written in Hive using the</p> <p>CSV lacks a splittable structure, which may affect performance due to limited parallel processing.</p> </li> <li> <p>ORCFile (Optimized Row Columnar File)</p> <p>Introduced by Hortonworks, ORC is a highly efficient way to store Hive data. It provides efficient compression and encoding schemes with enhanced performance to handle complex data types.</p> <p>Built for the Hive query engine, ORC is a columnar storage format that allows Hive to read, write, and process data faster. It allows for efficient compression, which saves storage space, and adds improvements in the speed of data retrieval, making it suitable for performing high-speed queries. It stores collections of rows, not individual rows. Each file consists of row index, column statistics, and stripes (a row of data consisting of several rows) that contain the column data.</p> <p>Supports complex types: Structs, Lists, Maps, and Unions. Also supports advanced features like bloom filters and indexing. A bloom filter is a data structure that can identify whether an element might be present in a set, or is definitely not present. In the context of ORC, bloom filters can help skip unnecessary reads when performing a lookup on a particular column value.</p> <p>An ORC file contains groups of row data called stripes, along with auxiliary information in a file footer. At the end of the file a postscript holds compression parameters and the size of the compressed footer. The default stripe size is 250 MB. Large stripe sizes enable large, efficient reads from HDFS. The file footer contains a list of stripes in the file, the number of rows per stripe, and each column's data type. It also contains column-level aggregates count, min, max, and sum.</p> <p></p> <p>Strip structure: As shown in the diagram, each stripe in an ORC file holds index data, row data, and a stripe footer. ORC organizes data for each column into streams, the stripe footer records the location and size of these streams within the stripe. This allows the reader to locate and access specific parts of the stripe efficiently. Row data is used in table scans. Index data includes min and max values for each column and the row positions within each column. Row index entries provide offsets that enable seeking to the right compression block and byte within a decompressed block.  Note that ORC indexes are used only for the selection of stripes and row groups and not for answering queries.</p> </li> <li> <p>Parquet</p> <p>Columnar storage format, available to any project in the Hadoop ecosystem. It's designed to bring efficient columnar storage of data compared to row-based like CSV or TSV files.</p> <p>It is columnar in nature and designed to bring efficient columnar storage of data. Provides efficient data compression and encoding schemes with enhanced performance to handle complex data in comparison to row-based files like CSV.</p> <p>Schema evolution is handled in the file metadata allowing compatible schema evolution. It supports all data types, including nested ones, and integrates well with flat data, semi-structured data, and nested data sources.</p> <p>Parquet is considered a de-facto standard for storing data nowadays</p> <p>Data compression \u2013 by applying various encoding and compression algorithms, Parquet file provides reduced memory consumption</p> <p>Columnar storage \u2013 this is of paramount importance in analytic workloads, where fast data read operation is the key requirement. But, more on that later in the article\u2026</p> <p>Language agnostic \u2013 as already mentioned previously, developers may use different programming languages to manipulate the data in the Parquet file</p> <p>Open-source format \u2013 meaning, you are not locked with a specific vendor</p> <p>Support for complex data types</p> <p>In traditional, row-based storage, the data is stored as a sequence of rows. Something like this:</p> <p></p> <p>Parquet is a columnar format that stores the data in row groups! Wait, what?! Wasn\u2019t it enough complicated even before this? Don\u2019t worry, it\u2019s much easier than it sounds:) Let\u2019s go back to our previous example and depict how Parquet will store this same chunk of data:</p> <p>Let\u2019s stop for a moment and understand above diagram, as this is exactly the structure of the Parquet file (some additional things were intentionally omitted, but we will come soon to explain that as well). Columns are still stored as separate units, but Parquet introduces additional structures, called Row group.</p> <p>Why is this additional structure super important?</p> <p>In OLAP scenarios, we are mainly concerned with two concepts: projection and predicate(s). Projection refers to a SELECT statement in SQL language \u2013 which columns are needed by the query. Back to our previous example, we need only the Product and Country columns, so the engine can skip scanning the remaining ones.</p> <p>Predicate(s) refer to the WHERE clause in SQL language \u2013 which rows satisfy criteria defined in the query. In our case, we are interested in T-Shirts only, so the engine can completely skip scanning Row group 2, where all the values in the Product column equal socks!</p> <p>This means, every Parquet file contains \u201cdata about data\u201d \u2013 information such as minimum and maximum values in the specific column within the certain row group. Furthermore, every Parquet file contains a footer, which keeps the information about the format version, schema information, column metadata, and so on.</p> <p></p> </li> <li> <p>AVRO</p> <p>It's a row-oriented format that is highly splittable. It also supports schema evolution - you can have Avro data files where each file has a different schema but all are part of the same table.</p> <ul> <li> <p>Schema-Based:      Avro uses a schema to define the structure of the data. The schema is written in JSON and is included in the serialized data, allowing data to be self-describing and ensuring that the reader can understand the data structure without external information.</p> </li> <li> <p>Compact and Fast:      Avro data is serialized in a compact binary format, which makes it highly efficient in terms of both storage and transmission.</p> </li> <li> <p>Compression:      Avro supports various compression codes such as Snappy, Deflate, Bzip2, Xz</p> </li> <li> <p>Schema Evolution:      Avro supports schema evolution, allowing the schema to change over time without breaking compatibility with old data. This is particularly useful in big data environments where data structures might evolve.</p> </li> <li> <p>Rich Data Structures:      Avro supports complex data types, including nested records, arrays, maps, and unions, allowing for flexible and powerful data modelling.</p> </li> <li> <p>Interoperability:      Avro is designed to work seamlessly with other big data tools and frameworks, especially within the Hadoop ecosystem, such as Apache Hive, Apache Pig, and Apache Spark.</p> </li> <li> <p>Language Agnostic:      Avro has libraries for many programming languages, including Java, C, C++, Python, and more, enabling cross-language data exchange.</p> </li> </ul> <p></p> </li> </ul> <p>--- ORC vs Parquet vs AVRO</p> <p> </p> <p>--- How to decide which file format to choose?</p> <ul> <li> <p>Columnar vs Row-based: Columnar storage like Parquet and ORC is efficient for read-heavy workloads and is especially effective for queries that only access a small subset of total columns, as it allows skipping over non-relevant data quickly. Row-based storage like Avro is typically better for write-heavy workloads and for queries that access many or all columns of a table, as all of the data in a row is located next to each other.</p> </li> <li> <p>Schema Evolution: If your data schema may change over time, Avro is a solid choice because of its support for schema evolution. Avro stores the schema and the data together, allowing you to add or remove fields over time. Parquet and ORC also support schema evolution, but with some limitations compared to Avro.</p> </li> <li> <p>Compression: Parquet and ORC, being columnar file formats, allow for better compression and improved query performance as data of the same type is stored together. Avro also supports compression but being a row-based format, it might not be as efficient as Parquet or ORC.</p> </li> <li> <p>Splittability: When compressed, splittable file formats can still be divided into smaller parts and processed in parallel. Parquet, ORC, and Avro are all splittable, even when compressed.</p> </li> <li> <p>Complex Types and Nested Data: If your data includes complex nested structures, then Parquet is a good choice because it provides efficient encoding and compression of nested data.</p> </li> </ul>"},{"location":"hive/overview/#partitioning-in-hive","title":"Partitioning in Hive","text":"<p>Apache Hive organizes tables into partitions. Partitioning is a way of dividing a table into related parts based on the values of particular columns like date, city, and department.</p> <p>Each table in the hive can have one or more partition keys to identify a particular partition. Using partition it is easy to do queries on slices of the data.</p> <p> </p> <p>--- Why is partitioning important?</p> <ol> <li>Speeds Up Data Query: Partitioning reduces data search space for queries, speeding up data retrieval.</li> <li>Reduces I/O Operations: Only relevant data partitions are scanned, reducing unnecessary I/O operations.</li> <li>Improves Query Performance: By limiting data read, partitioning boosts query performance.</li> <li>Saves Resources: Querying only relevant partitions uses fewer computational resources.</li> <li>Manages Large Data Sets: Helps handle large datasets by dividing them into smaller, manageable parts.</li> <li>Filter Data Efficiently: Speeds up queries that commonly filter by certain columns.</li> <li>Enables Scalability: As data grows, new partitions can be added without degradation in performance.</li> <li>Data Management and Archiving: Makes it easier to archive or delete data based on time or other attributes.</li> </ol> <p>--- Types of Partitioning</p> <ul> <li> <p>Static Partitioning</p> <ol> <li>Insert input data files individually into a partition table is Static Partition.</li> <li>Usually when loading files (big files) into Hive tables static partitions are preferred.</li> <li>Static Partition saves your time in loading data compared to dynamic partition.</li> <li>You \u201cstatically\u201d add a partition in the table and move the file into the partition of the table.</li> <li>We can alter the partition in the static partition.</li> <li>Static partition is the default partition strategy in hive</li> <li>Static partition is in Strict Mode.</li> <li>You should use where clause to use limit in the static partition.</li> <li>You can perform Static partition on Hive Manage table or External table.</li> </ol> <p>Note</p> <p>Syntax to load data in Static Partitioned Table</p> <p>LOAD DATA INPATH '/hdfs/path/to/datafile' INTO TABLE employees PARTITION (year='2023');</p> <p>OR</p> <p>INSERT OVERWRITE TABLE employees PARTITION (year='2023') SELECT name, age FROM emp_data WHERE year = '2023';</p> </li> <li> <p>Dynamic Partitioning</p> <ol> <li>Single insert to partition table is known as a dynamic partition.</li> <li>Usually, dynamic partition loads the data from the non-partitioned table.</li> <li>Dynamic Partition takes more time in loading data compared to static partition.</li> <li>When you have large data stored in a table then the Dynamic partition is suitable.</li> <li>If you want to partition a number of columns but you don\u2019t know how many columns then also dynamic partition is suitable.</li> <li>Dynamic partition there is no required where clause to use limit.</li> <li>We can\u2019t perform alter on the Dynamic partition.</li> <li>You can perform dynamic partition on hive external table and managed table.</li> <li>If you want to use the Dynamic partition in the hive then the mode is in non-strict mode.     SET hive.exec.dynamic.partition = true;     SET hive.exec.dynamic.partition.mode = nonstrict;</li> </ol> <p>Note</p> <p>Syntax to load data in Dynamic Partitioned Table</p> <p>INSERT OVERWRITE TABLE employees PARTITION (year) SELECT name, age, year FROM emp_data</p> </li> </ul>"},{"location":"hive/overview/#bucketing-in-hive","title":"Bucketing in Hive","text":"<p>You\u2019ve seen that partitioning gives results by segregating HIVE table data into multiple files only when there is a limited number of partitions, what if partitioning the tables results in a large number of partitions. This is where the concept of bucketing comes in. When a column has a high cardinality, we can\u2019t perform partitioning on it. A very high number of partitions will generate too many Hadoop files which would increase the load on the node. That\u2019s because the node will have to keep the metadata of every partition, and that would affect the performance of that node In simple words, You can use bucketing if you need to run queries on columns that have huge data, which makes it difficult to create partitions.</p> <ol> <li>The concept of bucketing is based on the hashing technique.</li> <li>modules of current column value and the number of required buckets is calculated (let say, F(x) % 3).</li> <li>Based on the resulted value, the data stored into the corresponding bucket.</li> <li>The Records with the same bucketed column stored in the same bucket</li> <li>This function requires you to use the Clustered By clause to divide a table into buckets.</li> </ol> <p> </p> <p>--- Difference between Partitioning and Bucketing</p> <p> </p> <p>Partitioning and bucketing are not dependent on each other, but they can be used together to improve query performance and data management. They serve different purposes and can coexist to complement each other. </p> <p>Using partitioning and bucketing together allows Hive to prune data at the partition level and further organize data within a partition by distributing it into buckets.</p> <p>--- Benefits of partitioning</p> <ol> <li>Filtering: If queries often filter data based on a certain column, partitioning on that column can significantly reduce the amount of data read, thus improving performance. For example, if a table is partitioned by date and queries frequently request data from a specific date, partitioning can speed up these queries.</li> <li>Aggregation: If you're aggregating data based on the partition column, partitioning can optimize these operations by reducing the amount of data Hive needs to read.</li> <li>Data Management: Partitioning helps manage and organize data better. It can be useful for removing or archiving data efficiently. For example, you can quickly drop a partition to delete data for a specific date. </li> </ol> <p>--- Benefits of bucketing</p> <ol> <li>Sampling: Bucketing allows efficient sampling of data. Since each bucket essentially represents a sample of data, you can quickly get a sample by querying a single bucket.</li> <li>Join Operations: Bucketing can be used to perform more efficient map-side joins when joining on the bucketed column. If two tables are bucketed on the join columns and are of similar size, Hive can perform a bucketed map join, which is much faster than a regular join.</li> <li>Handling Skew: If data is skewed (i.e., some values appear very frequently), bucketing can distribute the data more evenly across files, improving query performance.</li> </ol>"},{"location":"hive/overview/#primary-key-in-hive","title":"Primary key in Hive","text":"<p>Apache Hive does not enforce primary key constraints natively like traditional RDBMS systems do. Hive is designed to operate on large datasets using a distributed computing approach, which is fundamentally different from how relational databases manage data. However, from Hive 2.1.0 onwards, there is support for defining primary keys during table creation for informational purposes, but they are not enforced by Hive itself.</p> <p>--- Map Side Join (Broadcast Join) in Hive</p> <p>A Map Side Join (also known as Map Join) is an optimized version of the join operation in Hive, where one table is small enough to fit into memory. This smaller table (also known as the dimension table) is loaded into memory, and the larger table (also known as the fact table) is read line by line. Because the join operation occurs at the map phase and doesn't need a reduce phase, it's much faster than a traditional join operation.</p> <p>To use a Map Join, the following properties need to be set:</p> <ol> <li>hive.auto.convert.join: This property should be set to true. It allows Hive to automatically convert a common join into a Map Join based on the sizes of the tables.</li> <li>hive.mapjoin.smalltable.filesize: This property sets the maximum size for the small table that can be loaded into memory. If the size of the table exceeds the value set by this property, Hive won't perform a Map Join. The default value is 25000000 bytes (approximately 25MB).     SET hive.auto.convert.join=true;     SET hive.mapjoin.smalltable.filesize=50000000;  // setting limit to 50MB</li> </ol> <p>Also, keep in mind that if your join operation includes more than two tables, the table in the last position of the FROM clause is considered the large table (fact table), and the other tables are considered small tables  (dimension tables). This matters because Hive attempts to perform the Map Join operation using the last table as the fact table.</p> <p>--- Bucket Map Join in Hive</p> <p>A Bucket Map Join is an optimization of a Map Join in Hive where both tables are bucketed on the join columns. Instead of loading the entire small table into memory as done in a standard Map Join, the Bucket Map Join only needs to load the relevant bucket from the small table into memory, reducing memory usage and potentially allowing larger tables to be used in a Map Join.</p> <p>To perform a Bucket Map Join, the following conditions need to be satisfied:</p> <p>Both tables should be bucketed on the join column.The number of buckets in the large table should be a multiple of the number of buckets in the small table.</p> <p>To enable Bucket Map Joins, the following properties need to be set:</p> <ol> <li>hive.auto.convert.join: This property should be set to true. It allows Hive to automatically convert a common join into a Map Join based on the sizes of the tables.</li> <li>hive.optimize.bucketmapjoin: This property should be set to true to allow Hive to convert common joins into Bucket Map Joins when possible.     SET hive.auto.convert.join=true;     SET hive.optimize.bucketmapjoin=true;</li> </ol> <p>--- Sorted Merge Bucket Join in Hive</p> <p>A Sorted Merge Bucket (SMB) Join in Hive is an optimization for bucketed tables where not only are the tables bucketed on the join column, but also sorted on the join column. This is similar to the bucket map join, but SMB join does not require one table to fit into memory, making it more scalable.</p> <p>For a Sorted Merge Bucket Join, the following conditions need to be satisfied:</p> <ol> <li>Both tables should be bucketed and sorted on the join column.</li> <li>Both tables should have the same number of buckets.</li> </ol> <p>When these conditions are satisfied, each mapper can read a bucket from each table at a time and perform the join, significantly reducing the disk I/O operations.</p> <p>To enable SMB Joins, the following properties need to be set:</p> <ol> <li>hive.auto.convert.sortmerge.join: This property should be set to true. It allows Hive to automatically convert common joins into Sorted Merge Bucket Joins when possible.</li> <li>hive.optimize.bucketmapjoin.sortedmerge: This property should be set to true to allow Hive to perform a SMB Join.</li> </ol> <p>--- Skew Join in Hive</p> <p>A Skew Join in Hive is an optimization technique for handling skewed data, where some values appear very frequently compared to others in the dataset. In a typical MapReduce job, skewed data can lead to a few reducers taking much longer to complete than others because they process a majority of the data. This can negatively impact the overall performance of the join.</p> <p>A Skew Join in Hive tries to handle this problem by performing the join in two stages:</p> <ol> <li>In the first stage, Hive identifies the skewed keys and processes all the non-skewed keys.</li> <li>In the second stage, Hive processes the skewed keys. The skewed keys are partitioned into different reducers based on a hash function, thus reducing the burden on a single reducer.</li> </ol> <p>To perform a Skew Join, the following conditions need to be satisfied:</p> <ol> <li>The join should be a two-table join. Currently, Hive does not support multi-table skew joins.</li> <li>There should be skew in key distribution. If the key distribution is uniform, a skew join may not provide any advantage and can be less efficient than a regular join.</li> </ol> <p>To enable Skew Joins, the following properties need to be set:</p> <ol> <li>hive.optimize.skewjoin: This property should be set to true. It enables the skew join optimization.</li> <li>hive.skewjoin.key: This property sets the minimum number of rows for a key to be considered skewed. The default value is 10000.</li> </ol>"},{"location":"ignite/ignite/","title":"Apache Ignite","text":"<p>Apache Ignite provides a convenient and easy-to-use interface for developers to work with large-scale data sets in real time and other aspects of in-memory computing.</p>"},{"location":"ignite/ignite/#features","title":"Features","text":"<p>Apache Ignite has the following features:</p> <ol> <li>Data grid</li> <li>Compute grid</li> <li>Service grid</li> <li>Bigdata accelerator</li> <li>Streaming grid</li> </ol> <p>The following figure illustrates the basic features of Apache Ignite:</p>"},{"location":"ignite/ignite/#primary-capabilities","title":"Primary Capabilities","text":"<ul> <li>Elasticity: An Apache Ignite cluster can grow horizontally by adding new nodes.</li> <li>Persistence: Apache Ignite data grid can persist cache entries in RDBMS, even in NoSQL like MongoDB or Cassandra.</li> <li>Cache as a Service (CaaS): Apache Ignite supports Cache-as-a-Service across the organization which allows multiple applications from different departments to access managed in-memory cache instead of slow disk base databases.</li> <li>2\u207f Level Cache: Apache Ignite is the perfect caching tier to use as a 2\u207f level cache in Hibernate and MyBatis.</li> <li>High-performance Hadoop accelerator: Apache Ignite can replace Hadoop task tracker and job tracker and HDFS to increase the performance of big data analysis.</li> <li>Share state in-memory across Spark applications: Ignite RDD allows easily sharing of state in-memory between different Spark jobs or applications. With Ignite in-memory shared RDD's, any Spark application can put data into Ignite cache which will be accessible by another Spark application later.</li> <li>Distributed computing: Apache Ignite provides a set of simple APIs that allows a user to distribute computation and data processing across multiple nodes in the cluster to gain high performance. Apache Ignite distributed services is very useful to develop and execute microservice like architecture.</li> <li>Streaming: Apache Ignite allows processing continuous never-ending streams of data in scalable and fault-tolerant fashion in-memory, rather than analyzing the data after it has been stored in the database.</li> </ul>"},{"location":"ignite/ignite/#caching-topology","title":"Caching Topology","text":"<p>Ignite provides three different approaches to caching topology: Partitioned, Replicated and Local. A cache mode is configured for each cache individually. Every caching topology has its own goal with pros and cons. The default cache topology is partitioned, without any backup option.</p> <ol> <li>Partitioned: The goal of this topology is to get extreme scalability. In this mode, the Ignite cluster transparently partitions the cached data to distribute the load across an entire cluster evenly. By partitioning the data evenly, the size of the cache and the processing power grows linearly with the size of the cluster. The responsibility for managing the data is automatically shared across the cluster. Every node or server in the cluster contains its primary data with a backup copy if defined.</li> <li> <p>With partitioned cache topology, DML operations on the cache are extremely fast, because only one primary node (optionally 1 or more backup node) needs to be updated for every key. For high availability, a backup copy of the cache entry should be configured. The backup copy is the redundant copy of one or more primary copies, which will live in another node.</p> </li> <li> <p>Replicated: The goal of this approach is to get extreme performance. With this approach, cache data is replicated to all members of the cluster. Since the data is replicated to each cluster node, it is available for use without any waiting. This provides highest possible speed for read-access; each member accesses the data from its own memory. The downside is that frequent writes are very expensive. Updating a replicated cache requires pushing the new version to all other cluster members. This will limit the scalability if there are a high frequency of updates.</p> </li> <li> <p>Local: This is a very primitive version of cache mode; with this approach, no data is distributed to other nodes in the cluster. As far as the Local cache does not have any replication or partitioning process, data fetching is very inexpensive and fast. It provides zero latency access to recently and frequently used data. The local cache is mostly used in read-only operations. It also works very well for read/write-through behavior, where data is loaded from the data sources on cache misses. Unlike a distributed cache, local cache still has all the features of distributed cache; it provides query caching, automatic data eviction and much more.</p> </li> </ol>"},{"location":"ignite/ignite/#caching-strategy","title":"Caching Strategy","text":"<p>With the explosion of high transactions web applications and mobile apps, data storage has become the main bottleneck of performance. In most cases, persistence stores such as relational databases cannot scale out perfectly by adding more servers. In this circumstance, in-memory distributed cache offers an excellent solution to data storage bottleneck. It extends multiple servers (called a grid) to pool their memory together and keep the cache synchronized across all servers. There are two main strategies to use in a distributed in-memory cache.</p> <ol> <li>Cache-aside: In this approach, an application is responsible for reading and writing from the persistence store. The cache doesn't interact with the database at all. This is called cache-aside. The cache behaves as a fast scaling in-memory data store. The application checks the cache for data before querying the data store. Also, the application updates the cache after making any changes to the persistence store. However, even though cache-aside is very fast, there are quite a few disadvantages with this strategy. Application code can become complex and may lead to code duplication if multiple applications deal with the same data store. When there are cache data misses, the application will query the data store, update the caches and continue processing. This can result in multiple data store visits if different application threads perform this processing at the same time.</li> <li>Read-through and Write-through: This is where application treats in-memory cache as the main data store, and reads data from it and writes data to it. In-memory cache is responsible for propagating the query to the data store on cache misses. Also, the data will be updated automatically whenever it is updated in the cache. All read-through and write-through operations will participate in the overall cache transaction and will be committed or rolled back as a whole. Read-through and write-through have numerous advantages over cache-aside. First of all, it simplifies application code. Read-through allows the cache to reload objects from the database when it expires automatically. This means that your application does not have to hit the database in peak hours because the latest data is always in the cache.</li> <li>Write behind: It is also possible to use write-behind to get better write performance. Write-behind lets your application quickly update the cache and return. It then aggregates the updates and asynchronously flushes them to persistence store as a bulk operation. Also with Write-behind, you can specify throttling limits, so the database writes are not performed as fast as the cache updates and therefore the pressure on the database is lower. Additionally, you can schedule the database writes to occur during off-peak hours, which can minimize the pressure on the Database.</li> </ol>"},{"location":"ignite/ignite/#cap-theorem-and-where-does-ignite-stand","title":"CAP Theorem and Where Does Ignite Stand","text":"<ul> <li>CA system: In this approach, you sacrifice partition tolerance for getting consistency and availability. Your database system offers transactions, and the system is highly available. Most of the relational databases are classified as CA systems. This system has serious problems with scaling.</li> <li>CP system: the opposite of the CA system. In CP system availability is sacrificed for consistency and partition-tolerance. In the event of the node failure, some data will be lost.</li> <li>AP system: This system is always available and partitioned. Also this system scales easily by adding nodes to the cluster. Cassandra is a good example of this type of system.</li> </ul> <p>Now, we can return back to our question, where does Ignite stand in the CAP theorem? At first glance, Ignite can be classified by CP, because Ignite is fully ACID compliant distributed transactions with partitioned tolerance. But this is half part of the history. Apache Ignite can also be considered an AP system. But why does Ignite have two different classifications? Because it has two different transactional modes for cache operations, transactional and atomic. In transactional mode, you can group multiple DML operations in one transaction and make a commit into the cache. In this scenario, Ignite will lock data on access by a pessimistic lock. If you configure backup copy for the cache, Ignite will use 2p commit protocol for its transaction. On the other hand, in atomic mode Ignite supports multiple atomic operations, one at a time. In the atomic mode, each DML operation will either succeed or fail and neither Read nor Write operation will lock the data at all. This mode gives a higher performance than the transactional mode. When you make a write in Ignite cache, for every piece of data there will be a master copy in primary node and a backup copy (if defined). When you read data from Ignite grid, you always read from the Primary node, unless the Primary node is down, at which time data will be read from the backup. From this point of view, you gain the system availability and the partition-tolerance of the entire system as an AP system. In the atomic mode, Ignite is very similar to Apache Cassandra. However, real world systems rarely fall neatly into all of these above categories, so it's more helpful to view CAP as a continuum. Most systems will make some effort to be consistent, available, and partition tolerant, and many can be tuned depending on what's most important.</p>"},{"location":"ignite/ignite/#zero-spof","title":"Zero SPOF","text":"<p>In any distributed system, node failure should be expected, particularly as the size of the cluster grows. The Zero Single Point of Failure (SPOF) design pattern ensures that no single part of a system can stop the entire cluster or system from working. Sometimes, the system using master-slave replication or the mixed master-master system falls into this category. Prior to Hadoop 2.0.0, the Hadoop NameNode was an SPOF in an HDFS cluster. Netflix has calculated the revenue loss for each ms of downtime or latency, and it is not small at all. Most businesses do not want single points of failure for the obvious reason. Apache Ignite, as a horizontally scalable distributed system, is designed in such way that all nodes in the cluster are equal, you can read and write from any node in the cluster. There are no master-slave communications in the Ignite cluster. Data is backed up or replicated across the cluster so that failure of any node doesn't bring down the entire cluster or the application. This way Ignite provides a dynamic form of High Availability. Another benefit of this approach is the ease at which new nodes can be added. When new nodes join the cluster, they can take over a portion of data from the existing nodes. Because all nodes are the same, this communication can happen seamlessly in a running cluster.</p>"},{"location":"ignite/ignite/#how-sql-queries-work-in-ignite","title":"How SQL Queries Work in Ignite","text":"<p>In chapter one, we introduced Ignite SQL query feature very superficially. In chapter four, we will go into more details about Ignite SQL queries. It's interesting to know how a query processes under the hood of Ignite. There are two main approaches to process SQL queries in Ignite:</p> <ul> <li>In-memory Map-Reduce: If you are executing any SQL query against a Partitioned cache, Ignite under the hood splits the query into in-memory map queries and a single reduce query. The number of map queries depends on the size of the partitions and number the partitions in the cluster. Then all map queries are executed on all data nodes of the participating caches, providing results to the reducing node, which will, in turn, run the reduce query over these intermediate results. If you are not familiar with the Map-Reduce pattern, you can imagine it as a Java Fork-join process.</li> <li>H2 SQL engine: if you are executing SQL queries against Replicated or Local cache, Ignite knows that all data is available locally and runs a simple local SQL query in the H2 database engine. Note that, in replicated caches, every node contains a replica data for other nodes. H2 database is a free database written in Java and can work in an embedded mode. Depending on the configuration, every Ignite node can have an embedded H2 SQL engine.</li> </ul>"},{"location":"ignite/ignite/#performance-tuning-sql-queries","title":"Performance Tuning SQL Queries","text":"<p>There are a few principles you should follow or consider when using SQL queries against Ignite cache:</p> <ul> <li>Carefully use the index, Indexes also consumes memory (on-heap/off-heap). Also, each index needs to be updated separately. If you have a huge update on a cache, index update can seriously decrease your application performance.</li> <li>Index only fields, that are participating in SQL WHERE clause.</li> <li>Do not overuse the non-collocated distributed joins approach in practice because the performance of this type of joins is worse than the performance of the affinity collocation-based joins due to the fact that there will be much more network round-trips and data movement between the nodes to fulfill a query.</li> <li>In SQL projection statement, select fields that you exactly needs. Extra fields often increase the data roundtrip over the network.</li> <li>If the query is using operator OR then it may use indexes in a way you not would expect. For example, for query <code>select name from Person where sex='M' and (age = 20 or age = 30)</code> index on field age will not be used even if it is obviously more selective than index on field sex and thus is preferable. To workaround this issue you have to rewrite the query with UNION ALL (notice that UNION without ALL will return DISTINCT rows, which will change query semantics and introduce additional performance penalty) like <code>select name from Person where sex='M' and age = 20 UNION ALL select name from Person where sex='M' and age = 30</code>. This way indexes will be used correctly.</li> <li>If query contains operator IN then it has two problems: it is impossible to provide variable list of parameters (you have to specify the exact list in query like <code>where id in (?, ?, ?)</code>), but you can not write it like <code>where id in ?</code> and pass array or collection) and this query will not use index. To work around both problems, you can rewrite the query in the following way: <code>select p.name from Person p join table(id bigint = ?) i on p.id = i.id</code>. Here you can provide object array (Object[]) of any length as a parameter, and the query will use an index on field id. Note that primitive arrays (int[], long[], etc..) can not be used with this syntax, you have to pass an array of boxed primitives.</li> </ul>"},{"location":"ignite/ignite/#expiration-eviction-of-cache-entries-in-ignite","title":"Expiration &amp; Eviction of Cache Entries in Ignite","text":"<p>Apache Ignite caches are very powerful and can be configured and tuned to suit the needs of most applications. Apache Ignite provides two different approaches for data refreshing, which refers to an aspect of a copy of data (e.g,. entries in a cache) being up-to-date with the source version of the data. A stale copy of the data is considered to be out of use. Expiration and Evictions of cache entries is one of the key aspects of caching.</p> <p>Expiration:</p> <p>Usually, the purpose of a cache is to store short-lived data that needs to refresh regularly. You can use Apache Ignite expiry policy to store entry only for a certain period of time. Once expired, the entry is automatically removed from the cache. For instance, the cache could be configured to expire entries ten seconds after they are put in. Sometimes, it is called Time-to-live or TTL. Or to expire 20 seconds after the last time the entry was accessed or retrieve from the cache. Apache Ignite provides five differents predefined expire policy as follows:</p> <ol> <li>CreatedExpiryPolicy - Defines the expiry of a Cache Entry based on when it was created. An update does not reset the expiry time</li> <li>AccessedExpiryPolicy - Defines the expiry of a Cache Entry based on the last time it was accessed. Accessed does not include a cache update.</li> <li>ModifiedExpiryPolicy - Defines the expiry of a Cache Entry based on the last time it was updated. Updating includes created and changing (updating) an entry.</li> <li>TouchedExpiryPolicy - Defines the expiry of a Cache. Entry based on when it was last touched. A touch includes creation, update or, access.</li> <li>EternalExpiryPolicy - Specifies that Cache Entries won't expire. This, however, doesn't mean they won't be evicted if an underlying implementation needs to free-up resources whereby it may choose to evict entries that are not due to expire.</li> <li>CustomExpiryPolicy - Implements javax.cache.expiry interface, which defines functions to determine when cache entries will, expire based on creation, access and modification operations.</li> </ol> <p>Eviction:</p> <p>Usually, caches are unbounded, i.e. they grow indefinitely and it is up to the application to removed unneeded cache entries. A cache eviction algorithm is a way of deciding which entries to evict (removed) when the cache is full. However, when maximum on-heap memory is full, entries are evicted into the off-heap memory, if one is enabled. Some eviction policies support batch eviction and eviction by memory size limit. If a batch eviction is enabled then eviction starts when cache size (batchSize) is greater than the maximum cache size. In this cases, batchSize entries will be evicted. If eviction by memory size limit is enabled, then eviction starts when the size of cache entries in bytes becomes greater than the maximum memory size.</p> <ol> <li>LRU: This eviction policy is based on LRU algorithm. The oldest element is the Less Recently Used (LRU) element gets evicted first. The last used timestamp is updated when an element is put into the cache, or an element is retrieved from the cache with a get call. This algorithm takes a random sample of the Elements and evicts the smallest. Using the sample size of 15 elements, empirical testing shows that an Element in the lowest quartile of use is evicted 99% of the time. This eviction policy supports batch eviction and eviction by memory size limits. This eviction policy is suitable for most of all applications and recommended by Apache Ignite.</li> <li>FIFO: Elements are evicted in the same order as they come in. When a put call is made for a new element (and assuming that the max limit is reached for the memory store), the element that was placed first (First-In) in the store is the candidate for eviction (First-Out). This algorithm is used if the use of an element makes it less likely to be used in the future. An example here would be an authentication cache. It takes a random sample of the Elements and evicts the smallest. Using the sample size of 15 elements, empirical testing shows that an Element in the lowest quartile of use is evicted 99% of the time. This implementation is very efficient since it does not create any additional table-like data structures. The ordering information is maintained by attaching ordering metadata to cache entries. This eviction policy supports batch eviction and eviction by memory size limit.</li> <li>Sorted: Sorted eviction policy is similar to FIFO eviction policy with the difference that the entries order is defined by default or user defined comparator and ensures that the minimal entry (i.e. the entry that has integer key with the smallest value) gets evicted first. Default comparator uses cache entries keys for comparison that imposes a requirement for keys to implementing Comparable interface. The user can provide own comparator implementation which can use keys, values or both for entries comparison. Supports batch eviction and eviction by memory size limit.</li> <li>Random: This cache eviction policy selects random cache entry for eviction if cache size exceeds the getMaxSize parameter. This implementation is extremely light weight, lock-free, and does not create any data structures to maintain any order for eviction. Random eviction will provide the best performance over any key queue in which every key has the same probability of being accessed. This eviction policy implementation doesn't support near cache and doesn't work on client nodes. This eviction policy is mainly used for debugging and benchmarking purposes. </li> </ol>"},{"location":"ignite/ignite/#discovery-and-communication-mechanisms","title":"Discovery and communication mechanisms","text":"<p>Any distributed system that scales linearly has at least two mechanisms: one for communication with each other, and another to discover other nodes in the cluster. These two mechanisms or techniques are the backbone of any cluster that scale-out horizontally when needed. Also, they are responsible for forming the cluster, adding new nodes, handling failures or passing messages to the nodes.</p> <p>Discovery mechanism features:</p> <ol> <li>Connect a new node to the cluster topology.</li> <li>Disconnect a node from the cluster.</li> <li>Maintains the order in which nodes connected/disconnected to or from the cluster.</li> <li>Ability to send user messages through the cluster.</li> <li>Ability to set an authenticator that will validate the connected nodes</li> </ol> <p>Ignite provides DiscoverySpi Java interface that allows discovering remote nodes in the cluster. Moreover, Ignite provides two specific DiscoverySpi implementations: TcpDiscoverySpi and ZookeeperDiscoverySpi for different scenarios.</p> <ol> <li>TCP/IP discovery: is the default implementation of the Ignite DiscoverySpi interface that allows all nodes (with enabling multicast) to discover each other inside the same network. This specific implementation uses TCP/IP for node discovery, designed, and optimized for 10s and 100-300 of nodes deployment. When using this implementation Ignite forms a ring- shaped topology. So, almost all network exchanges (except few cases) is done through it. TcpDiscoverySpi uses IP finder (TcpDiscoveryIpFinder) to share and store information about nodes IP addresses. At startup, TcpDiscoverySpi tries to send messages to random IP address taken from the TcpDiscoveryIpFinder about self-start.TcpDiscoverySpi uses local port range to discover the nodes by default. The default local port range is 100. It is not necessary to open the entire range of discovery port in the range from 47500 to 47600 in each member of the Ignite cluster.Note that you are only required to provide at least one IP address of a remote node with the TcpDiscoveryVmIpFinder, but usually, it is advisable to provide 2 or 3 addresses of grid nodes that you plan to start at some point of time in the future. Ignite will automatically discover all other grid nodes once a connection to any of the provided IP addresses is established. The TcpDiscoveryVmIpFinder uses in non-shared mode by default. The list of IP addresses should contain an address of the local node as well if you plan to start a server node in this mode. It will let the node not to wait while other nodes join the cluster but instead it becomes the first cluster node and operate usually. Otherwise, you might get into the situation like that, where one node waits for the other nodes while joining to the cluster. However, you can use the combination of both Multicast and Static IP based discovery together.The Apache Ignite TCP/IP Discovery SPI has a few major drawbacks. The transmission time of the messages between nodes is directly proportional to the number of the nodes in the cluster. It means that an Ignite cluster with more than 100 nodes may take a few more seconds for a system message to traverse through the cluster events such as joining of a new node or detecting a split-brain situation can take a while, which can affect the overall performance of the cluster. Therefore, this implementation is not an optimal solution for a large cluster</li> <li>ZooKeeper discovery: It\u2019s proposed to move from the ring topology to the star topology to overcome the above disadvantages, in the center of which the Zookeeper\u2075\u2079 service is used. At the same time, the Zookeeper cluster appears as the connection and synchronization point for the Ignite cluster. Such an implementation allows scaling Ignite cluster to 100s and 1000s of nodes preserving linear scalability and performance.The basic idea behind the discovery service through Zookeeper is that for each node to be able to identify its current state and store that information into the centralized place. Primary usages of such storage are to provide as a minimum, IP and Port number of the node to all interested nodes that might need to communicate with it. This data is often extended with other types of the data such as sequence order of a node that how it connected to the cluster. ZooKeeper cluster is used as primary storage of information for the current topology states. It also stores attributes of the nodes (user-defined attributes),the order that a node connected to the cluster, queues for storing user-defined events. All the messages about topology exchanges through Zookeeper, nodes are not communicated directly with each other. So, ZooKeeper cluster is used as primary storage of information for the current topology states in this implementation. Also, it stores attributes of the nodes (user-defined attributes), the order that a node connected to the cluster, queues for storing user-defined events. All the messages about topology exchanges through Zookeeper, nodes are not communicated directly with each other.Another essential functionality of ZooKeeper is the mechanism of notifying clients about changes. ZooKeeper allows you to store arbitrary, client information directly in the service. The recorded information can be accessed by all ZooKeeper clients once it save. ZooKeeper also provides opportunities to handle the split-brain scenario and network segmentation.</li> </ol> <p>Communication:</p> <p>Besides to discover nodes in a cluster, there are still needs for some direct communication between nodes for sending and receiving messages such as task execution, monitoring partition exchanges, etc. Ignite provides a CommunicationSpi which is responsible for peer- to-peer communication and data exchanges between nodes. Ignite CommunicationSpi is one of the most crucial SPI in Ignite. It is used heavily throughout the system and provides means for all data exchanges between nodes, such as internal implementation details and user-driven messages.Ignite comes with a built-in CommunicationSpi implementation: TcpCommunicationSpi, which uses TCP/IP protocols and Java NIO to communicate with other nodes.TcpCommunicationSpi uses IP address and port of the local node attributes to communicate with other nodes. At startup, this SPI tries to start listening to a local port specified by the configuration. SPI will automatically increment the port number until it can successfully bind for listening if the local port is occupied. TcpCommunicationSpi caches connections to remote nodes, so it does not have to reconnect every time a message is sent. Idle connections are kept active for 10 minutes by default, and they are closed. You can configure the idle connection timeout by the setIdleConnectionTimeout parameter.</p>"},{"location":"ignite/ignite/#client-connectors-variety","title":"Client Connectors Variety","text":"<ul> <li>Ignite Client connectors</li> </ul>"},{"location":"kafka/iq/","title":"KAFKA_IQ","text":"<p>7. What are consumers or users?</p> <p>Kafka Consumer subscribes to a topic(s), and also reads and processes messages from the topic(s). Moreover, with a consumer group name, Consumers label themselves. In other words, within each subscribing consumer group, each record published to a topic is delivered to one consumer instance. Make sure it is possible that Consumer instances can be in separate processes or on separate machines.</p> <p>8. What ensures load balancing of the server in Kafka?</p> <p>As the main role of the Leader is to perform the task of all read and write requests for the partition, whereas Followers passively replicate the leader. Hence, at the time of Leader failing, one of the Followers takeover the role of the Leader. Basically, this entire process ensures load balancing of the servers.</p> <p>9. What roles do Replicas and the ISR play?</p> <p>Basically, a list of nodes that replicate the log is Replicas. Especially, for a particular partition. However, they are irrespective of whether they play the role of the Leader. In addition, ISR refers to In-Sync Replicas. On defining ISR, it is a set of message replicas that are synced to the leaders.</p> <p>11. In the Producer, when does QueueFullException occur?</p> <p>Whenever the Kafka Producer attempts to send messages at a pace that the Broker cannot handle at that time QueueFullException typically occurs. However, to collaboratively handle the increased load, users will need to add enough brokers(servers, nodes), since the Producer doesn't block.</p> <p>13. What is Geo-Replication in Kafka?</p> <p>For our cluster, Kafka MirrorMaker offers geo-replication. Basically, messages are replicated across multiple data centers or cloud regions, with MirrorMaker. So, it can be used in active/passive scenarios for backup and recovery; or also to place data closer to our users, or support data locality requirements.</p> <p>14. Compare: RabbitMQ vs Apache Kafka</p> <p>One of the Apache Kafka's alternative is RabbitMQ. So, let's compare both: Features - Apache Kafka\u2013 Kafka is distributed, durable and highly available, here the data is shared as well as replicated. - RabbitMQ\u2013 There are no such features in RabbitMQ. Performance rate - Apache Kafka\u2013 To the tune of 100,000 messages/second. - RabbitMQ- In case of RabbitMQ, the performance rate is around 20,000 messages/second.</p> <p>15. Compare: Traditional queuing systems vs Apache Kafka</p> <p>Traditional queuing systems\u2013 It deletes the messages just after processing completion typically from the end of the queue. Apache Kafka\u2013 But in Kafka, messages persist even after being processed. That implies messages in Kafka don't get removed as consumers receive them. Logic-based processing - Traditional queuing systems\u2013Traditional queuing systems don't permit to process logic based on similar messages or events. - Apache Kafka\u2013 Kafka permits to process logic based on similar messages or events.</p> <p>16. What is the benefits of Apache Kafka over the traditional technique?</p> <ul> <li>Scalability: Kafka is designed for horizontal scalability. It can scale out by adding more brokers (servers) to the Kafka cluster to handle more partitions and thereby increase throughput. This scalability is seamless and can handle petabytes of data without downtime.</li> <li>Performance: Kafka provides high throughput for both publishing and subscribing to messages, even with very large volumes of data. It uses a disk structure that optimizes for batched writes and reads, significantly outperforming traditional databases in scenarios that involve high-volume, high-velocity data.</li> <li>Durability and Reliability: Kafka replicates data across multiple nodes, ensuring that data is not lost even if some brokers fail. This replication is configurable, allowing users to balance between redundancy and performance based on their requirements.</li> <li>Fault Tolerance: Kafka is designed to be fault-tolerant. The distributed nature of Kafka, combined with its replication mechanisms, ensures that the system continues to operate even when individual components .</li> <li>Real-time Processing: Kafka enables real-time data processing by allowing producers to write data into Kafka topics and consumers to read data from these topics with minimal latency. This capability is critical for applications that require real-time analytics, monitoring, and response.</li> <li>Decoupling of Data Streams: Kafka allows producers and consumers to operate independently. Producers can write data to Kafka topics without being concerned about how the data will be processed. Similarly, consumers can read data from topics without needing to coordinate with producers. This decoupling simplifies system architecture and enhances flexibility.</li> <li>Replayability: Kafka stores data for a configurable period, enabling applications to replay historical data. This is valuable for new applications that need access to historical data or for recovering from errors by reprocessing data.</li> <li>High Availability: Kafka's distributed nature and replication model ensure high availability. Even if some brokers or partitions become unavailable, the system can continue to function, ensuring continuous operation of critical applications.</li> </ul> <p>18. What is the maximum size of a message that kafka can receive?</p> <p>The maximum size of a message that Kafka can receive is determined by the message.max.bytes configuration parameter for the broker and the max.message.bytes parameter for the topic. By default, Kafka allows messages up to 1 MB (1,048,576 bytes) in size, but both parameters can be adjusted to allow larger messages if needed.</p> <p>19. What is the Zookeeper's role in Kafka's ecosystem and can we use Kafka without Zookeeper?</p> <p>Zookeeper in Kafka is used for managing and coordinating Kafka brokers. It helps in leader election for partitions, cluster membership, and configuration management among other tasks. Historically, Kafka required Zookeeper to function. However, with the introduction of KRaft mode (Kafka Raft Metadata mode), it's possible to use Kafka without Zookeeper. KRaft mode replaces Zookeeper by using a built-in consensus mechanism for managing cluster metadata, simplifying the architecture and potentially improving performance and scalability.</p> <p>21. How can you improve the throughput of a remote consumer?</p> <ul> <li>Increase Bandwidth: Ensure the network connection has sufficient bandwidth to handle the data being consumed.</li> <li>Optimize Data Serialization: Use efficient data serialization formats to reduce the size of the data being transmitted.</li> <li>Concurrency: Implement concurrency in the consumer to process data in parallel, if possible.</li> <li>Batch Processing: Where applicable, batch data together to reduce the number of roundtrip times needed.</li> <li>Caching: Cache frequently accessed data on the consumer side to reduce data retrieval times.</li> <li>Compression: Compress data before transmission to reduce the amount of data being sent over the network.</li> <li>Optimize Network Routes: Use optimized network paths or CDN services to reduce latency.</li> <li>Adjust Timeouts and Buffer Sizes: Fine-tune network settings, including timeouts and buffer sizes, for optimal data transfer rates.</li> </ul> <p>22. How can get Exactly-Once Messaging from Kafka during data production?</p> <p>During data production to get exactly once messaging from Kafka you have to follow two things: avoiding duplicates during data consumption and avoiding duplication during data production. Here are the two ways to get exactly one semantics while data production: Avail a single writer per partition, every time you get a network error checks the last message in that partition to see if your last write succeeded In the message include a primary key (UUID or something) and de-duplicate on the consumer 1. Enable Idempotence: Configure the producer for idempotence by setting <code>enable.idempotence</code> to <code>true</code>. This ensures that messages are not duplicated during network errors. 2. Transactional API: Use Kafka's Transactional API by initiating transactions on the producer. This involves setting the <code>transactional.id</code> configuration and managing transactions with <code>beginTransaction()</code>, <code>commitTransaction()</code>, and <code>abortTransaction()</code> methods. It ensures that either all messages in a transaction are successfully published, or none are in case of failure, thereby achieving exactly-once semantics. 3. Proper Configuration: Alongside enabling idempotence, adjust <code>acks</code> to <code>all</code> (or <code>-1</code>) to ensure all replicas acknowledge the messages, and set an appropriate <code>retries</code> and <code>max.in.flight.requests.per.connection</code> (should be 1 when transactions are used) to handle retries without message duplication. 4. Consistent Partitioning: Ensure that messages are partitioned consistently if the order matters. This might involve custom partitioning strategies to avoid shuffling messages among partitions upon retries.</p> <p>23. What is In-Sync Messages(ISR) in Apache Kafka?</p> <p>In Apache Kafka, ISR stands for In-Sync Replicas. It's a concept related to Kafka's high availability and fault tolerance mechanisms. For each partition, Kafka maintains a list of replicas that are considered \"in-sync\" with the leader replica. The leader replica is the one that handles all read and write requests for a specific partition, while the follower replicas replicate the leader's log. Followers that have fully caught up with the leader log are considered in-sync. This means they have replicated all messages up to the last message acknowledged by the leader. The ISR ensures data durability and availability. If the leader fails, Kafka can elect a new leader from the in-sync replicas, minimizing data loss and downtime.</p> <p>24. How can we reduce churn (frequent changes) in ISR?</p> <p>ISR is a set of message replicas that are completely synced up with the leaders, in other word ISR has all messages that are committed. ISR should always include all replicas until there is a real failure. A replica will be dropped out of ISR if it deviates from the leader. - Optimize Network Configuration: Ensure that the network connections between brokers are stable and have sufficient bandwidth. Network issues can cause followers to fall behind and drop out of the ISR. - Adjust Replica Lag Configuration: Kafka allows configuration of parameters like <code>replica.lag.time.max.ms</code> which defines how long a replica can be behind the leader before it is considered out of sync. Adjusting this value can help manage ISR churn by allowing replicas more or less time to catch up. - Monitor and Scale Resources Appropriately: Ensure that all brokers have sufficient resources (CPU, memory, disk I/O) to handle their workload. Overloaded brokers may struggle to keep up, leading to replicas falling out of the ISR. - Use Dedicated Networks for Replication Traffic: If possible, use a dedicated network for replication traffic. This can help prevent replication traffic from being impacted by other network loads.</p> <p>25. When does a broker leave ISR?</p> <p>A broker may leave the ISR for a few reasons: - Falling Behind: If a replica falls behind the leader by more than the configured thresholds (<code>replica.lag.time.max.ms</code> or <code>replica.lag.max.messages</code>), it is removed from the ISR. - Broker Failure: If a broker crashes or is otherwise disconnected from the cluster, its replicas are removed from the ISR. - Manual Intervention: An administrator can manually remove a replica from the ISR, although this is not common practice and should be done with caution.</p> <p>26. What does it indicate if replica stays out of Isr for a long time?</p> <p>If a replica stays out of the ISR (In-Sync Replicas) for a long time, it indicates that the replica is not able to keep up with the leader's log updates. This can be due to network issues, hardware failure, or high load on the broker. As a result, the replica might become a bottleneck for partition availability and durability, since it cannot participate in acknowledging writes or be elected as a leader if the current leader fails.</p> <p>27. What happens if the preferred replica is not in the ISR list?</p> <p>If the preferred replica is not in the In-Sync Replicas (ISR) for a Kafka topic, the producer will either wait for the preferred replica to become available (if configured with certain ack settings) or send messages to another available broker that is part of the ISR. This ensures data integrity by only using replicas that are fully up-to-date with the leader. Consumers might experience a delay in data availability if they are set to consume only from the preferred replica and it is not available</p> <p>28. Is it possible to get the message offset after producing to a topic?</p> <p>You cannot do that from a class that behaves as a producer like in most queue systems, its role is to fire and forget the messages. The broker will do the rest of the work like appropriate metadata handling with id\u2019s, offsets, etc. As a consumer of the message, you can get the offset from a Kafka broker. If you look in the SimpleConsumer class, you will notice it fetches MultiFetchResponse objects that include offsets as a list. In addition to that, when you iterate the Kafka Message, you will have MessageAndOffset objects that include both, the offset and the message sent. Yes, it is possible to get the message offset after producing a message in Kafka. When you send a message to a Kafka topic, the producer API can return metadata about the message, including the offset of the message in the topic partition</p> <p>30. Can you explain the concept of leader and follower in kafka ecosystem?</p> <p>In Apache Kafka, the concepts of \"leader\" and \"follower\" refer to roles that brokers play within a Kafka cluster to manage partitions of a topic. - Leader: For each partition of a topic, there is one broker that acts as the leader. The leader is responsible for handling all read and write requests for that partition. When messages are produced to a partition, they are sent to the leader broker, which then writes the messages to its local storage. The leader broker ensures that messages are stored in the order they are received. - Follower: Followers are other brokers in the cluster that replicate the data of the leader for fault tolerance. Each follower continuously pulls messages from the leader to stay up-to-date, ensuring that it has an exact copy of the leader's data. In case the leader broker fails, one of the followers can be elected as the new leader, ensuring high availability.</p> <p>33. What do you mean by Kafka schema registry?</p> <p>A Schema Registry is present for both producers and consumers in a Kafka cluster, and it holds Avro schemas. For easy serialization and de-serialization, Avro schemas enable the configuration of compatibility parameters between producers and consumers. The Kafka Schema Registry is used to ensure that the schema used by the consumer and the schema used by the producer are identical. The producers just need to submit the schema ID and not the whole schema when using the Confluent schema registry in Kafka. The consumer looks up the matching schema in the Schema Registry using the schema ID.</p> <p>34. Tell me about some of the use cases where Kafka is not suitable.</p> <p>Following are some of the use cases where Kafka is not suitable - Kafka is designed to manage large amounts of data. Traditional messaging systems would be more appropriate if only a small number of messages need to be processed every day. - Although Kafka includes a streaming API, it is insufficient for executing data transformations. For ETL (extract, transform, load) jobs, Kafka should be avoided. - There are superior options, such as RabbitMQ, for scenarios when a simple task queue is required. - If long-term storage is necessary, Kafka is not a good choice. It simply allows you to save data for a specific retention period and no longer.</p> <p>35. What do you understand about Kafka MirrorMaker?</p> <p>The MirrorMaker is a standalone utility for copying data from one Apache Kafka cluster to another. The MirrorMaker reads data from original cluster topics and writes it to a destination cluster with the same topic name. The source and destination clusters are separate entities that can have various partition counts and offset values.</p> <p>36. Describe message compression in Kafka. What is the need of message compression in Kafka? Also mention if there are any disadvantages of it.</p> <p>Producers transmit data to brokers in JSON format in Kafka. The JSON format stores data in string form, which can result in several duplicate records being stored in the Kafka topic. As a result, the amount of disc space used increases. As a result, before delivering messages to Kafka, compression or delaying of data is performed to save disk space. Because message compression is performed on the producer side, no changes to the consumer or broker setup are required.</p> <p>Advantages: - It decreases the latency of messages transmitted to Kafka by reducing their size. - Producers can send more net messages to the broker with less bandwidth. - When data is saved in Kafka using cloud platforms, it can save money in circumstances where cloud services are paid. - Message compression reduces the amount of data stored on disk, allowing for faster read and write operations.</p> <p>Disadvantages: - Producers must use some CPU cycles to compress their work. - Decompression takes up several CPU cycles for consumers. - Compression and decompression place a higher burden on the CPU.</p> <p>37. What do you understand about log compaction and quotas in Kafka?</p> <p>Log compaction is a way through which Kafka assures that for each topic partition, at least the last known value for each message key within the log of data is kept. This allows for the restoration of state following an application crash or a system failure. During any operational maintenance, it allows refreshing caches after an application restarts. Any consumer processing the log from the beginning will be able to see at least the final state of all records in the order in which they were written, because of the log compaction.</p> <p>A Kafka cluster can apply quotas on producers and fetch requests as of Kafka 0.9. Quotas are byte-rate limits that are set for each client-id. A client-id is a logical identifier for a request-making application. A single client-id can therefore link to numerous producers and client instances. The quota will be applied to them all as a single unit. Quotas prevent a single application from monopolizing broker resources and causing network saturation by consuming extremely large amounts of data.</p> <p>38. What do you mean by an unbalanced cluster in Kafka? How can you balance it?</p> <p>It's as simple as assigning a unique broker id, listeners, and log directory to the server.properties file to add new brokers to an existing Kafka cluster. However, these brokers will not be allocated any data partitions from the cluster's existing topics, so they won't be performing much work unless the partitions are moved or new topics are formed.</p> <p>A cluster is referred to as unbalanced if it has any of the following problems : - Leader Skew - Broker Skew</p> <p>39. What do you mean by BufferExhaustedException and OutOfMemoryException in Kafka?</p> <p>When the producer can't assign memory to a record because the buffer is full, a BufferExhaustedException is thrown. If the producer is in non-blocking mode, and the rate of production exceeds the rate at which data is transferred from the buffer for long enough, the allocated buffer will be depleted, the exception will be thrown.</p> <p>If the consumers are sending huge messages or if there is a spike in the number of messages sent at a rate quicker than the rate of downstream processing, an OutOfMemoryException may arise. As a result, the message queue fills up, consuming memory space.</p> <p>40. What are Znodes in Kafka Zookeeper? How many types of Znodes are there?</p> <p>The nodes in a ZooKeeper tree are called znodes. Version numbers for data modifications, ACL changes, and timestamps are kept by Znodes in a structure. ZooKeeper uses the version number and timestamp to verify the cache and guarantee that updates are coordinated. Each time the data on Znode changes, the version number connected with it grows.</p> <p>There are three different types of Znodes: - Persistence Znode: These are znodes that continue to function even after the client who created them has been disconnected. Unless otherwise specified, all znodes are persistent by default. - Ephemeral Znode: Ephemeral znodes are only active while the client is still alive. When the client who produced them disconnects from the ZooKeeper ensemble, the ephemeral Znodes are automatically removed. They have a significant part in the election of the leader. - Sequential Znode: When znodes are constructed, the ZooKeeper can be asked to append an increasing counter to the path's end. The parent znode's counter is unique. Sequential nodes can be either persistent or ephemeral.</p> <p>41. What is meant by the Replication Tool?</p> <p>The Replication Tool in Kafka is used for a high-level design to maintain Kafka replicas. Some of the replication tools available are</p> <ul> <li>Preferred Replica Leader Election Tool: Partitions are distributed to multiple brokers in a cluster, each copy known as a replica. The preferred replica usually refers to the leader. The brokers distribute the leader role evenly across the cluster for various partitions. Still, an imbalance can occur over time due to failures, planned shutdowns, etc. in such cases, you can use the replication tool to maintain the load balancing by reassigning the preferred replicas and hence, the leaders.</li> <li>Topics tool: Kafka topics tool is responsible for handling all management operations related to topics, which include Listing and describing topics, Creating topics, Changing topics, Adding partitions to a topic, Deleting topics</li> <li>Reassign partitions tool: This tool changes the replicas assigned to a partition. This means adding or removing followers associated with a partition.</li> <li>StateChangeLogMerger tool: This tool is used to collect data from the brokers in a particular cluster, formats it into a central log, and help to troubleshoot issues with state changes. Often, problems may arise with the leader election for a particular partition. This tool can be used to determine what caused the problem.</li> <li>Change topic configuration tool: used to Add new config options, Change existing config options, and Remove config options</li> </ul> <p>42. How can Kafka be tuned for optimal performance?</p> <p>Tuning for optimal performance involves consideration of two key measures: latency measures, which denote the amount of time taken to process one event, and throughput measures, which refer to how many events can be processed in a specific time. Most systems are optimized for either latency or throughput, while Kafka can balance both.</p> <p>Tuning Kafka for optimal performance involves the following steps: - Tuning Kafka producers: Data that the producers have to send to brokers is stored in a batch. When the batch is ready, the producer sends it to the broker. For latency and throughput, to tune the producers, two parameters must be taken care of: batch size and linger time. The batch size has to be selected very carefully. If the producer is sending messages all the time, a larger batch size is preferable to maximize throughput. However, if the batch size is chosen to be very large, then it may never get full or take a long time to fill up and, in turn, affect the latency. Batch size will have to be determined, taking into account the nature of the volume of messages sent from the producer. The linger time is added to create a delay to wait for more records to get filled up in the batch so that larger records are sent. A longer linger time will allow more messages to be sent in one batch, but this could compromise latency. On the other hand, a shorter linger time will result in fewer messages getting sent faster - reduced latency but reduced throughput as well. - Tuning Kafka broker: Each partition in a topic is associated with a leader, which will further have 0 or more followers. It is important that the leaders are balanced properly and ensure that some nodes are not overworked compared to others. - Tuning Kafka Consumers: It is recommended that the number of partitions for a topic is equal to the number of consumers so that the consumers can keep up with the producers. In the same consumer group, the partitions are split up among the consumers.</p> <p>43. How can all brokers available in a cluster be listed?</p> <p>Two ways to get the list of available brokers in an Apache Kafka cluster are as follows: - Using zookeeper-shell.sh <pre><code>zookeeper-shell.sh &lt;zookeeper_host&gt;:2181 ls /brokers/ids\n</code></pre>   Which will give an output like:   <code>WATCHER:: WatchedEvent state:SyncConnected type:None path:null [0, 1, 2, 3]</code>   This indicates that there are four alive brokers - 0,1,2 and 3 - Using zkCli.sh   First, you have to log in to the ZooKeeper client   <pre><code>zkCli.sh -server &lt;zookeeper_host&gt;:2181\nls /brokers/ids\n</code></pre>   Both the methods used above make use of the ZooKeeper to find out the list of available brokers</p> <p>44. What is the Kafka MirrorMaker?</p> <p>The Kafka MirrorMaker is a stand-alone tool that allows data to be copied from one Apache Kafka cluster to another. The Kafka MirrorMaker will read data from topics in the original cluster and write the topics to a destination cluster with the same topic name. The source and destination clusters are independent entities and can have different numbers of partitions and varying offset values.</p> <p>45. What is meant by Kafka Connect?</p> <p>Kafka Connect is a tool provided by Apache Kafka to allow scalable and reliable streaming data to move between Kafka and other systems. It makes it easier to define connectors that are responsible for moving large collections of data in and out of Kafka. Kafka Connect is able to process entire databases as input. It can also collect metrics from application servers into Kafka topics so that this data can be available for Kafka stream processing.</p> <p>47. What is the need for message compression in Apache Kafka?</p> <p>Message compression in Kafka does not require any changes in the configuration of the broker or the consumer. It is beneficial for the following reasons: - Due to reduced size, it reduces the latency in which messages are sent to Kafka. - Reduced bandwidth allows the producers to send more net messages to the broker. - When the data is stored in Kafka via cloud platforms, it can reduce the cost in cases where the cloud services are paid. - Message compression leads to reduced disk load, which will lead to faster read and write requests.</p> <p>50. When does Kafka throw a BufferExhaustedException?</p> <p>BufferExhaustedException is thrown when the producer cannot allocate memory to a record due to the buffer being too full. The exception is thrown if the producer is in non-blocking mode and the rate of data production exceeds the rate at which data is sent from the buffer for long enough for the allocated buffer to be exhausted.</p> <p>51. What are the responsibilities of a Controller Broker in Kafka?</p> <p>The main role of the Controller is to manage and coordinate the Kafka cluster, along with the Apache ZooKeeper. Any broker in the cluster can take on the role of the controller. However, once the application starts running, there can be only one controller broker in the cluster. When the broker starts, it will try to create a Controller node in ZooKeeper. The first broker that creates this controller node becomes the controller.</p> <p>The controller is responsible for - creating and deleting topics - Adding partitions and assigning leaders to the partitions - Managing the brokers in a cluster - adding new brokers, active broker shutdown, and broker failures - Leader Election - Reallocation of partitions.</p> <p>52. What causes OutOfMemoryException?</p> <p>OutOfMemoryException can occur if the consumers are sending large messages or if there is a spike in the number of messages wherein the consumer is sending messages at a rate faster than the rate of downstream processing. This causes the message queue to fill up, taking up memory.</p> <p>53. Explain the graceful shutdown in Kafka.</p> <p>Any broker shutdown or failure will automatically be detected by the Apache cluster. In such a case, new leaders will be elected for partitions that were previously handled by that machine. This can occur due to server failure and even if it is intentionally brought down for maintenance or any configuration changes. In cases where the server is intentionally brought down, Kafka supports a graceful mechanism for stopping the server rather than just killing it.</p> <p>Whenever a server is stopped: - Kafka ensures that all of its logs are synced onto a disk to avoid needing any log recovery when it is restarted. Since log recovery takes time, this can speed up intentional restarts. - Any partitions for which the server is the leader will be migrated to the replicas prior to shutting down. This ensures that the leadership transfer is faster, and the time during which each partition is unavailable will be reduced to a few milliseconds.</p> <p>54. How can a cluster be expanded in Kafka?</p> <p>In order to add a server to a Kafka cluster, it just has to be assigned a unique broker id, and Kafka has to be started on this new server. However, a new server will not automatically be assigned any of the data partitions until a new topic is created. Hence, when a new machine is added to the cluster, it becomes necessary to migrate some existing data to these machines. The partition reassignment tool can be used to move some partitions to the new broker. Kafka will add the new server as a follower of the partition that it is migrating to and allow it to completely replicate the data on that particular partition. When this data is fully replicated, the new server can join the ISR; one of the existing replicas will delete the data that it has with respect to that particular partition.</p> <p>55. What is meant by the Kafka schema registry?</p> <p>For both the producers and consumers associated with a Kafka cluster, a Schema Registry is present, which stores Avro schemas. Avro schemas allow the configuration of compatibility settings between the producers and the consumers for seamless serialization and deserialization. Kafka Schema Registry is used to ensure that there is no difference in the schema that is being used by the consumer and the one that is being used by the producer. While using the Confluent schema registry in Kafka, the producers only need to send the schema ID and not the entire schema. The consumer uses the schema ID to look up the corresponding schema in the Schema Registry.</p> <p>61. Suppose you are sending messages to a Kafka topic using kafkaTemplate. You come across a requirement that states that if a failure occurs while delivering messages to a Kafka topic, you must retry sending the messages on the same partition with the same offset. How can you achieve this using kafkatemplate?</p> <p>If you give the key while delivering the message, it will be stored in the same partition regardless of how many times you send it. The hashed key is used by Kafka to decide which partition needs to be updated. The only way to ensure that a failed message has the same offset when retried is to ensure that nothing is put into the topic before retrying it.</p> <p>62. Assume your brokers are hosted on AWS EC2. If you're a producer or consumer outside of the Kafka cluster network, you will only be capable of reaching the brokers over their public DNS, not their private DNS. Now, assume your client (producer or consumer) is outside your Kafka cluster's network, and you can only reach the brokers via their public DNS. The private DNS of the brokers hosting the leader partitions, not the public DNS, will be returned by the broker. Unfortunately, since your client is not present on your Kafka cluster's network, they will be unable to resolve the private DNS, resulting in the LEADER NOT AVAILABLE error. How will you resolve this network error?</p> <p>When you first start using Kafka brokers, you might have many listeners. Listeners are just a combination of hostname or IP, port, and protocol. Each Kafka broker's server.properties file contains the properties listed below. The important property that will enable you to resolve this network error is advertised.listeners. - listeners \u2013 a list of comma-separated hostnames and ports that Kafka brokers listen to. - advertised.listeners \u2013 a list of comma-separated hostnames and ports that will be returned to clients. Only include hostnames that will be resolved at the client (producer or consumer) level, such as public DNS. - inter.broker.listener.name \u2013 listeners used for internal traffic across brokers. These hostnames do not need to be resolved on the client side, but all of the cluster's brokers must resolve them. - listener.security.protocol.map \u2013 lists the supported protocols for each listener.</p> <p>63. Let's suppose a producer writes records to a Kafka topic at a rate of 10000 messages per second, but the consumer can only read 2500 messages per second. What are the various strategies for expanding your consumer group?</p> <p>The solution to this question has two parts: topic partitions and consumer groups. Partitions are used to split a Kafka topic. The producer's message is divided among the topic's partitions based on the message key. You can suppose that the key is chosen in such a way that messages are spread evenly between the partitions. Consumer groups are a method of grouping consumers together to maximize a consumer application's throughput. Each consumer in a consumer group holds on to a topic partition. If the Kafka topic has four partitions and the consumer group has four consumers, each consumer will read from a single partition. If there are six partitions and four consumers, the data will be read in parallel from only four partitions. As a result, maintaining a 1-to-1 mapping of partition to the consumer in the consumer group is preferable. Now, you can do two things to increase processing on the consumer side: - You can increase the topic's partition count (say from existing 1 to 4). - You can build a Kafka consumer group with four consumer instances tied to it. This would enable the consumers to read data from the topic in parallel, allowing it to expand from 2500 to 10000 messages per second.</p> <p>64. What is Kafka's producer acknowledgment? What are the various types of acknowledgment settings that Kafka provides?</p> <p>A broker sends an ack or acknowledgment to the producer to verify the reception of the message. Ack level is a configuration parameter in the Producer that specifies how many acknowledgments the producer must receive from the leader before a request is considered successful. The following types of acknowledgment are available: - acks=0: In this setting, the producer does not wait for the broker's acknowledgment. There is no way to know if the broker has received the record. - acks=1: In this situation, the leader logs the record to its local log file and answers without waiting for all of its followers to acknowledge it. The message can only be lost in this instance if the leader fails shortly after accepting the record but before the followers have copied it; otherwise, the record would be lost. - acks=all: A set leader in this situation waits for all in-sync replica sets to acknowledge the record. As long as one replica is alive, the record will not be lost, and the best possible guarantee will be provided. However, because a leader must wait for all followers to acknowledge before replying, the throughput is significantly lower.</p> <p>65. How do you get Kafka to perform in a FIFO manner?</p> <p>Kafka organizes messages into topics, which are then divided into partitions. The partition is an immutable list of ordered messages that is updated regularly. A message in the partition is uniquely recognized by a sequential number called offset. FIFO behavior is possible only within the partitions. Following the methods below will help you achieve FIFO behavior: - To begin, we first set the enable the auto-commit property to be false:   <code>Set enable.auto.commit=false</code> - We should not call the <code>consumer.commitSync();</code> method after the messages have been processed. - Then we may \"subscribe\" to the topic and ensure that the consumer system's register is updated. - You should use Listener consumerRebalance, and call a consumer inside a listener.   <code>seek(topicPartition, offset)</code>. - The offset related to the message should be kept together with the processed message once it has been processed.</p> <p>69. How would you secure a Kafka cluster?</p> <p>Top candidates would use multiple layers of security and strategies such as: - SSL/TLS for encryption of data in transit - SASL/SCRAM for authentication - A Kerberos integration - Network policies for controlling access to the Kafka cluster - ACLs (Access Control Lists) for authorizing actions by users or groups on specific topics</p> <p>72. What authentication mechanisms can you use in Kafka?</p> <p>Kafka supports: - SSL/TLS for encrypting data and optionally authenticating clients using certificates - SASL (Simple Authentication and Security Layer) which supports mechanisms like GSSAPI (Kerberos), PLAIN, and SCRAM to secure Kafka brokers against unauthorized access - Integration with enterprise authentication systems like LDAP</p> <p>73. Describe an instance where Kafka might lose data and how you would prevent it.</p> <p>A good response will mention cases such as unclean leader elections, broker failures, or configuration errors that lead to data loss. Candidates should explain how they'd configure Kafka's replication factors, min.insync.replicas, and acknowledgment settings to prevent data loss. They should also mention they'd do regular backups and set up consistent monitoring to prevent issues.</p> <p>74. What is linger.ms in Kafka producers?</p> <p>Definition: linger.ms is a producer configuration that specifies the time (in milliseconds) the producer waits before sending a batch of messages.</p> <p>Behavior: - If the batch is full (batch.size reached), it is sent immediately. - If the batch is not full, the producer waits for the linger.ms time before sending the batch, hoping more records will arrive.</p> <p>Purpose: - To improve throughput by batching more records into a single request. - Reduces the number of network calls but may slightly increase latency.</p> Aspect commitSync() commitAsync() Type Synchronous Asynchronous Blocking Blocks until the broker acknowledges the commit Does not block; continues processing Reliability Highly reliable; throws exception on failure Less reliable; errors may be ignored Performance Slower due to waiting for acknowledgment Faster due to non-blocking behavior Use Case Critical systems (e.g., financial transactions) High-throughput systems (e.g., analytics) <p>Default Value: 0 , meaning no waiting and the producer sends records as soon as possible.</p> <p>Example Scenario: If linger.ms = 10 and batch.size isn't reached, the producer will wait 10ms before sending the batch, potentially grouping more messages together.</p> <p>75. How does Kafka manage backpressure?</p> <p>Kafka handles backpressure by controlling the flow of data between producers, brokers, and consumers through these mechanisms:</p> <p>Producer-Side: - Buffering: Producers buffer records up to buffer.memory . If the buffer is full, the producer blocks or throws an exception (based on max.block.ms ). - Batching: Producers optimize sending data in batches ( batch.size ) to handle high-throughput workloads efficiently.</p> <p>Broker-Side: - Replication Quotas: Kafka enforces quotas for replication to ensure brokers aren't overwhelmed. - I/O Throttling: Limits disk and network I/O rates to maintain cluster stability.</p> <p>Consumer-Side: - Pause and Resume: Consumers can pause fetching records if they can't process fast enough, avoiding memory overload. - Fetch Min/Max Bytes: Controls how much data is fetched at a time to prevent excessive resource usage.</p> <p>76. CommitSync() vs CommitAsync() in Kafka consumers</p> Aspect commitSync() commitAsync() Type Synchronous Asynchronous Blocking Blocks until the broker acknowledges the commit Does not block; continues processing Error Handling Direct exception handling Handle via a callback function Reliability Highly reliable; throws exception on failure Less reliable; errors may be ignored Performance Slower due to waiting for acknowledgment Faster due to non-blocking behavior Use Case Critical systems (e.g., financial transactions) High-throughput systems (e.g., analytics) <p>78. Explain the term Log Anatomy</p> <p>We view logs as the partitions. Basically, a data source writes messages to the log. One of the advantages is, at any time one or more consumers read from the log they select.</p> <p>79. What is a Data Log in Kafka?</p> <p>As we know, messages are retained for a considerable amount of time in Kafka. Moreover, there is flexibility for consumers that they can read as per their convenience. Although, there is a possible case that if Kafka is configured to keep messages for 24 hours and possibly that time the consumer is down for a time greater than 24 hours, then the consumer may lose those messages. However, still, we can read those messages from the last known offset, but only at a condition that the downtime on part of the consumer is just 60 minutes. Moreover, on what consumers are reading from a topic Kafka doesn\u2019t keep state.</p> <p>86. What are the considerations for scaling Kafka clusters and applications in a production environment?</p> <p>Scaling Kafka clusters and applications involves considerations such as adding more brokers, increasing the number of partitions, optimizing hardware resources, and fine-tuning configuration parameters. Load balancing and monitoring tools are also essential for managing scalability effectively.</p>"},{"location":"kafka/overview/","title":"Kafka","text":""},{"location":"kafka/overview/#kafka-architecture","title":"Kafka Architecture","text":"<p>--- Kafka Cluster</p> <p>A Kafka cluster is a system of multiple interconnected Kafka brokers (servers). These brokers cooperatively handle data distribution and ensure fault tolerance, thereby enabling efficient data processing and reliable storage.</p> <p>--- Kafka Broker</p> <p>A Kafka broker is a server in the Apache Kafka distributed system that stores and manages the data (messages). It handles requests from producers to write data, and from consumers to read data. Multiple brokers together form a Kafka cluster.</p> <p>--- Kafka Zookeeper</p> <p>Apache ZooKeeper is a service used by Kafka for cluster coordination, failover handling, and metadata management. It keeps Kafka brokers in sync, manages topic and partition information, and aids in broker failure recovery and leader election.</p> <p>--- Kafka Producer</p> <p>In Apache Kafka, a producer is an application that sends messages to Kafka topics. It handles message partitioning based on specified keys, serializes data into bytes for storage, and can receive acknowledgments upon successful message delivery. Producers also feature automatic retry mechanisms and error handling capabilities for robust data transmission.</p> <p>--- Kafka Consumer</p> <p>A Kafka consumer is an application that reads (or consumes) messages from Kafka topics. It can subscribe to one or more topics, deserializes the received byte data into a usable format, and has the capability to track its offset (the messages it has read) to manage the reading position within each partition. It can also be part of a consumer group to share the workload of reading messages.</p>"},{"location":"kafka/overview/#role-of-zookeeper-in-kafka","title":"Role of Zookeeper in Kafka","text":"<p>Zookeeper is a critical component used to monitor Kafka clusters and coordinate with them. It stores all the metadata information related to Kafka clusters, including the status of replicas and leaders. This metadata is crucial for configuration information, cluster health, and leader election within the cluster. Zookeeper nodes working together to manage distributed systems are known as a Zookeeper Cluster or Zookeeper Ensemble.</p> <p>Note</p> <p>If a Kafka server hosting a partition's leader fails, Zookeeper quickly identifies this and coordinates the election of a new leader from the available replicas, ensuring continuous operation.   Zookeeper uses specific parameters and maintains various internal states to manage Kafka.</p> <p>--- Zookeeper Configuration Concepts</p> <ul> <li> <p>initLimit - Defines the time in milliseconds that a Zookeeper follower node can take to initially connect to a leader.</p> </li> <li> <p>syncLimit - Defines the time in milliseconds that a Zookeeper follower can be out of sync with the leader.</p> </li> <li> <p>clientPort - This is the port number (e.g., 2181) where Zookeeper clients connect. It refers to the data directory used to store client node server details.</p> </li> <li> <p>maxClientCnxns - This parameter sets the maximum number of client connections that a single Zookeeper server can handle at once.</p> </li> <li> <p>server.1, server.2, server.3 - These entries define the server IDs and their IP addresses/ports within the Zookeeper ensemble (e.g., server.1: 2888:3888). These are crucial for leader election among the Zookeeper servers.</p> </li> </ul> <p>--- Kafka Partition States (as managed by Zookeeper)</p> <ul> <li>New Nonexistent Partition - This state indicates that a partition was either never created or was created and then subsequently deleted.</li> <li>Nonexistent Partition (after deletion) - This state specifically means the partition was deleted.</li> <li>Offline Partition - A partition is in this state when it should have replicas assigned but has no leader elected.</li> <li>Online Partition - A partition enters this state when a leader is successfully elected for it. If all leader election processes are successful, the partition transitions from Offline Partition to Online Partition.</li> </ul> <p>--- Kafka Replica States (as managed by Zookeeper)</p> <ul> <li>New Replica - Replicas are created during topic creation or partition reassignment. In this state, a replica can only receive follower state change requests.</li> <li>Online Replica - A replica is considered Online when it is started and has assigned replicas for its partition. In this state, it can either become a leader or become a follower based on state change requests.</li> <li>Offline Replica - If a replica dies (becomes unavailable), it moves to this state. This typically happens when the replica is down. -- Nonexistent Replica - If a replica is deleted, it moves into this\u00a0state.</li> </ul>"},{"location":"kafka/overview/#partitions","title":"Partitions","text":"<p>Topics are split into partitions. All messages within a specific partition are ordered and immutable (meaning they cannot be changed after being written). Each message within a partition has a unique ID called an Offset. This offset denotes the message's position within that specific partition.</p> <p>--- Partitions play a crucial role in Kafka's functionality and scalability</p> <ul> <li> <p>Parallelism - Partitions enable parallelism. Since each partition can be placed on a separate machine (broker), a topic can handle an amount of data that exceeds a single server's capacity. This allows producers and consumers to read and write data to a topic concurrently, thus increasing throughput.</p> </li> <li> <p>Ordering - Kafka guarantees that messages within a single partition will be kept in the exact order they were produced. However, if order is important across partitions, additional design considerations are needed.</p> </li> <li> <p>Replication - Partitions of a topic can be replicated across multiple brokers based on the topic's replication factor. This increases data reliability and availability.</p> </li> <li> <p>Failover - In case of a broker failure, the leadership of the partitions owned by that broker will be automatically taken over by another broker, which has the replica of these partitions.</p> </li> <li> <p>Consumer Groups - Each partition can be consumed by one consumer within a consumer group at a time. If more than one consumer is needed to read data from a topic simultaneously, the topic needs to have more than one partition.</p> </li> <li> <p>Offset - Every message in a partition is assigned a unique (per partition) and sequential ID called an offset. Consumers use this offset to keep track of their position in the partition.</p> </li> </ul> <p>--- Kafka Partition Assignment Strategies</p> <p>When rebalancing happens, Kafka uses specific algorithms to determine how partitions are assigned to consumers.</p> <ul> <li>Range Partitioner - The Range Partitioner assigns a contiguous \"range\" of partitions to each consumer. It sorts partitions numerically (e.g., 0, 1, 2, 3, 4, 5).It then divides the total number of partitions by the number of consumers to determine the number of partitions each consumer should handle. A contiguous block of partitions (a \"range\") is assigned to each consumer.This strategy ensures a relatively uniform distribution of the number of partitions per consumer, though not necessarily the load if data is skewed across partitions.</li> </ul> <p></p> <p>Example</p> <p>Suppose a topic has 6 partitions (0, 1, 2, 3, 4, 5) and there are 2 consumers in the group. Consumer 1 would be assigned partitions 0, 1, 2 (a range of three partitions). Consumer 2 would be assigned partitions 3, 4, 5 (the next range of three partitions).</p> <ul> <li>Round Robin Partitioner - The Round Robin Partitioner distributes partitions among consumers in a rotating, round-robin fashion.It iterates through the sorted list of partitions (0, 1, 2, 3, 4, 5).It assigns the first partition to Consumer 1, the second to Consumer 2, the third back to Consumer 1, and so on.</li> </ul> <p></p> <p>Example</p> <p>Suppose a topic has 6 partitions (0, 1, 2, 3, 4, 5) and there are 2 consumers in the group.   Consumer 1 would be assigned partitions 0, 2, 4.   Consumer 2 would be assigned partitions 1, 3, 5.</p> <p>This strategy aims for a more even distribution of partitions, which can sometimes lead to better load balancing if the message load per partition is relatively uniform.</p> <p>--- Kafka Cluster &amp; Partition Reassignment</p> <ul> <li>Kafka Cluster Controller - In a Kafka cluster, one of the brokers is designated as the controller. This controller is responsible for managing the states of partitions and replicas and for performing administrative tasks such as reassigning partitions.</li> <li>Partition Growth - It is important to note that the partition count of a Kafka topic can always be increased, but never decreased. This is because reducing partitions could lead to data loss.</li> <li>Partition Reassignment Use Cases - Partition reassignment is used in several scenarios. Moving a partition across different brokers. Rebalancing the replicas of a partition to a specific set of brokers. Increasing the replication factor of a topic.</li> </ul>"},{"location":"kafka/overview/#replications","title":"Replications","text":"<p>Replicas are essentially backups of partitions. They are not directly read as raw data. Their primary purpose is to prevent data loss and provide fault tolerance. If the server hosting an active partition fails, a replica can take over.</p> <p>One broker is marked leader and other brokers are called followers for a specific partition. This designated broker assumes the role of the leader for the topic partition. On the other hand, any additional broker that keeps track of the leader partition is called a follower and it stores replicated data for that partition.</p> <p>Tip</p> <p>Note that the leader receives and serves all incoming messages from producers and serves them to consumers. Followers do not serve read or write requests directly from producers or consumers. Followers just act as backups and can take over as the leader in case the current leader fails.</p> <p>--- In-Sync Replicas (ISR)</p> <p>When a partition is replicated across multiple brokers, not all replicas are necessarily in sync with the leader at all times. The in-sync replicas represent the number of replicas that are always up-to-date and synchronized with the partition\u2019s leader. The leader continuously sends messages to the in-sync replicas, and they acknowledge the receipt of those messages.</p> <p>The recommended value for ISR is always greater than 1.</p> <p>Tip</p> <p>The ideal value of ISR is equal to the replication factor.</p>"},{"location":"kafka/overview/#offsets","title":"Offsets","text":"<p>Offsets represent the position of each message within a partition and are uniquely identifiable, ever-increasing integers . There are three main variations of offsets :</p> <ul> <li>Log End Offset: This refers to the offset of the last message written to any given partition .</li> <li>Current Offset: This is a pointer to the last record that Kafka has already sent to the consumer in the current poll .</li> <li>Committed Offset: This indicates the offset of a message that a consumer has successfully consumed .</li> <li>Relationship: The committed offset is typically less than the current offset .</li> </ul> <p>In Apache Kafka, consumer offset management \u2013 that is, tracking what messages have been consumed \u2013 is handled by Kafka itself.</p> <p>When a consumer in a consumer group reads a message from a partition, it commits the offset of that message back to Kafka. This allows Kafka to keep track of what has been consumed, and what messages should be delivered if a new consumer starts consuming, or an existing consumer restarts.</p> <p>Earlier versions of Kafka used Apache ZooKeeper for offset tracking, but since version 0.9, Kafka uses an internal topic named \"__consumer_offsets\" to manage these offsets. This change has helped to improve scalability and durability of consumer offsets.</p> <p>Kafka maintains two types of offsets:</p> <ul> <li> <p>Current Offset: The current offset is a reference to the most recent record that Kafka has already provided to a consumer. As a result of the current offset, the consumer does not receive the same record twice.</p> </li> <li> <p>Committed Offset: The committed offset is a pointer to the last record that a consumer has successfully processed. We work with the committed offset in case of any failure in application or replaying from a certain point in event stream.</p> </li> </ul> <p>--- Committing an offset</p> <ul> <li> <p>Auto Commit: By default, the consumer is configured to use an automatic commit policy, which triggers a commit on a periodic interval. This feature is controlled by setting two properties:  enable.auto.commit &amp; auto.commit.interval.ms</p> <p>Although auto-commit is a helpful feature, it may result in duplicate data being processed Let\u2019s have a look at an example. You\u2019ve got some messages in the partition, and you\u2019ve requested your first poll. Because you received ten messages, the consumer raises the current offset to ten. You process these ten messages and initiate a new call in four seconds. Since five seconds have not passed yet, the consumer will not commit the offset. Then again, you\u2019ve got a new batch of records, and rebalancing has been triggered for some reason. The first ten records have already been processed, but nothing has yet been committed. Right? The rebalancing process has begun. As a result, the partition is assigned to a different consumer. Because we don\u2019t have a committed offset, the new partition owner should begin reading from the beginning and process the first ten entries all over again. A manual commit is the solution to this particular situation. As a result, we may turn off auto-commit and manually commit the records after processing them.</p> </li> <li> <p>Manual Commit: With Manual Commits, you take the control in your hands as to what offset you\u2019ll commit and when. You can enable manual commit by setting the enable.auto.commit property to false. There are two ways to implement manual commits :</p> <ol> <li>Commit Sync: The synchronous commit method is simple and dependable, but it is a blocking mechanism. It will pause your call while it completes a commit process, and if there are any recoverable mistakes, it will retry. Kafka Consumer API provides this as a prebuilt method.</li> <li>Commit Async: The request will be sent and the process will continue if you use asynchronous commit. The disadvantage is that commitAsync does not attempt to retry. However, there is a legitimate justification for such behavior. Let\u2019s have a look at an example. Assume you\u2019re attempting to commit an offset as 70. It failed for whatever reason that can be fixed, and you wish to try again in a few seconds. Because this was an asynchronous request, you launched another commit without realizing your prior commit was still waiting. It\u2019s time to commit-100 this time. Commit-100 is successful, however commit-75 is awaiting a retry. Now how would we handle this? Since you don\u2019t want an older offset to be committed. This could cause issues. As a result, they created asynchronous commit to avoid retrying. This behavior, however, is unproblematic since you know that if one commit fails for a reason that can be recovered, the following higher level commit will succeed.</li> </ol> </li> </ul> <p>-- What if AsyncCommit failure is non-retryable?</p> <p>Asynchronous commits can fail for a variety of reasons. For example, the Kafka broker might be temporarily down, the consumer may be considered dead by the group coordinator and kicked out of the group, the committed offset may be larger than the last offset the broker has, and so on.</p> <p>When the commit fails with a non-retryable error, the commitAsync method doesn't retry the commit, and your application doesn't get a direct notification about it, because it runs in the background. However, you can provide a callback function that gets triggered upon a commit failure or success, which can log the error and you can take appropriate actions based on it.</p> <p>But keep in mind, even if you handle the error in the callback, the commit has failed and it's not retried, which means the consumer offset hasn't been updated in Kafka. The consumer will continue to consume messages from the failed offset. In such scenarios, manual intervention or alerts might be necessary to identify the root cause and resolve the issue.</p> <p>On the other hand, synchronous commits (commitSync) will retry indefinitely until the commit succeeds or encounters a non-retryable failure, at which point it throws an exception that your application can catch and handle directly. This is why it's often recommended to have a final synchronous commit when you're done consuming messages.</p> <p>As a general strategy, it's crucial to monitor your consumers and Kafka infrastructure for such failures and handle them appropriately to ensure smooth data processing and prevent data loss or duplication.</p> <p>--- When to use SyncCommit vs AsyncCommit?</p> <p>Choosing between synchronous and asynchronous commit in Apache Kafka largely depends on your application's requirements around data reliability and processing efficiency.</p> <p>Here are some factors to consider when deciding between synchronous and asynchronous commit:</p> <ul> <li> <p>Synchronous commit (commitSync): Use it when data reliability is critical, as it retries indefinitely until successful or a fatal error occurs. However, it can block your consumer, slowing down processing speed.</p> </li> <li> <p>Asynchronous commit (commitAsync): Use it when processing speed is important and some data loss is tolerable. It doesn't block your consumer but doesn't retry upon failures.</p> </li> </ul> <p>Combination: Many applications use commitAsync for regular commits and commitSync before shutting down to ensure the final offset is committed. This approach balances speed and reliability.</p> <p>--- What is Out-of-Order Commit?</p> <p>Normally, you might expect offsets to be committed sequentially (e.g., commit for message 1, then 2, then 3, and so on). However, Kafka's design allows for out-of-order commits, meaning a consumer can commit a later offset even if earlier messages in the sequence haven't been explicitly committed.</p> <p>Consider a Kafka topic with messages 1, 2, 3, 4, 5, 6, etc..</p> <ul> <li>A consumer polls messages and receives 1, 2, 3, and 4.</li> <li>However, for some reason, the consumer only commits the offset for message 4 to the <code>__consumer_offset</code> topic. It does not send explicit commits for messages 1, 2, or 3.</li> <li>Then, the consumer goes down.</li> </ul> <p>When the consumer spins up again, a crucial question arises: Will the Kafka broker re-send messages 1, 2, and 3 (for which no explicit commit was received), or will it start from message 5?</p> <p>The Kafka broker will not re-send messages 1, 2, or 3.</p> <ul> <li>The broker simply checks the <code>__consumer_offset</code> topic for the latest committed offset for that particular consumer group and topic.</li> <li>In our scenario, the latest committed offset is for message 4 (which means the next message to read is 5).</li> <li>The broker will then start sending messages from message 5 onwards (i.e., 5, 6, 7, etc.).</li> <li>Kafka assumes that all messages prior to the latest committed offset have been successfully processed, even if individual commits for those messages were not received. This committed offset acts like a \"bookmark\".</li> </ul> <p>This behavior is termed \"out-of-order commit\" because, ideally, commits should be sequential (1, then 2, then 3, then 4). However, in this scenario, a commit for message 4 is received directly, without commits for messages 1, 2, or 3.</p> <p>--- Advantages of Out-of-Order Commit</p> <p>The primary advantage of out-of-order commit is reduced overhead.</p> <ul> <li>Committing an offset for every single message individually can produce a lot of overhead, as it's a complex operation.</li> <li>Instead, consumers can consume a batch of messages (e.g., 1, 2, 3, 4).</li> <li>After processing the entire batch, the consumer only needs to commit the offset of the last message in that batch (e.g., message 4).</li> <li>This way, the entire batch is effectively acknowledged, and the broker will not re-send any messages within that batch, understanding them as successfully processed. This significantly improves efficiency by reducing the number of commit operations.</li> </ul> <p>--- Disadvantages of Out-of-Order Commit</p> <p>While efficient, out-of-order commit has a significant disadvantage: potential message loss.</p> <ul> <li>Imagine a scenario where a consumer processes messages using multiple threads or a complex backend system.</li> <li>If messages 1, 2, and 3 fail during processing, but message 4 (which was processed by a separate, successful thread) is committed.</li> <li>Even though messages 1, 2, and 3 failed, because message 4's offset was committed, the broker will not re-send those failed messages when the consumer restarts.</li> <li>This can lead to data loss or inconsistent processing if not handled carefully at the application level.</li> </ul>"},{"location":"kafka/overview/#kafka-log-segments","title":"Kafka Log Segments","text":"<p>Kafka Log Segments are a powerful mechanism that allows Kafka to efficiently manage and store vast amounts of streaming data. By breaking down large logs into smaller, configurable segments, Kafka ensures high performance, manageability, and robust data retention policies.</p> <p>All messages published by producers to a Kafka topic are stored within Kafka logs.</p> <p>These logs are the primary location where messages reside, playing a vital role in enabling communication between producers and consumers via the Kafka cluster.</p> <p>Traditionally, one might imagine all messages for a topic's partition being stored in a single, ever-growing log file. However, Kafka takes a more efficient approach:</p> <p>Instead of creating one single, large log file for a particular partition, Kafka creates several smaller files to store all messages. These small, individual files within a partition on a server are called segments.</p> <p>--- Why Segments?</p> <p>Imagine a very large book that keeps growing infinitely. If you needed to find a specific page, or if the book became corrupted, managing one massive file would be incredibly difficult and inefficient.</p> <p>Kafka segments address this by:</p> <ul> <li>Managing Large Volumes of Data: By breaking down a single massive log into smaller, manageable segments, Kafka can handle terabytes or petabytes of data more effectively.</li> <li>Efficient Retention Policies: Older segments can be easily deleted or archived without affecting the active segments where new messages are being appended.</li> <li>Improved Recovery: In case of corruption or failure, smaller segments are faster to recover or replicate.</li> </ul> <p>--- How New Segments are Created?</p> <p>Messages are continuously appended to the currently active log segment in a given partition. Kafka is configured with a maximum size limit for each log segment file. Once the current segment file reaches this configured size (in bytes), Kafka automatically creates a new, empty log segment file for subsequent messages. This ensures that no single log file becomes excessively large.</p> <p>--- Why Segmentation</p> <p>Kafka doesn't write all messages into a single, ever-growing log file. This would become unwieldy and inefficient for operations like deletion or replication. Instead, Kafka divides its log files into multiple smaller segments.</p> <p>This segmentation is controlled by the <code>log.segment.bytes</code> property. When a log segment reaches a configured size limit (e.g., 2000 bytes as set in a demo), Kafka closes the current segment and starts a new one. Each segment has its own <code>.log</code>, <code>.index</code>, and <code>.timeindex</code> files.</p> <p>A key pattern to observe in Kafka's segmented logs is how the files are named.</p> <p>Each log file, its corresponding <code>.index</code> file, and <code>.timeindex</code> file within a partition directory will share a common name prefix. This prefix is actually the starting offset of the first message contained within that log segment.</p> <p>Example</p> <p><code>00000000000000000000.log</code> indicates the segment starts from <code>offset 0</code>.</p> <p><code>00000000000000000027.log</code> indicates the segment starts from <code>offset 27</code>.</p> <p><code>00000000000000000090.log</code> indicates the segment starts from <code>offset 90</code>.</p> <p>This naming convention is crucial for quickly identifying which segment contains a particular message.</p> <p>--- How Lookup Works with Multiple Segments</p> <p>When a consumer requests a message by offset in a multi-segment environment, Kafka follows a three-step process:</p> <ul> <li>Locate the Segment File (by filename): The Kafka broker first determines which log segment contains the requested offset. It does this by checking the file names in the partition directory. Since file names indicate the starting offset of each segment, the broker can quickly identify the correct <code>.log</code> file without opening any files. For example, if <code>offset 100</code> is requested, the broker knows it must be in the <code>00000000000000000090.log</code> file because messages start from <code>offset 90</code> in this segment, and the next segment starts from <code>offset 109</code>.</li> <li>Lookup in the Segment's <code>.index</code> File: Once the correct log segment (<code>.log</code> file) is identified, the broker then goes to its corresponding <code>.index</code> file (e.g., <code>00000000000000000090.index</code>). It performs a binary search within this specific <code>.index</code> file to find the nearest offset and its byte <code>position</code>.</li> <li>Scan within the Segment's <code>.log</code> File: Finally, with the approximate byte <code>position</code> from the index file, the broker navigates to that position within the actual <code>.log</code> file and starts scanning from there to find the exact message(s) requested.</li> </ul> <p>This multi-step approach ensures that even with hundreds or thousands of gigabytes of messages, Kafka can locate any message with minimal disk I/O and latency.</p> <p>--- Dumping and Reading Contents of Log, Index, or TimeIndex Files</p> <p>This command helps you view the structured content of these binary files.</p> <p>Note</p> <p>kafka-run-class.bat kafka.tools.DumpLogSegments --files [path_to_log_file.log] --print-data-log</p> <p>Example for .log file:   kafka-run-class.bat kafka.tools.DumpLogSegments --files C:\\kafka\\kafka-logs\\my-topic-0\\00000000000000000000.log --print-data-log</p> <p>Example for .index file:   kafka-run-class.bat kafka.tools.DumpLogSegments --files C:\\kafka\\kafka-logs\\my-topic-0\\00000000000000000000.index --print-data-log</p> <p>Example for .timeindex file:   kafka-run-class.bat kafka.tools.DumpLogSegments --files C:\\kafka\\kafka-logs\\my-topic-0\\00000000000000000000.timeindex --print-data-log</p>"},{"location":"kafka/overview/#producers","title":"Producers","text":"<p>Producers are applications that write or publish data to the topics within a Kafka cluster . They use the Producer API to send data . Producers can choose to write data either at the topic level (letting Kafka distribute it across partitions) or to specific partitions of a topic.</p> <p>--- Producer Configurations</p> <p>When configuring a Kafka producer, several important settings are available:</p> <ul> <li>Bootstrap Servers:    Used to connect to the Kafka Cluster.    This specifies the host/port for establishing the initial connection.</li> <li>Client ID:    Used to track requests and is mainly for debugging purposes.    Primarily used for server-side logging. -. Key Serializer (and Value Serializer):    Converts the key/value into a stream of bytes.    Kafka producers send objects as a stream of bytes, and this setting is used to persist and transmit the object across the network.</li> <li>Connection Max Idle Ms:    Specifies the maximum number of milliseconds for an idle connection.    After this period, if the producer sends to the broker, it will use a disconnected connection.</li> <li> <p>Acks (Acknowledgements):    Determines the acknowledgement behavior when a producer sends records.</p> <p>Acks = 0: Producers will not wait for any acknowledgement from the server.  </p> <p></p> <p>The producer does not wait for any reply from the broker after sending a message. It assumes the message is successfully written immediately.This is the riskiest approach regarding data loss. If the broker goes offline, an exception occurs, or the message is simply not received due to network issues, the producer will not be aware of it, and the message will be lost. Offers the lowest latency because there's no waiting period for acknowledgements.</p> <p>Acks = 1 (Default): The leader broker will write the record to its local log and respond without waiting for full acknowledgement from all followers. In this case, if the leader fails immediately after acknowledgement (before followers replicate), the record will be lost.</p> <p></p> <p>The producer waits for a success response from the broker only when the leader partition has received the message. Once the leader has written the message, it sends an acknowledgment back to the producer, allowing the producer to proceed. Improves data safety compared to <code>acks=0</code>. If the message cannot be written to the leader (e.g., leader crashes), the producer receives an error and can retry sending the message, thus avoiding potential data loss. While better, <code>acks=1</code> does not guarantee that the message has been replicated to other in-sync replicas. If the leader partition fails after acknowledging the message to the producer but before the message is replicated to its followers, the message could still be lost.</p> <p>Acks = -1: The leader will wait for the full set of in-sync replicas to acknowledge the record. This guarantees that the record will not be lost as long as at least one in-sync replica remains alive. This provides the strongest guarantee and is equivalent to acks=all.</p> <p> </p> <p>The producer receives a success response from the broker only when the message has been successfully written not only to the leader partition but also to all configured in-sync replicas (ISRs). The leader receives the message and writes it. The leader then sends the message to all its in-sync follower replicas. Once all in-sync replicas confirm they have written the message, they send acknowledgements back to the leader. Only after the leader accumulates acknowledgements from all in-sync replicas, does it send the final success response to the producer. This approach offers the highest level of data safety and guarantees against message loss. The chance of message loss is very low. The primary drawback is added latency. The producer has to wait longer for the message to be fully replicated across all replicas before it gets the final acknowledgment. This makes it a slower process compared to <code>acks=0</code> or <code>acks=1</code>.</p> </li> <li> <p>Compression Type:    Used to compress the messages.    The default value is \"none\".    Supported types include gzip, snappy, lz4, and zstd.    Compression is particularly useful for batches of data, and the efficiency of batching also affects the compression ratio.</p> </li> <li> <p>Max Request Size:    Defines the maximum size of a request in bytes.    This setting impacts the number of record batches the producer will send in a single request.    It is used for sending large requests and also affects the cap on the maximum compressed record\u00a0batch\u00a0size\u00a0.</p> </li> <li> <p>Batching:    Producer Batching: The producer collects records together and sends them in a single batch. This approach aims to reduce network latency and CPU load, thereby improving overall performance.</p> </li> <li> <p>Batch Size Configuration:    The size of a batch is determined by a configuration parameter called batch.size, with a default value of 16KB. Each partition will contain multiple records within a batch. Importantly, all records within a single batch will be sent to the same topic and partition.</p> </li> <li> <p>linger.ms:    This parameter specifies a duration the producer will wait to allow more records to accumulate and be sent together in a batch, further optimizing the sending process.    Memory Usage: Configuring larger batches may lead the producer to use more memory. There's also a concept of \"pre-allocation\" where a specific batch size is anticipated for additional records.</p> </li> <li> <p>Buffer Memory:    Producers use buffer memory to facilitate batching records.    Records ranging from 1MB to 2MB can be delivered to the server.    If the buffer memory becomes full, the producer will block for a duration specified by max.block.ms until memory becomes available. This max.block.ms represents the maximum time the producer will wait.    It is crucial to compare this max.block.ms setting with the total memory the producer is configured to use.    It's noted that not all producer memory is exclusively used for batching; some additional memory is allocated for compression and insight requests.</p> </li> </ul> <p>--- How Keys Determine Partition Assignment</p> <p>Kafka uses the message key to decide which partition an incoming message will be written to.</p> <ul> <li> <p>With a Key (Hashing Algorithm):</p> <p>When a key is provided, Kafka applies a hashing algorithm to the key.    The output of the hashing function (which is based on the key) is then mapped to a specific partition out of the topic's available partitions.</p> <p> </p> <p>Example</p> <p>If the hashing concept is \"divide by three and use the remainder,\" a key of <code>7</code> would result in a remainder of <code>1</code>, so the message would go to partition 1. A key of <code>5</code> would result in a remainder of <code>2</code>, so that message would go to partition 2.</p> <p>All messages that share the same key will always go to the same partition. This is because the same key will consistently produce the same hash output, leading to the same partition assignment. This property is vital for maintaining message order for a specific logical entity (e.g., all events related to a particular user ID).</p> <p>Tip</p> <p>It's possible for different keys to end up in the same partition if their hash outputs happen to be the same, but messages with an identical key will always map to the same partition.</p> </li> <li> <p>Without a Key (Null Key - Round Robin):</p> <p>     If the message key is <code>null</code> (i.e., no key is provided), Kafka uses a round-robin fashion to distribute messages among partitions.    This means the first message goes to partition 0, the second to partition 1, the third to partition 2, and then it cycles back to partition 0, and so on. This ensures an even distribution of messages when order per key is not a concern.</p> </li> </ul> <p>--- When a producer sends messages in Kafka, the process involves</p> <p>When an application produces a message using a Kafka API (e.g., Java or Python API), the record goes through a series of internal steps before being sent to the Kafka cluster.</p> <p></p> <p>An application sends a \"record\". A record typically contains a topic (where the record should go), an optional partition (though rarely specified directly by the user), a key, and the value (the actual message content). The key is primarily used for calculating the target partition.</p> <ul> <li> <p>Step 1: Serialization</p> <p>The first thing the producer does is serialization for both the key and value.</p> <p>Purpose: Messages need to be sent over a network, which requires them to be converted into a binary format (a stream of zeros and ones or a byte array). The serializer handles this conversion from an object (your message data) to a byte array.</p> </li> <li> <p>Step 2: Partitioning</p> <p>After serialization, the binary data is sent to the partitioner.   At this stage, the producer determines to which partition of the topic the record will be sent.</p> <p>If a key is provided: A hashing algorithm is applied to the key. The output of this hash is then typically divided by the total number of partitions for the topic, and the remainder determines the destination partition.   Example: If a key <code>7</code> is hashed and then processed with \"divide by three and use the remainder\", it might map to partition <code>1</code>. A key <code>5</code> might map to partition <code>2</code> [based on video explanation in previous context]. The key ensures that messages with the same key consistently go to the same partition.   If no key is provided (null key): Messages are distributed in a round-robin fashion across all available partitions, ensuring an even distribution.</p> </li> <li> <p>Step 3: Buffering</p> <p>Once the destination partition for a record is determined, the record is not immediately sent to the Kafka cluster.   Instead, it is written into an internal buffer specifically assigned to that partition. Buffers accumulate multiple messages.</p> <p>This accumulation allows the producer to Perform I/O operations more efficiently. Sending many small messages individually is less efficient than sending them in larger chunks.Apply compression more effectively. Compression algorithms often work better with larger blocks of data, as they can identify more patterns and redundancies. The producer aims to \"patch\" (batch) records for this efficiency.</p> </li> <li> <p>Step 4: Batching</p> <p>From the internal buffer, multiple messages are \"clubbed\" together into batches. These batches are then sent to the Kafka cluster.</p> <p>Two important configuration parameters control when a batch is sent:</p> <p><code>linger.ms</code>: This parameter instructs the producer to wait up to a certain number of milliseconds (e.g., 5 milliseconds) before sending the accumulated content. This allows more messages to collect in the buffer, potentially forming a larger, more efficient batch.</p> <p><code>batch.size</code>: This parameter defines the maximum size (in bytes) that a batch can reach (e.g., 5 MB).</p> <p>A batch is sent to the Kafka cluster when either the <code>linger.ms</code> timeout expires OR the <code>batch.size</code> limit is reached, whichever condition is satisfied first.</p> </li> <li> <p>Step 5: Sending to Kafka Cluster</p> <p>Once a batch is formed based on <code>linger.ms</code> or <code>batch.size</code> conditions, the complete batch is sent to the Kafka cluster.</p> </li> <li> <p>Step 6: Retries</p> <p>It's possible for message writing to fail due to networking issues or other problems.   Kafka producers can be configured to retry sending a message if the initial attempt fails.   You can set the number of retries (e.g., <code>retry=5</code>), meaning the producer will attempt to rewrite the message up to five times before throwing an exception.</p> </li> <li> <p>Step 7: Receiving Record Metadata</p> <p>If the message writing is successful (after retries, if any), the Kafka cluster sends <code>RecordMetadata</code> back to the producing application.</p> <p>This <code>RecordMetadata</code> provides crucial information about the successfully written record, including:   Partition: The specific partition where the data was written.   Offset: The offset (position) of the record within that partition.   Timestamp: The time when the record arrived or was written to Kafka.</p> </li> </ul> <p>--- Importance of Understanding Producer Internals</p> <p>Having a clear understanding of these internal mechanisms is vital for:</p> <ul> <li> <p>Troubleshooting: For instance, if messages are being produced at a very high rate and exceeding the producer's internal buffer volume (e.g., 32 MB), messages might not be written to Kafka. Knowing this allows you to increase the buffer volume to accommodate the incoming message flow.</p> </li> <li> <p>Performance Tuning: Properly configuring parameters like <code>linger.ms</code> and <code>batch.size</code> can significantly impact throughput and latency.</p> </li> <li> <p>Reliability: Understanding how retries work helps in building resilient applications that can handle transient failures.</p> </li> </ul> <p>--- Producer Corner Cases in Kafka Tuning (Replica Management)</p> <ul> <li>Replica Fetchers: This setting determines the number of threads responsible for replicating data from leaders. It's crucial to have a sufficient number of replica fetchers to enable complete parallel replication if multiple threads are available.</li> <li>replica.fetch.max.bytes: This parameter dictates the maximum amount of data used to fetch from any partition in each fetch request. It is generally beneficial to increase this parameter.</li> <li>replica.socket.receive.buffer.bytes: The size of buffers can be increased, especially if more threads are available.</li> <li>Creating Replicas: Increasing the level of parallelism allows for data to be written in parallel, which automatically leads to an increase in throughput.</li> <li>num.io.threads: This parameter is determined by the amount of disk available in the cluster and directly influences the value for I/O\u00a0threads\u00a0.</li> </ul> <p>--- Experiment Scenario</p> <p>A Kafka producer is configured with default settings, including <code>linger.ms=0</code> and <code>buffer.memory=32MB</code> (which is sufficient for small messages). The producer attempts to send 1000 messages (numbered 0 to 999) rapidly in a loop.</p> <p>The consumer only receives messages up to number <code>997</code>, and the Kafka logs confirm messages only up to <code>997</code> were written. Messages <code>998</code> and <code>999</code> are lost.</p> <p>The messages <code>998</code> and <code>999</code> were still present in the Kafka producer's internal buffer when the Python code finished execution (<code>Process finished with exit code 0</code>). The I/O thread did not get enough time to take these remaining messages from the buffer and publish them to the Kafka cluster before the application terminated.</p> <p>To ensure all messages are delivered, even when sending rapidly, you must explicitly tell the producer to flush its buffer before exiting or closing the connection. This is done using <code>producer.flush()</code> and <code>producer.close()</code> methods.</p> <p><code>producer.flush()</code>: This method explicitly flushes all accumulated messages from the producer's buffer to the Kafka cluster. It blocks until all messages in the buffer have been successfully sent and acknowledged by the brokers.</p> <p><code>producer.close()</code>: This method closes the producer connection, releasing any resources it holds. It implicitly calls <code>flush()</code> before closing, but it's often good practice to call <code>flush()</code> explicitly beforehand, especially if there's any risk of the <code>close()</code> method being interrupted.</p>"},{"location":"kafka/overview/#handling-producer-failures","title":"Handling Producer Failures","text":"<p>What happens if the I/O thread fails to write a batch to the Kafka cluster? Kafka producers can be configured to retry sending the failed batches.</p> <p></p> <p>--- Configuring Retries</p> <p>When creating the producer, you can specify whether to enable retries and for how long Kafka should attempt to rewrite a failed batch. <code>delivery.timeout.ms</code>: This parameter configures the maximum time (in milliseconds) Kafka will attempt to write a message or retry writing if it fails initially.</p> <p>--- Error Handling</p> <ul> <li> <p>If Retries are Not Enabled - If the initial write fails and retries are disabled, the producer immediately invokes an error handler. On the client side, you will receive an error indicating that the messages were not written successfully. You would then need to handle this manually, perhaps by resending the messages from your application.</p> </li> <li> <p>If Retries are Enabled - Kafka will attempt to retry writing the batch. If the <code>delivery.timeout.ms</code> is exceeded during the retry attempts, the process will again go to the error handler, and an error will be reported to the client. If the timeout is not over, Kafka will continue to retry writing the batch.</p> </li> </ul> <p>--- Ensuring Exactly-Once Semantics: Idempotent Producer</p> <p>While retries are essential for reliability, they can introduce a new problem: message duplication. An idempotent producer helps solve this.</p> <ul> <li> <p>The Duplication Problem</p> <p>A batch of messages is sent to Kafka.   The batch is successfully written to the Kafka cluster.   However, the acknowledgement (ACK) or return response from Kafka back to the producer is lost or fails to be sent.   The producer, not having received the ACK, assumes the batch failed to write.   Consequently, the producer resends the exact same batch of messages to the Kafka cluster.   Without idempotence enabled, Kafka would simply write this \"new\" batch again, leading to duplicate messages in the topic.</p> </li> <li> <p>How Idempotence Works</p> <p>An idempotent producer prevents this duplication.</p> <p><code>enable.idempotence</code>: When this parameter is set to <code>true</code> (enabled), Kafka performs a duplicate check before writing any incoming batch.   Duplicate Detection: If Kafka detects that an incoming batch is a duplicate (meaning it has already been successfully written), it intelligently understands that the producer likely didn't receive the previous acknowledgement.   Action for Duplicates: Instead of rewriting the batch and causing duplication, Kafka's cluster will re-send the acknowledgement for that particular batch to the producer.   Outcome: The producer receives the ACK, understands the batch was successful, and avoids resending, thus avoiding duplication.</p> <p>To avoid duplicacy in scenarios where ACKs might be lost, you should enable idempotence in your Kafka producer configuration.</p> </li> </ul> <p>--- Maintaining Message Order</p> <p>Even with retries and idempotence, another issue can arise: out-of-order messages within a single partition.</p> <p>Kafka guarantees message ordering within a single partition. However, with multiple \"in-flight\" requests, this guarantee can be compromised during retries:</p> <p>Consider a scenario where three batches (Batch 1, Batch 2, Batch 3) are sent sequentially to the same partition:</p> <ul> <li>Batch 1 is sent and successfully written.</li> <li>Batch 2 is sent but fails to write initially. A retry operation for Batch 2 is initiated.</li> <li>While Batch 2 is undergoing retry (which takes some time), the I/O thread proceeds to send Batch 3, which is then successfully written to the partition before Batch 2's retry completes.</li> <li>Finally, Batch 2's retry succeeds, and it is written to the partition.</li> </ul> <p>Actual order of sending: Batch 1, Batch 2, Batch 3. Resulting order in the partition: Batch 1, Batch 3, Batch 2.</p> <p>This \"screws up\" the intended message order within the partition, which is generally undesirable for many applications.</p> <ul> <li> <p>Understanding <code>in-flight</code> Requests</p> <p>An <code>in-flight request</code> refers to a request that has been sent by the producer but has not yet received a completion response or acknowledgement.   The <code>max.in.flight.requests.per.connection</code> parameter determines the maximum number of requests that can be \"in-flight\" (sent but not yet acknowledged) at any given time for a particular connection. If this is set to, say, <code>5</code>, the producer can send up to five requests concurrently without waiting for any of them to complete.</p> <p>Setting <code>max.in.flight.requests.per.connection</code> to 1</p> <p>To guarantee strict message ordering within a partition, you can set <code>max.in.flight.requests.per.connection = 1</code>.</p> <p>By setting this to <code>1</code>, the producer will send only one batch at a time and wait for its completion (i.e., receipt of the acknowledgement, even after retries) before sending the next batch.</p> <p>If a batch fails and requires a retry, no other batches will be sent to the cluster until that specific batch (and its retries) are fully completed. This ensures that messages are always written in their original sequential order within the partition.</p> <p>While <code>max.in.flight.requests.per.connection = 1</code> ensures strict ordering, it comes at a cost:</p> <p>It effectively makes the message writing operation synchronous, meaning throughput will be reduced because the producer waits for each batch to complete before moving to the next. This is a trade-off between strict ordering and high throughput.</p> </li> </ul> <p>--- Ways to send messages to kafka</p> <ul> <li> <p>Method 1: Fire and Forget</p> <p>The \"Fire and Forget\" method is the simplest way to send messages.</p> <p>In this technique, the producer sends a message to the Kafka server and does not wait for any acknowledgment or confirmation about its successful arrival. The producer simply \"fires\" the message and \"forgets\" about it.</p> <p>Kafka is a highly reliable system, and Kafka clusters are generally highly available. The producer also automatically retries sending messages if the first attempt fails. Therefore, most of the time (99.9%), messages will arrive successfully.</p> <p>Imagine sending a letter by mail without requesting a delivery confirmation. You assume it will arrive because the postal service is generally reliable.</p> <p>Messages are sent immediately without any delay for waiting on responses. You can send a large number of messages in a very short amount of time.   If something goes wrong (e.g., the Kafka broker goes down), the producer will not be aware that the messages failed to reach the cluster. These messages will be permanently lost. \"The client is not even getting any information that whether really the message is written or not\".</p> </li> <li> <p>Method 2: Synchronous Send</p> <p>The \"Synchronous Send\" method ensures that the producer receives a confirmation for each message sent before proceeding to the next.</p> <p>After sending a message, the producer waits for a response (acknowledgment or error) from the Kafka cluster before sending the next message. This makes the entire sending process sequential.</p> <p>This is like waiting in a queue to get a movie ticket. You cannot get your ticket until the person in front of you has successfully received theirs.</p> <p>The <code>producer.send()</code> method returns a <code>Future</code> object. By calling the <code>.get()</code> method on this <code>Future</code> object, the producer blocks and waits for the operation to complete. You can specify a <code>timeout</code> for how long to wait.</p> <p>If successful, the <code>get()</code> method returns <code>RecordMetadata</code> containing information like the topic name, partition number, and offset where the message was published. If it fails, an exception is raised.</p> <p>The producer knows whether each message was successfully written or if an error occurred. This allows for robust error handling and logging on the client side.</p> <p>Very slow. This method makes the overall message sending process significantly slower because each message requires a full \"round trip\" to the Kafka cluster and back. For example, if the network round-trip time is 10 milliseconds, sending 100 messages would take at least 1 second (100 messages  10 ms/message).   \"This is kind of making the whole process little bit slow\".</p> </li> <li> <p>Method 3: Asynchronous Send (with Callbacks)</p> <p>The Asynchronous Send method with callbacks offers a middle ground, combining the speed of \"Fire and Forget\" with the error-handling capabilities of \"Synchronous Send\".</p> <p>The producer sends messages rapidly without waiting for an immediate response, similar to \"Fire and Forget.\" However, it attaches callback functions that will be invoked automatically in the background once the message delivery operation (success or failure) is complete.</p> <p>When <code>producer.send()</code> is called, it still returns a <code>Future</code> object. Instead of calling <code>.get()</code>, you use <code>.add_callback()</code> to register a function to be called on success, and <code>.add_errback()</code> to register a function to be called on failure. These callback functions receive information about the success (e.g., <code>RecordMetadata</code>) or failure (e.g., exception).</p> <p>Messages are sent very quickly, as the producer does not wait for a response for each message before sending the next. This is \"ultra fast\" compared to synchronous sending. Despite sending rapidly, the producer is still notified of message delivery status (success or failure) through the callback functions. This overcomes the main drawback of the \"Fire and Forget\" method. This technique is widely followed in real-world Kafka applications due to its balance of speed and reliability.</p> </li> <li> <p>Choosing the Right Method</p> <p>Fire and Forget: Use when maximum throughput is critical, and occasional message loss is acceptable or can be handled by downstream systems (e.g., logging, metrics, real-time analytics where exact message counts aren't critical).</p> <p>Synchronous Send: Use when guaranteed per-message delivery status is paramount, and you can tolerate lower throughput due to the blocking nature. This might be suitable for low-volume, high-value data where immediate confirmation is essential.</p> <p>Asynchronous Send (with Callbacks): This is generally the most recommended approach for most production scenarios. It offers a good balance of high throughput and reliable error handling, providing notifications without blocking the main sending thread.</p> </li> </ul> <p>--- The Challenge of key=null</p> <p>When a Kafka producer sends messages to a topic, it typically uses a message key to determine which partition the message should go to. Messages with the same key are guaranteed to land in the same partition, ensuring ordered processing for those related messages.</p> <p>However, a common scenario arises when no key is provided (the <code>key</code> is <code>null</code>). In this situation, Kafka needs a strategy to distribute these messages across the available partitions. The video discusses how this distribution happens and how Kafka has optimized this process over time.</p> <ul> <li> <p>Approach 1: Simple Round-Robin Distribution</p> <p>The simplest way to distribute messages when <code>key=null</code> is using a round-robin fashion.</p> <p>The first message is published to Partition 0 (P0).   The second message goes to Partition 1 (P1).   The third message goes to Partition 2 (P2), and so on.   Once all partitions have received a message, the distribution cycles back to Partition 0.</p> <p>Example: Let's assume a Kafka topic has 5 partitions (P0, P1, P2, P3, P4) and messages <code>1, 2, 3, 4, 5, 6</code> are being produced.</p> <p>Message <code>1</code> goes to P0.   Message <code>2</code> goes to P1.   Message <code>3</code> goes to P2.   Message <code>4</code> goes to P3.   Message <code>5</code> goes to P4.   Message <code>6</code> goes back to P0.</p> <p>Each individual message is published to a different partition. This constant switching between partitions adds overhead and consumes more time.   The Kafka broker, which is responsible for receiving and storing messages, has to individually send each message to a potentially different partition. This task is CPU-intensive for the broker, which also needs to handle many other important activities.</p> </li> <li> <p>Approach 2: Optimized Sticky Partitioning</p> <p>To overcome the drawbacks of round-robin distribution, Kafka introduced an optimized approach known as Sticky Partitioning. This method focuses on efficiency by leveraging Kafka producer's internal batching mechanism.</p> <p>When the message <code>key</code> is <code>null</code>, Kafka optimizes distribution by publishing all messages within a particular batch to a single partition.</p> <ol> <li>The producer creates a batch of messages.</li> <li>This entire batch is published to one partition (e.g., P0).</li> <li>The next batch created by the producer will then be published to the next partition (e.g., P1), and so on, in a round-robin like fashion for the batches, not individual messages.</li> </ol> <p>Example:   Using the same 5 partitions (P0, P1, P2, P3, P4) and messages <code>1, 2, 3, 4, 5, 6</code>.   Suppose the producer's internal batching accumulates messages <code>1, 2, 3</code> into the first batch, and <code>4, 5, 6</code> into the second batch.</p> <ol> <li>Batch 1 (<code>1, 2, 3</code>) goes to P0.</li> <li>Batch 2 (<code>4, 5, 6</code>) goes to P1.</li> <li>The next batch (if any) would go to P2, and so on.</li> </ol> <p>This approach is called \"Sticky Partitioning\" because the producer/broker \"sticks\" to one particular partition for an entire batch of messages, rather than switching partitions for individual messages within that batch.</p> <p>The complete batch is written to one place (a single partition). This reduces the overhead of writing to different locations for individual messages, resulting in faster processing and lower latency.   The broker is not constantly computing partition assignments for individual messages. Once a batch is created by the producer, the broker simply publishes that entire batch to a designated partition. This makes the process less CPU-intensive for the broker.</p> <p>Because of these advantages, sticky partitioning is the approach followed by Kafka's backend nowadays when no key is passed with the messages.</p> </li> </ul>"},{"location":"kafka/overview/#consumers","title":"Consumers","text":"<p>Consumers are applications that read or consume data from the topics using the Consumer API . Consumers can read data from the topic level (accessing all partitions of a topic) or from specific partitions . Consumers are always associated with a consumer group .</p> <p>A consumer group is a group of related consumers that perform a task . Each message in a partition is consumed by only one consumer within a consumer group, ensuring load balancing and avoiding duplicate processing within that group. Multiple groups can consume the same message.</p> <p>--- Consumer groups in Apache Kafka have several key advantages</p> <ul> <li>Load Balancing: Consumer groups allow the messages from a topic's partitions to be divided among multiple consumers in the group. This effectively balances the load and allows for higher throughput.</li> <li>Fault Tolerance: If a consumer in a group fails, the partitions it was consuming from are automatically reassigned to other consumers in the group, ensuring no messages are lost or left unprocessed.</li> <li>Scalability: You can increase the processing speed by simply adding more consumers to a group. This makes it easy to scale your application according to the workload.</li> <li>Parallelism: Since each consumer in a group reads from a unique set of partitions, messages can be processed in parallel, improving the overall speed and efficiency of data processing.</li> <li>Ordering Guarantee: Within each partition, messages are consumed in order. As a single partition is consumed by only one consumer in the group, this preserves the order of messages as they were written into the partition.</li> </ul> <p>--- Consumer Configurations</p> <ul> <li>Key Deserializer: This refers to the deserializer class used for keys, which must implement the org.apache.kafka.common.serialization.Deserializer interface .</li> <li>Group ID: A unique string (group.id) identifies the consumer group to which a consumer belongs . This property is essential if the consumer utilizes group management technology, the offset commit API, or a topic-based offset management strategy .</li> <li>fetch.min.bytes: This parameter sets the minimum amount of data the server should return for a fetch request . If the available data is less than this threshold, the request will wait for more data to accumulate . This strategy reduces the number of requests to the broker . The request will block until fetch.min.bytes data is available or the fetch.max.wait.ms timeout expires . While this can cause fetches to wait for larger data amounts, it generally improves throughput at the cost of some additional latency .</li> <li>Heartbeat Interval: This defines the periodic interval at which heartbeats are sent to the consumer coordinator when using logical group management facilities . Heartbeats serve to ensure the consumer session remains active and facilitate rebalancing when consumers join or leave the group . The value for the heartbeat interval must be less than session.timeout.ms, typically not exceeding one-third of session.timeout.ms . Adjusting this can help control the expected time for normal rebalances .</li> <li>session.timeout.ms: This timeout is used to detect client failures within Kafka's group management facility. Clients send periodic heartbeats to signal their liveness. If the session times out, the consumer is removed by the brokers from the group, triggering a rebalance. The value for session.timeout.ms must fall between group.min.session.timeout.ms and group.max.session.timeout.ms.</li> <li>max.partition.fetch.bytes: This sets the maximum amount of data the server will return per partition. However, if the very first record batch is larger than this specified size, that first batch will still be returned to ensure continuous progress. The maximum fetch.batch.size accepted by brokers is determined by message.max.bytes.</li> <li>Max Bytes: This refers to the maximum amount of data the server should return for a fetch request. Records are filtered in batches by the consumer. Similar to max.partition.fetch.bytes, if the first record batch exceeds this limit, it will still be returned to ensure continuous progress.</li> </ul> <p>--- When a consumer interacts with Kafka to consume messages</p> <ul> <li>Consumer Poll: The consumer issues a Consumer.poll() request, which may retrieve a certain number of records (e.g., approximately 15 records) .</li> <li>Consumer Commit: After processing messages, the consumer calls Consumer.commit() to acknowledge that messages up to a certain offset (e.g., in P1, up to offset 5) have been successfully processed .</li> </ul> <p>After the consumer receives messages from a poll request, a parallel process begins to manage offset commits. Offsets represent the position of the last consumed message in a partition. Committing an offset tells Kafka which messages have been successfully processed by a consumer.</p> <p>This automatic commit mechanism is controlled by two key properties:</p> <p><code>enable.auto.commit</code>:        By default, this property is set to <code>true</code> for Python consumer APIs.        When <code>true</code>, the Kafka consumer will automatically commit offsets at regular intervals.    <code>auto.commit.interval.ms</code>:        This property defines the time interval (in milliseconds) between automatic offset commits.        The default value is 5,000 milliseconds (5 seconds).</p> <p>The commit timer starts after a polling request is completed and messages are received.</p> <p>--- How Consumers in Consumer Group read messages?</p> <p>A single Kafka consumer can read from all partitions of a topic. This is often the case when you have only one consumer in a consumer group.</p> <p>However, when you have multiple consumers in a consumer group, the partitions of a topic are divided among the consumers. This allows Kafka to distribute the data across the consumers, enabling concurrent data processing and improving overall throughput.</p> <p>It's also important to note that while a single consumer can read from multiple partitions, a single partition can only be read by one consumer from a specific consumer group at a time. This ensures that the order of the messages in the partition is maintained when being processed.</p> <p></p> <p>When a poll is complete, a parallel thread starts an \"autocommit timer\". This thread waits for the <code>auto.commit.interval.ms</code> duration to elapse. Once the configured time is over, the offset is committed to the <code>__consumer_offset</code> topic in Kafka. If an error occurs during message processing before the commit interval is over, the commit is interrupted.</p> <p>In the main thread, while the auto-commit timer runs in parallel, the consumer processes the messages received from the broker.</p> <p>The consumer collects all records received in a poll response and begins processing individual messages one by one. This processing can involve various activities, such as writing messages to a database or performing business logic.</p> <p>Once all messages from a particular poll response are processed successfully, the consumer automatically makes another polling request to fetch the next set of messages. This creates a continuous loop of fetching and processing.</p> <p>--- Consumer Group</p> <ul> <li>Consumer Group: A consumer group is a logical entity within the Kafka ecosystem that primarily facilitates parallel processing and scalable message consumption for consumer clients .        Every consumer must be associated with a consumer group .        There is no duplication of messages among consumers within the same consumer group .</li> <li> <p>Consumer Group Rebalancing: This is the process of re-distributing partitions among the consumers within a consumer group.</p> <p>Scenarios for Rebalancing: Rebalancing occurs in several situations:        A consumer joins the consumer group.        A consumer leaves the consumer group.        New partitions are added to a topic, making them available for new consumers.        Changes in connection states.</p> </li> <li> <p>Group Coordinator: In a Kafka cluster, one of the brokers is assigned the role of group coordinator to manage consumer groups.        The group coordinator maintains and manages the list of consumer groups.        It initiates a callback to communicate the new partition assignments to all consumers during rebalancing.        Important Note: Consumers within a group undergoing rebalancing will be blocked from reading messages until the rebalance process is complete.</p> </li> <li>Group Leader: The first consumer to join a consumer group is elected as the Group Leader.        The Group Leader maintains a list of active members and selects the assignment strategy.        The Group Leader is responsible for executing the rebalance process.        Once the new assignment is determined, the Group Leader sends it to the group coordinator.</li> <li>Consumer Joining a Group: When a consumer starts:        It sends a \"Find Coordinator\" request to locate the group coordinator for its group.        It then initiates the rebalance protocol by sending a \"Joining\" request.        Subsequently, members of the consumer group send a \"SyncGroup\" request to the coordinator.        Each consumer also periodically sends a \"Heartbeat\" request to the coordinator to keep its session alive.</li> </ul>"},{"location":"kafka/overview/#rebalancing","title":"Rebalancing","text":"<p>In Apache Kafka, rebalancing refers to the process of redistributing the partitions of topics across all consumers in a consumer group. Rebalancing ensures that all consumers in the group have an equal number of partitions to consume from, thus evenly distributing the load.</p> <p>--- Rebalancing can be triggered by several events:</p> <ul> <li>Addition or removal of a consumer: If a new consumer joins a consumer group, or an existing consumer leaves (or crashes), a rebalance is triggered to redistribute the partitions among the available consumers.</li> <li>Addition or removal of a topic's partition: If a topic that a consumer group is consuming from has a partition added or removed, a rebalance will be triggered to ensure that the consumers in the group are consuming from the correct partitions.</li> <li>Consumer finishes consuming all messages in its partitions: When a consumer has consumed all messages in its current list of partitions and commits the offset back to Kafka, a rebalance can be triggered to assign it new partitions to consume from.</li> </ul> <p>While rebalancing ensures fair partition consumption across consumers, it's important to note that it can also cause some temporary disruption to the consuming process, as consumers may need to stop consuming during the rebalance. To minimize the impact, Kafka allows you to control when and how a consumer commits its offset, so you can ensure it happens at a point that minimizes any disruption from a rebalance.</p> <p>--- Read strategies in Kafka</p> <p>In Apache Kafka, the consumer's position is referred to as the \"offset\". Kafka maintains the record of the current offset at the consumer level and provides control to the consumer to consume records from a position that suits their use case. This ability to control where to start reading records provides flexibility to the consumers. Here are the main reading strategies:</p> <ul> <li>Read From the Beginning: If a consumer wants to read from the start of a topic, it can do so by setting the consumer property auto.offset.reset to earliest. This strategy is useful for use cases where you want to process all the data in the topic.</li> <li>Read From the End (Latest): If a consumer only cares about new messages and doesn't want to read the entire history of a topic, it can start reading from the end. This is done by setting auto.offset.reset to latest.</li> <li>Read From a Specific Offset: If a consumer wants to read from a particular offset, it can do so using the seek() method on the KafkaConsumer object. This method changes the position of the consumer to the specified offset.</li> <li>Committing and Reading from Committed Offsets: The consumer can commit offsets after it has processed messages. If the consumer dies and then restarts, it can continue processing from where it left off by reading the committed offset.</li> </ul> <p>--- Manual Commit, At most once &amp; Exactly Once</p> <p></p> <p>After receiving messages from a poll request, the consumer processes them in a continuous loop, and then explicitly commits its progress.</p> <p>All records received in the poll response are collected by the consumer.The consumer then picks up and processes each individual record one by one. This processing might involve various operations, such as:        Storing data in a database: Persisting the message content into a data store.        Performing business logic: Executing specific application functions based on the message.        Sending to another service: Forwarding the message to another part of your system.</p> <p>If there are more records in the collected batch, the consumer loops back to process the next one until all records from that poll response are handled.</p> <p>Once all the messages received in that particular poll response have been successfully processed, the consumer program explicitly issues a command to commit the offset to the Kafka broker. This action updates Kafka's record of the consumer's progress, marking the messages as processed.</p> <p>After the successful commit, the consumer then returns to the polling state, making another request for new messages, continuing the infinite loop of fetching, processing, and committing.</p> <p></p> <p>Even with manual offset commits, Kafka's guarantee remains at-least-once processing. This means that a message might be processed more than once, especially if an error occurs before the offset for that message (or batch) is committed.</p> <p>Consider the following scenario:</p> <ul> <li>A consumer polls and receives a batch of 10 messages.</li> <li>The consumer successfully processes the first 3 messages.</li> <li>While attempting to process the 4th message, an error occurs (e.g., a database connection drops, or invalid data is encountered).</li> <li>Immediate Interruption: As soon as the error occurs, the processing flow is interrupted, and the consumer immediately reverts back to the poll step.</li> <li>No Offset Commit: Since not all 10 messages were successfully processed (specifically, the 4th message failed) and the manual commit step for the entire batch had not yet been reached, no offset was committed for the first 3 messages either.</li> <li>Reprocessing: When the consumer polls again, Kafka will send messages from the last committed offset. Since the offset for the previous batch was never committed, Kafka will re-send the same set of 10 messages.</li> <li>Consequently, the first 3 messages, which were already processed in the previous attempt, will be reprocessed.</li> </ul> <p></p> <p>After receiving messages from a poll request, the consumer processes them in a continuous loop. The key to the \"exactly-once\" strategy demonstrated is the immediate commitment of the offset after processing each single message.</p> <p>The consumer iterates through each <code>message</code> object received from the <code>consumer.poll()</code> call (often simplified to <code>for message in consumer:</code>).</p> <p>Important details like <code>message.value</code>, <code>message.key</code>, <code>message.topic</code>, <code>message.partition</code>, <code>message.offset</code>, and <code>message.timestamp</code> can be extracted from each <code>message</code> object.</p> <p>The consumer then performs its application logic (e.g., storing data in a database, executing business logic) using the message's content. The <code>print</code> statements in the demo code are considered the \"processing engine\" for this example.</p> <p>As soon as a single message is successfully processed, the consumer program explicitly issues a command to commit the offset to the Kafka broker. This is the core mechanism ensuring that if the consumer crashes after processing a message but before a batch commit, that specific message won't be reprocessed.</p> <p>The <code>consumer.commit()</code> Method: This method takes a dictionary where keys are <code>TopicPartition</code> objects and values are <code>OffsetAndMetadata</code> objects.    <code>TopicPartition</code>: Identifies the specific topic and partition. It's constructed using <code>message.topic</code> and <code>message.partition</code>.    <code>OffsetAndMetadata</code>: Contains the offset to commit and optional metadata.    Offset Value: The offset value provided for commit is <code>current_message_offset + 1</code>. This is because Kafka interprets a committed offset <code>X</code> as meaning messages up to <code>X-1</code> have been successfully processed, and the next message to send should be <code>X</code>.    Metadata: You can pass additional metadata (e.g., <code>message.timestamp</code>).</p> <p>After successfully processing and committing the offset for a single message, the consumer continues the loop, ready to process the next message in the fetched batch or poll for new messages if the batch is exhausted.</p> <p>This approach of committing after each message significantly reduces the window for reprocessing, making it align with the \"exactly-once\" claim by the source. If the consumer crashes after processing a message but before its specific offset is committed, that message could be re-processed. However, with per-message commits, this window is minimized to the time it takes to process and commit one message.</p> <p>--- Why the One-Consumer-Per-Partition Rule?</p> <p>The central question addressed is: Why does Kafka not allow multiple consumers to consume messages from the same partition simultaneously?. This restriction is in place to prevent several critical issues related to data integrity and efficient processing.</p> <ul> <li> <p>Problem 1: Load Balancing and Message Reprocessing</p> <p>The primary reason for introducing consumer groups was to achieve load balancing and accelerate message processing. However, if multiple consumers were allowed to consume from the same partition, this goal would be undermined.</p> <p>Consider a scenario where a topic has a partition, say Partition 3, which contains messages arranged in segments with offsets (unique identifiers for messages within a partition).</p> <p>Suppose Consumer 4 and Consumer 5 are both consuming from Partition 3.</p> <ol> <li>Consumer 4 processes messages from <code>offset 0</code> to <code>offset 4096</code>. Consumer 4 knows it has consumed up to <code>offset 4096</code> and expects to pull from <code>offset 4097</code> next time.</li> <li>However, Consumer 5, running in parallel, does not know what Consumer 4 has already processed. Consumer 5 might also attempt to consume messages starting from <code>offset 0</code>.</li> <li>Result: Both Consumer 4 and Consumer 5 would end up processing the same range of messages (<code>offset 0</code> to <code>offset 4096</code>).</li> </ol> <p>This leads to reprocessing of the same messages multiple times, which is not genuine parallel processing or load balancing. True parallel processing involves different workers handling different parts of a larger task, not the same part repeatedly. The consumer group concept was designed for multiple consumers to process different chunks of messages.</p> </li> <li> <p>Problem 2: Violation of Message Order Guarantees</p> <p>Kafka guarantees message ordering only within a single partition. This means that messages sent to a specific partition will always be processed by consumers in the order they were written, based on their offsets. This guarantee is crucial for many applications, especially those where the sequence of events is vital.</p> <p>Consider a banking domain example:</p> <ol> <li>A customer first adds money to their account (Event A) and then withdraws money (Event B).</li> <li>It's essential that the addition of money is processed before the deduction to maintain correct account balance and display the correct sequence of events in the application.</li> <li>To ensure all events related to a specific account go to the same partition, a common strategy is to use hashing based on the account number (e.g., <code>account_number % total_partitions</code>). This ensures that if Event A goes to Partition 2, Event B (for the same account) will also go to Partition 2, and Event B will have a higher offset than Event A.</li> </ol> <p>The Issue if multiple consumers were allowed on one partition:</p> <ol> <li>Suppose Consumer 4 and Consumer 5 are both consuming from the same partition, and you somehow try to split the offset ranges (e.g., one consumes one range, the other consumes another).</li> <li>If they consume messages in parallel, the order of execution cannot be guaranteed.</li> <li>Consumer 5 might process the \"deduction of money\" event first, update the database, and display it in the front-end application. Simultaneously, Consumer 4 might process the \"addition of money\" event later.</li> <li>Consequence: This would result in a poor customer experience, as the transactions would not be displayed in the order they occurred. Kafka's design prevents this violation of crucial message ordering.</li> </ol> <p>Therefore, even if the reprocessing issue were somehow overcome by splitting offset ranges, the critical guarantee of message ordering within a partition would be lost if multiple consumers processed it concurrently.</p> </li> </ul>"},{"location":"kafka/overview/#index-timeindex","title":"Index &amp; Timeindex","text":"<p>When you set up a Kafka cluster, all the Kafka and ZooKeeper logs are stored in a dedicated <code>kafka-logs</code> folder. Within this folder, each topic and its partitions have their own directories.</p> <p>For instance, a topic named <code>my-topic</code> with one partition (partition 0) will have a folder named <code>my-topic-0</code>.</p> <p>Inside these partition-specific folders, Kafka stores all its messages in log files. These messages are appended sequentially to the log files, meaning new messages are always added to the end. This append-only design is fundamental to Kafka's performance.</p> <p>However, alongside these main log files (often with a <code>.log</code> extension), you'll notice other files with <code>.index</code> and <code>.timeindex</code> extensions. These are not where the actual message data resides, but they play a critical role in making message retrieval highly efficient.</p> <p>--- The Challenge of Large Log Files</p> <p>Imagine a scenario where a Kafka log file grows to be enormously large as more and more messages are produced. If a consumer requests messages starting from a specific offset (e.g., <code>offset 1500</code>), the Kafka broker would have to scan the entire log file from the beginning until it finds the requested offset. This full-file scanning is highly inefficient, especially with high message throughput and retention. It's analogous to searching for a specific record in a traditional database without any indexes \u2013 you'd have to read every single row until you find what you're looking for. To avoid such inefficiencies, databases use indexing. Kafka implements a similar concept for its log files.</p> <p>--- The Role of <code>.index</code> Files (Offset Index)</p> <p>The <code>.index</code> files are Kafka's solution to the large log file scanning problem. Their primary purpose is to help the Kafka broker quickly find the exact position (byte offset) of a message for a given offset within a log file.</p> <p>An <code>.index</code> file contains a mapping of <code>offset</code> to <code>position</code> (byte offset) within the corresponding <code>.log</code> file.</p> <p>Example</p> <p>Content of an <code>.index</code> file:   <pre><code>offset: 831 position: 17165\noffset: 925 position: 19165\noffset: 1480 position: 30165\noffset: 1587 position: 32165\n</code></pre></p> <p>This indicates that the message with <code>offset 831</code> is located at byte <code>position 17165</code> in the actual log file.</p> <p>How it Works</p> <ol> <li>When a consumer requests messages from a specific offset (e.g., <code>offset 1500</code>).</li> <li>The Kafka broker first consults the <code>.index</code> file.</li> <li>Since the offsets in the index file are stored in sorted (ascending) order, the broker can perform a binary search on the offset values.</li> <li>This binary search quickly identifies the range where the requested offset should be. For <code>offset 1500</code>, the broker would find that it falls between <code>offset 1480</code> and <code>offset 1587</code>.</li> <li> <p>Knowing the byte <code>position</code> for <code>offset 1480</code> (which is <code>30165</code>), the broker knows it only needs to scan the actual log file from that approximate position onwards, drastically reducing the search area instead of starting from the beginning of the file.</p> </li> <li> <p>Configuration: <code>log.index.interval.bytes</code>    Kafka doesn't write an entry into the <code>.index</code> file for every single message. Instead, it writes entries periodically based on the accumulated data size.</p> </li> </ol> <p>The default configuration property that controls this behavior is <code>log.index.interval.bytes</code>. Its default value is 4096 bytes. This means that roughly every 4096 bytes of new data accumulated in the Kafka topic, a new <code>offset</code> and its corresponding <code>position</code> will be written to the <code>.index</code> file.</p> <p>--- Relative Offsets for Efficiency</p> <p>To save space and improve efficiency, the <code>.index</code> file stores relative offsets instead of absolute offsets.</p> <p>Every log segment (which we'll discuss next) has a base offset \u2013 the starting offset of messages within that segment.    In the <code>.index</code> file, the offsets are stored as the difference (or \"shift\") from this base offset.     For example, if a segment starts at <code>offset 90</code> and a message has an absolute offset of <code>108</code>, the index file would store <code>18</code> (108 - 90) as the offset value.    When the broker performs a lookup, it adds this relative offset to the segment's base offset to get the actual absolute offset.    However, when you use tools like <code>kafka-run-class.bat</code> to inspect an index file, the tool performs this calculation in the backend and displays the absolute offsets for readability.</p> <p>--- The Role of <code>.timeindex</code> Files (Time Index)</p> <p>Purpose and Structure:    The <code>.timeindex</code> files complement the <code>.index</code> files by allowing Kafka to efficiently locate messages based on their timestamp. This is particularly useful for business requirements where consumers need messages published after a certain point in time.</p> <p>A <code>.timeindex</code> file contains a mapping of <code>timestamp</code> to <code>offset</code>.</p> <p>Example Content of a <code>.timeindex</code> file:    <pre><code>timestamp: 1678886400000 offset: 925\ntimestamp: 1678886401000 offset: 1587\n</code></pre></p> <p>How it Works</p> <ol> <li>When a consumer requests messages published after a specific timestamp.</li> <li>The broker consults the <code>.timeindex</code> file.</li> <li>Similar to the offset index, timestamps in the <code>.timeindex</code> are sorted, allowing for a binary search to quickly find the approximate offset corresponding to the requested timestamp.</li> <li>Once an approximate offset is found from the <code>.timeindex</code> (e.g., <code>offset 925</code> for a timestamp).</li> <li>The broker then uses this offset to perform a lookup in the <code>.index</code> file (offset index).</li> <li>The <code>.index</code> file provides the exact byte position in the log file where that offset begins.</li> <li>Finally, the broker starts scanning the actual log file from that byte position, checking the message timestamps (which are part of the message payload) to ensure they meet the time requirement.</li> </ol>"},{"location":"kafka/overview/#unclear-leader-election-tradeoff","title":"Unclear Leader Election &amp; TradeOff","text":"<p>--- The Scenario: Leader Broker Failure and Data Loss Risk</p> <p>Consider a situation where you have a topic partition <code>P2</code> with replicas on <code>Broker 1</code>, <code>Broker 2</code>, and <code>Broker 3</code>. Suppose <code>Broker 3</code> holds the leader for <code>P2</code>, and <code>Broker 1</code> and <code>Broker 2</code> hold its followers. Initially, all three are in-sync (ISR: 3, 2, 1).</p> <p>If the leader broker (<code>Broker 3</code>) goes down, a new leader must be chosen from the remaining in-sync replicas. For example, <code>Broker 2</code> might be elected as the new leader. At this point, the ISR would only include the active, in-sync replicas (e.g., ISR: 2, 1), as the original leader's broker is down.</p> <p>Now, let's say the new leader (<code>Broker 2</code>) receives <code>Message 3</code> from a producer, but before <code>Message 3</code> can be fully replicated to the remaining follower (<code>Broker 1</code>), <code>Broker 2</code> also goes down.</p> <p>At this point:</p> <p><code>Message 3</code> was written to <code>Broker 2</code>'s partition.   <code>Broker 2</code> is now down, making <code>Message 3</code> inaccessible via its original location.   <code>Broker 1</code> is still up, but it only has <code>Message 1</code> and <code>Message 2</code>, not <code>Message 3</code> because replication was incomplete.</p> <p>There is currently no in-sync replica that holds all the latest data, including <code>Message 3</code>. This is where the trade-off comes in.</p> <p>--- The Trade-Off: Availability vs. Durability</p> <p>When there are no in-sync replicas available for a partition, Kafka faces a critical decision:</p> <ul> <li> <p>Prioritize Durability (No Data Loss)</p> <p>The partition <code>P2</code> becomes unavailable, and producers cannot publish new messages to it. Data loss is prevented because the system waits for the original leader or a lagging replica to come back online and fully synchronize before accepting new writes. The system is highly durable (reliable) but potentially less available.</p> </li> <li> <p>Prioritize Availability (Potential Data Loss)</p> <p>A partition that is not fully in-sync (e.g., <code>Broker 1</code>'s partition <code>P2</code>, which is missing <code>Message 3</code>) is chosen as the new leader. Producers can then immediately start publishing new messages (<code>Message 4</code>, <code>Message 5</code>, etc.) to this new leader. <code>Message 3</code> is permanently lost because it was only present on the now-down <code>Broker 2</code> and never fully replicated to <code>Broker 1</code>. The system is highly available (continuous operation) but potentially less durable due to data loss.</p> <p>This choice is controlled by a Kafka configuration property: <code>unclean.leader.election.enable</code>.</p> </li> </ul> <p>--- unclean.leader.election.enable Configuration</p> <p>This crucial Kafka property determines which of the two options (durability or availability) Kafka will prioritize during a leader failure when no in-sync replicas are available.</p> <ul> <li> <p>unclean.leader.election.enable = true</p> <p>Allows Kafka to elect a replica that is not in-sync (i.e., it's \"unclean\" because it doesn't have all the latest data) as the new leader. Keeps the system highly available. Producers can continue writing immediately. There might be some amount of data loss (as seen with <code>Message 3</code> in our example).</p> <p>Suitable for scenarios where a small amount of data loss is acceptable, such as: Log aggregation ,Metrics calculation</p> </li> <li> <p>unclean.leader.election.enable = false</p> <p>Prevents a replica that is not in-sync from becoming the leader. The system will wait until an in-sync replica becomes available or the original leader recovers and synchronizes. Ensures the system is highly durable, meaning no data will be lost.</p> <p>The partition will be unavailable for a period, blocking producers from publishing messages until an in-sync leader is established.</p> <p>Essential for scenarios where data loss is absolutely unacceptable, such as: Transaction-related information in banking or financial sectors. Any system where monetary transactions are involved</p> </li> </ul>"},{"location":"projects/FlightBookingDataPipelinewithAirflow%26CICD/","title":"Flight Booking Data Pipeline","text":""},{"location":"projects/FlightBookingDataPipelinewithAirflow%26CICD/#flight-booking-data-pipeline-with-airflow-cicd","title":"Flight Booking Data Pipeline with Airflow &amp; CI/CD","text":"<p>This project implements a production-ready Flight Booking Data Pipeline using a modern data engineering stack. The pipeline ingests raw flight booking data, processes it using PySpark on Dataproc Serverless, orchestrates workflows using Airflow, stores the results in BigQuery, and automates deployments with GitHub Actions.</p> <p>The architecture follows industry-standard best practices \u2014 modular, scalable, and fully CI/CD-enabled.</p> <p>Note</p> <p>For Code and more details, Kindly visit: https://github.com/manishchetpalli/DataEngineeringProjects/tree/main/Flight-Booking-Airflow-CICD</p>"},{"location":"projects/FlightBookingDataPipelinewithAirflow%26CICD/#tech-stack","title":"Tech Stack","text":"<ul> <li>GitHub \u2013 Version control &amp; repository hosting  </li> <li>GitHub Actions \u2013 CI/CD automation for Dev &amp; Prod  </li> <li>Google Cloud Storage (GCS) \u2013 Raw and processed data storage  </li> <li>PySpark \u2013 Distributed data processing  </li> <li>Dataproc Serverless \u2013 Serverless PySpark job execution  </li> <li>Apache Airflow \u2013 Workflow orchestration  </li> <li>BigQuery \u2013 Data warehouse for analytics  </li> </ul>"},{"location":"projects/FlightBookingDataPipelinewithAirflow%26CICD/#project-objectives","title":"Project Objectives","text":"<ul> <li>Process flight booking data using PySpark to generate meaningful business insights  </li> <li>Orchestrate PySpark job execution via Airflow DAGs  </li> <li>Load transformed data into BigQuery  </li> <li>Automate deployment using GitHub Actions for both Dev and Prod environments  </li> </ul>"},{"location":"projects/FlightBookingDataPipelinewithAirflow%26CICD/#project-structure","title":"Project Structure","text":"<pre><code>\u251c\u2500\u2500 airflow_job/\n\u2502 \u2514\u2500\u2500 airflow_job.py\n\u251c\u2500\u2500 spark_job/\n\u2502 \u2514\u2500\u2500 spark_transformation_job.py\n\u251c\u2500\u2500 variables/\n\u2502 \u251c\u2500\u2500 dev/variables.json\n\u2502 \u2514\u2500\u2500 prod/variables.json\n\u251c\u2500\u2500 .github/\n\u2502 \u251c\u2500\u2500 ci-cd.yml\n\u251c\u2500\u2500 flight_booking.csv\n\u2514\u2500\u2500 readme.md\n</code></pre>"},{"location":"projects/FlightBookingDataPipelinewithAirflow%26CICD/#pipeline-flow","title":"Pipeline Flow","text":"<ol> <li> <p>Raw Data Ingestion Raw flight booking data is stored in GCS under the <code>raw/flight_data/</code> path.</p> </li> <li> <p>PySpark Processing (Dataproc Serverless) PySpark job performs:</p> <ul> <li>Data cleaning  </li> <li>Transformations  </li> <li>Route-level insights  </li> <li>Booking &amp; revenue trend calculations  </li> </ul> </li> <li> <p>Orchestrated by Airflow Airflow DAG:</p> <ul> <li>Triggers Dataproc Serverless job  </li> <li>Manages retries, scheduling, logging  </li> <li>Notifies on success or failure  </li> </ul> </li> <li> <p>Load into BigQuery The transformed output is loaded into BigQuery tables such as:</p> <ul> <li><code>flight_bookings_cleaned</code></li> <li><code>booking_summary</code></li> <li><code>route_demand_insights</code></li> <li><code>revenue_trends</code></li> </ul> </li> <li> <p>CI/CD with GitHub Actions</p> <ul> <li>CI: Runs linting, unit tests, PySpark job validation on pull requests  </li> <li>CD: Automatically deploys updated DAGs &amp; PySpark code to Dev/Prod based on branch merges  </li> </ul> </li> </ol> <p>This project is for educational and industrial demonstration purposes.</p>"},{"location":"projects/flink-ignite/","title":"Flink Ignite Custom Dialect","text":"<p>This project draws inspiration from this Medium article: Enhancing Real-Time Analytics with Apache Ignite and Flink SQL</p> <p>A custom Apache Flink JDBC dialect implementation for Apache Ignite, enabling seamless integration between Flink and Ignite via the JDBC connector. This dialect allows Flink to read from and write to Apache Ignite tables using SQL and the Table API.</p>"},{"location":"projects/flink-ignite/#features","title":"Features","text":"<ul> <li>Custom <code>JdbcDialect</code> for Apache Ignite</li> <li>Supports upsert (MERGE) statements for idempotent writes</li> <li>Compatible with Flink Table &amp; SQL API</li> <li>Auto-discovery via Java SPI (no manual registration required)</li> <li>Supports a wide range of SQL types</li> </ul>"},{"location":"projects/flink-ignite/#requirements","title":"Requirements","text":"<ul> <li>Java 8+</li> <li>Apache Maven</li> <li>Apache Flink 1.16.1</li> <li>Apache Ignite 2.15.0</li> </ul>"},{"location":"projects/flink-ignite/#build-instructions","title":"Build Instructions","text":"<ol> <li>Clone this repository:    <pre><code>git clone &lt;repo-url&gt;\ncd flink-ignite-custom-diaelect\n</code></pre></li> <li>Build the JAR using Maven:    <pre><code>mvn clean package\n</code></pre>    The resulting JAR will be in <code>target/flink-ignite-dialect-1.0.0.jar</code>.</li> </ol>"},{"location":"projects/flink-ignite/#usage","title":"Usage","text":"<ol> <li>Add the JAR to Flink:</li> <li>Copy <code>flink-ignite-dialect-1.0.0.jar</code> to the <code>lib/</code> directory of your Flink distribution or add it to your job\u2019s classpath.</li> <li>Ensure Ignite JDBC Driver is available:</li> <li>The dialect uses <code>org.apache.ignite.IgniteJdbcThinDriver</code>. Make sure the Ignite JDBC driver JAR is also present in the Flink <code>lib/</code> directory.</li> <li>Define a Flink Table with Ignite:    Example Flink SQL DDL:    <pre><code>CREATE TABLE ignite_table (\n  id INT PRIMARY KEY NOT ENFORCED,\n  name STRING,\n  value DOUBLE\n) WITH (\n  'connector' = 'jdbc',\n  'url' = 'jdbc:ignite:thin://&lt;ignite-host&gt;:10800',\n  'table-name' = 'my_table',\n  'driver' = 'org.apache.ignite.IgniteJdbcThinDriver'\n);\n</code></pre></li> <li> <p>Replace <code>&lt;ignite-host&gt;</code> and <code>my_table</code> with your Ignite node and table name.</p> </li> <li> <p>Use in Flink SQL or Table API:</p> </li> <li>You can now read from and write to Ignite tables using Flink SQL or the Table API.</li> </ol>"},{"location":"projects/flink-ignite/#configuration-options","title":"Configuration Options","text":"<p>Common connector options (see Flink JDBC connector docs):</p> <ul> <li><code>'connector' = 'jdbc'</code></li> <li><code>'url' = 'jdbc:ignite:thin://&lt;ignite-host&gt;:10800'</code></li> <li><code>'table-name' = '&lt;ignite-table&gt;'</code></li> <li><code>'driver' = 'org.apache.ignite.IgniteJdbcThinDriver'</code></li> <li><code>'username'</code> / <code>'password'</code> (if authentication is enabled)</li> <li><code>'sink.buffer-flush.max-rows'</code>, <code>'sink.buffer-flush.interval'</code>, etc.</li> </ul>"},{"location":"projects/flink-ignite/#how-it-works","title":"How It Works","text":"<ul> <li>The dialect is auto-registered via the Java Service Provider Interface (SPI) using the file:</li> <li><code>src/main/resources/META-INF/services/org.apache.flink.connector.jdbc.dialect.JdbcDialectFactory</code></li> <li>Flink will automatically use this dialect for any JDBC URL starting with <code>jdbc:ignite:thin:</code>.</li> </ul>"},{"location":"projects/kafkaapi/","title":"FastAPI Kafka Producer API","text":"<p>A FastAPI-based REST API for publishing JSON messages to Apache Kafka topics. Designed for high availability, containerized with Docker, and includes Prometheus metrics support for observability.</p> <p></p>"},{"location":"projects/kafkaapi/#features","title":"Features","text":"<ul> <li>REST to Kafka: Publish JSON data to Kafka via POST endpoint.</li> <li>Health Check: <code>/health</code> endpoint for service monitoring.</li> <li>Prometheus Metrics: Available at <code>/metrics</code>.</li> <li>Logging: Rotating file logs stored at <code>/var/log/fastapi_kafka_api.log</code>.</li> <li>Containerized: Docker &amp; Docker Compose support for fast deployment.</li> </ul> <p>Tip</p> <p>For better understanding, clone the repo <code>fastapipythonkafka</code> <pre><code>git clone https://github.com/manish-chet/fastapipythonkafka\n</code></pre></p> <p>Note</p> <p>If you're looking for the full FastAPI + Kafka implementation, see main.py</p>"},{"location":"projects/kafkaapi/#docker-usage","title":"Docker Usage","text":"<p>Build and run the container <pre><code>docker build -t fastapi-kafka .\ndocker run -p 5000:5000 fastapi-kafka\n</code></pre></p> <p>With Docker Compose use fast-api.yaml and kafka-docker.yaml to spin up FastAPI with Kafka locally: <pre><code>docker-compose -f kafka-docker.yaml -f fast-api.yaml up -d\n</code></pre></p>"},{"location":"projects/kafkaapi/#kubernetes-usage","title":"Kubernetes Usage","text":"<p>Deployment.yaml: 3 replicas with environment variables for Kafka.</p> <p>Service.yaml: LoadBalancer (or NodePort for local) to expose FastAPI.</p> <pre><code>kubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n</code></pre>"},{"location":"projects/kafkaapi/#publish-to-kafka","title":"Publish to Kafka","text":"<pre><code>curl -X POST \\\n  http://localhost:5000/kafka/publish/mytopic \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"key\": \"value\"}'\n</code></pre> <ul> <li> <p>Request</p> <p>Path Param: topic_name \u2013 Kafka topic to publish to</p> <p>Body: JSON object (arbitrary schema)</p> </li> <li> <p>Responses</p> <p>200 OK: Successfully published</p> <p>400: Empty body</p> <p>413: Message exceeds 10MB</p> <p>500: Internal server error</p> </li> </ul>"},{"location":"projects/kafkaapi/#local-deployment","title":"Local deployment","text":""},{"location":"projects/kafkaapi/#metrics","title":"Metrics","text":"<p>Prometheus metrics are exposed at <code>/metrics</code> (enabled by prometheus-fastapi-instrumentator).</p>"},{"location":"spark/spark/","title":"Spark","text":""},{"location":"spark/spark/#sparkoverview","title":"SparkOverview","text":"<p>--- Problems with MapReduce</p> <ol> <li>Batch Processing</li> <li>Complexity</li> <li>Data Movement</li> <li>Fault Tolerance</li> <li>No Support for Interactive Processing</li> <li>Not Optimal for Small Files</li> </ol> <p>--- Features of Spark</p> <ol> <li>Speed</li> <li>Powerful Caching</li> <li>Deployment</li> <li>Real-Time Processing</li> <li>Polyglot</li> <li>Scalability</li> </ol> <p>--- Spark Architecture</p> <p></p> <ul> <li> <p>Driver Program: The driver program is the heart of a Spark application. It runs the main() function of an application and is the place where the SparkContext is created. SparkContext is responsible for coordinating and monitoring the execution of tasks. The driver program defines datasets and applies operations (transformations &amp; actions) on them.</p> </li> <li> <p>SparkContext: The SparkContext is the main entry point for Spark functionality. It represents the connection to a Spark cluster and can be used to create RDDs, accumulators, and broadcast variables on that cluster.</p> </li> <li> <p>Cluster Manager: SparkContext connects to the cluster manager, which is responsible for the allocation of resources (CPU, memory, etc.) in the cluster. The cluster manager can be Spark's standalone manager, Hadoop YARN, Mesos, or Kubernetes.</p> </li> <li> <p>Executors: Executors are worker nodes' processes in charge of running individual tasks in a given Spark job. They run concurrently across different nodes. Executors have two roles. Firstly, they run tasks that the driver sends. Secondly, they provide in-memory storage for RDDs.</p> </li> <li> <p>Tasks: Tasks are the smallest unit of work in Spark. They are transformations applied to partitions. Each task works on a separate partition and is executed in a separate thread in executors.</p> </li> <li> <p>RDD: Resilient Distributed Datasets (RDD) are the fundamental data structures of Spark. They are an immutable distributed collection of objects, which can be processed in parallel. RDDs can be stored in memory between queries without the necessity for serialization.</p> </li> <li> <p>DAG (Directed Acyclic Graph): Spark represents a series of transformations on data as a DAG, which helps it optimize the execution plan. DAG enables pipelining of operations and provides a clear plan for task scheduling. Spark Architecture &amp; Its components</p> </li> <li> <p>DAG Scheduler: The Directed Acyclic Graph (DAG) Scheduler is responsible for dividing operator graphs into stages and sending tasks to the Task Scheduler. It translates the data transformations from the logical plan (which represents a sequence of transformations) into a physical execution plan. It optimizes the plan by rearranging and combining operations where possible, groups them into stages, and then submits the stages to the Task Scheduler.</p> </li> <li> <p>Task Scheduler: The Task Scheduler launches tasks via cluster manager. Tasks are the smallest unit of work in Spark, sent by the DAG Scheduler to the Task Scheduler. The Task Scheduler then launches the tasks on executor JVMs. Tasks for each stage are launched in as many parallel operations as there are partitions for the dataset.</p> </li> <li> <p>Master: The Master is the base of a Spark Standalone cluster (specific to Spark's standalone mode, not applicable if Spark is running on YARN or Mesos). It's the central point and entry point of the Spark cluster. It is responsible for managing and distributing tasks to the workers. The Master communicates with each of the workers periodically to check if it is still alive and if it has completed tasks.</p> </li> <li> <p>Worker: The Worker is a node in the Spark Standalone cluster (specific to Spark's standalone mode). It receives tasks from the Master and executes them. Each worker has multiple executor JVMs running on it. It communicates with the Master and Executors to facilitate task execution.The worker is responsible for managing resources and providing an execution environment for the executor JVMs.</p> </li> </ul> <p>--- What happens behind the scenes</p> <ol> <li>You launch the application.</li> <li>Spark creates a SparkContext in the Driver.</li> <li>Spark connects to the Cluster Manager (e.g., YARN, standalone, k8s).</li> <li>Cluster Manager allocates Workers and starts Executors.</li> <li>RDD transformations are converted into a DAG (Directed Acyclic Graph.</li> <li>Spark creates Stages, breaks them into Tasks (based on partitions).</li> <li>Tasks are shipped to Executors.</li> <li>Executors run the tasks and return results back to the Driver.</li> <li>Final results (e.g., word count) are written to HDFS.</li> </ol> <p>--- Spark Standalone</p> <p>Spark Standalone mode is a built-in cluster manager in Apache Spark that enables you to set up a dedicated Spark cluster without needing external resource managers like Hadoop YARN or Kubernetes.</p> <p>It is easy to deploy, suitable for development and testing, and supports distributed data processing across multiple nodes.</p> <p>Advantages:</p> <ol> <li>Easy to set up and manage</li> <li>No need for external resource managers</li> <li>Built-in web UI for monitoring</li> <li>Supports HA (High Availability) with ZooKeeper</li> </ol> <p>Limitations:</p> <ol> <li>Less fault-tolerant than YARN or Kubernetes</li> <li>Limited support for resource isolation and fairness</li> <li>Not recommended for large-scale production</li> </ol> <p>---  Spark with YARN</p> <p>Apache Spark on YARN means running Spark applications on top of Hadoop YARN (Yet Another Resource Negotiator) - the resource manager in Hadoop ecosystems. This setup allows Spark to share cluster resources with other big data tools (like Hive, HBase, MapReduce) in a multi-tenant environment.</p> <p>YARN handles resource management, job scheduling, and container allocation, while Spark focuses on data processing.</p> <ul> <li> <p>Resource Manager: It controls the allocation of system resources on all applications. A Scheduler and an Application Master are included. Applications receive resources from the Scheduler.</p> </li> <li> <p>Node Manager: Each job or application needs one or more containers, and the Node Manager monitors these containers and their usage. Node Manager consists of an Application Master and Container. The Node Manager monitors the containers and resource usage, and this is reported to the Resource Manager.</p> </li> <li> <p>Application Master: The ApplicationMaster (AM) is an instance of a framework-specific library and serves as the orchestrating process for an individual application in a distributed environment.</p> </li> </ul> <p>Advantages:</p> <ol> <li>Leverages existing Hadoop cluster (no separate setup)</li> <li>Resource sharing across Hadoop ecosystem</li> <li>Supports HDFS, Hive, HBase integration natively</li> <li>Production-grade scalability and stability</li> </ol> <p>Considerations:</p> <ol> <li>Slight overhead from YARN\u2019s container management</li> <li>Configuration tuning (memory, executor placement) is important</li> <li>YARN needs to be properly secured (Kerberos, ACLs)</li> </ol>"},{"location":"spark/spark/#data-type-schema","title":"Data Type &amp; Schema","text":"<pre><code>flight_df = spark.read.format(\"csv\") \\\n                .option(\"header\", \"false\") \\\n                .option(\"inferschema\",\"false\")\\\n                .option(\"mode\",\"FAILFAST\")\\\n                .load(\"flightdata.csv\") \n</code></pre> <p>--- Read modes in spark</p> Mode Description failFast Terminates the query immediately if any malformed record is encountered. This is useful when data integrity is critical. dropMalformed Drops all rows containing malformed records. This can be useful when you prefer to skip bad data instead of failing the entire job. permissive (default) Tries to parse all records. If a record is corrupted or missing fields, Spark sets <code>null</code> values for corrupted fields and puts malformed data into a special column named <code>_corrupt_record</code>. <p>There are two primary methods: using StructType and StructField classes, and using a DDL (Data Definition Language) string.</p> <p>These are classes in Spark used to define the schema structure.</p> <p>StructField represents a single column within a DataFrame. It holds information such as the column's name, its data type (e.g., String, Integer, Timestamp), and whether it can contain null values (nullable: True/False). If nullable is set to False, the column cannot contain NULL values, and an error will be thrown if it does.</p> <p>StructType defines the overall structure of a DataFrame. It is essentially a list or collection of StructField objects.</p> <p>Tip</p> <p>What happens if you set header=False when your data actually has a header? If you disable the header option (header=False) but your CSV file contains a header row, Spark will treat that header row as regular data. If this header row's values do not match the data types defined in your manual schema (e.g., a string \"Count\" being read into an Integer column), it can lead to null values in that column if the read mode is set to permissive, or an error if the mode is failfast</p> <p>---  Handling corrupted records</p> <p>When reading data, Spark offers different modes to handle corrupted records, which influence how the DataFrame is populated.</p> <p>In permissive mode, all records are allowed to enter the DataFrame. If a record is corrupted, Spark sets the malformed values to null and does not throw an error. For the example data with five total records (two corrupted), permissive mode will result in five records in the DataFrame, with nulls where data is bad.</p> <p>In dropMalformed mode, Spark discards any record it identifies as corrupted.</p> <p>In failfast mode, Spark immediately throws an error and stops the job as soon as it encounters the first corrupted record. This mode will result in zero records in the DataFrame because the job will fail.</p> <p>---  Print bad records</p> <p>To specifically identify and view the corrupted records, you need to define a manual schema that includes a special column named _corrupt_record. This column will capture the raw content of the corrupted record.</p> <p>Where to store bad record For scenarios with a large volume of corrupted records (e.g., thousands), printing them is not practical. Spark provides the badRecordsPath option to store all corrupted records in a specified location. These records are saved in JSON format at the designated path.</p> <p>--- Modes in DataFrame Writer API</p> <p>When working with Spark, after you have read data into a DataFrame and performed transformations, it is crucial to write the processed data back to disk to ensure its persistence. Currently, all the transformations and data processing occur in memory, so writing to disk makes the data permanent.</p> <p>A typical flow looks like: df.write.format(...).option(...).mode(...).save(path).</p> <p>The mode() method in the DataFrame Writer API is crucial as it dictates how Spark handles existing data at the target location. There are four primary modes:</p> <ul> <li> <p>append: If files already exist at the specified location, the new data from the DataFrame will be added to the existing files.</p> </li> <li> <p>overwrite: This mode deletes any existing files at the target location before writing the new DataFrame.</p> </li> <li> <p>errorIfExists: Spark will check if a file or location already exists at the target path. If it does, the write operation will fail and throw an error. Useful when you want to ensure that you do not accidentally overwrite or append to existing data.</p> </li> <li> <p>ignore: If a file or location already exists at the target path, Spark will skip the write operation entirely without throwing an error. The new file will not be written. This mode is suitable if you want to prevent new data from being written if data is already present, perhaps to avoid overwriting changes or to ensure data integrity</p> </li> </ul>"},{"location":"spark/spark/#transformations","title":"Transformations","text":"<p>--- Transformations</p> <p>In Spark, a transformation is an operation applied on an RDD (Resilient Distributed Dataset) or DataFrame/Dataset to create a new RDD or DataFrame/Dataset.</p> <p>Transformations refer to any processing done on data. They are operations that create a new DataFrame (or RDD) from an existing one, but they do not execute immediately. Spark is based on lazy evaluation, meaning transformations are only executed when an action is triggered</p> <p>Transformations in Spark are categorized into two types: narrow and wide transformations.</p> <p></p> <ul> <li> <p>Narrow Transformations</p> <p>In these transformations, all elements that are required to compute the records in a single partition live in the same partition of the parent RDD. Data doesn't need to be shuffled across partitions.</p> <p>These are transformations that do not require data movement between partitions. In a distributed setup, each executor can process its partition of data independently without needing to communicate with other executors</p> <p>Example</p> <p>map, filter, flatmap, sample</p> </li> <li> <p>Wide Transformations</p> <p>These transformations will have input data from multiple partitions. This typically involves shuffling all the data across multiple partitions.</p> <p>These transformations require data movement or \"shuffling\" between partitions. This means an executor might need data from another executor's partition to complete its computation. This data movement makes wide transformations expensive operations</p> <p>Example</p> <p>groupbykey, reducebykey, join, distinct, coalesce, repartition</p> </li> </ul>"},{"location":"spark/spark/#actions","title":"Actions","text":"<p>Actions in Apache Spark are operations that provide non-RDD values; they return a final value to the driver program or write data to an external system. Actions trigger the execution of the transformation operations accumulated in the Directed Acyclic Graph (DAG).</p> <p>Actions are operations that trigger the execution of all previous transformations and produce a result. When an action is hit, Spark creates a job.</p> <p>Example</p> <p>collect, count, save, show</p> <p>--- Read &amp; Write operation in Spark are Transformation/Action?</p> <p>Reading and writing operations in Spark are often viewed as actions, but they're a bit unique.</p> <p>Read Operation:Transformations , especially read operations can behave in two ways according to the arguments you provide</p> <p>Note</p> <ul> <li>Lazily evaluated - It will be performed only when an action is called.</li> <li>Eagerly evaluated - A job will be triggered to do some initial evaluations. In case of read.csv()</li> </ul> <p>If it is called without defining the schema and inferSchema is disabled, it determines the columns as string types and it reads only the first line to determine the names (if header=True, otherwise it gives default column names) and the number of fields.  Basically it performs a collect operation with limit 1, which means one new job is created instantly</p> <p>Now if you specify inferSchema=True, Here above job will be triggered first as well as one more job will be triggered which will scan through entire record to determine the schema, that's why you are able to see 2 jobs in spark UI</p> <p>Now If you specify schema explicitly by providing StructType() schema object to 'schema' argument of read.csv(), then you can see no jobs will be triggered here. This is because, we have provided the number of columns and type explicitly and catalogue of spark will store that information and now it doesn't need to scan the file to get that information and this will be validated lazily at the time of calling action.</p> <p>Write Operation: Writing or saving data in Spark, on the other hand, is considered an action. Functions like saveAsTextFile(), saveAsSequenceFile(), saveAsObjectFile(), or DataFrame write options trigger computation and result in data being written to an external system.</p>"},{"location":"spark/spark/#dag-and-lazy-evaluation","title":"DAG and Lazy Evaluation","text":"<p>Spark represents a sequence of transformations on data as a DAG, a concept borrowed from mathematics and computer science. A DAG is a directed graph with no cycles, and it represents a finite set of transformations on data with multiple stages. The nodes of the graph represent the RDDs or DataFrames/Datasets, and the edges represent the transformations or operations applied.</p> <p>Each action on an RDD (or DataFrame/Dataset) triggers the creation of a new DAG. The DAG is optimized by the Catalyst optimizer (in case of DataFrame/Dataset) and then it is sent to the DAG scheduler, which splits the graph into stages of tasks.</p> <p>--- Job, Stage and Task in Spark</p> <p></p> <ul> <li> <p>Application An application in Spark refers to any command or program that you submit to your Spark cluster for execution. Typically, one spark-submit command creates one Spark application. You can submit multiple applications, but each spark-submit initiates a distinct application.</p> </li> <li> <p>Job Within an application, jobs are created based on \"actions\" in your Spark code. An action is an operation that triggers the computation of a result, such as collect(), count(), write(), show(), or save(). If your application contains five actions, then five separate jobs will be created. Every job will have a minimum of one stage and one task associated with it.</p> </li> <li> <p>Stage A job is further divided into smaller parts called stages. Stages represent a set of operations that can be executed together without shuffling data across the network. Think of them as logical steps in a job's execution plan. Stages are primarily defined by \"wide dependency transformations\".</p> <p>Note</p> <p>Wide Dependency Transformations (e.g., repartition(), groupBy(), join()) require shuffling data across partitions, meaning data from one partition might be needed by another. Each wide dependency transformation typically marks the end of one stage and the beginning of a new one.</p> <p>Narrow Dependency Transformations (e.g., filter(), select(), map()) do not require data shuffling; an output partition can be computed from only one input partition. Multiple narrow transformations can be grouped into a single stage.</p> </li> <li> <p>Task A task is the actual unit of work that is executed on an executor. It performs the computations defined within a stage on a specific partition of data. The number of tasks within a stage is directly determined by the number of partitions the data has at that point in the execution. If a stage operates on 200 partitions, it will typically launch 200 tasks.</p> </li> </ul> <p>Relationship Summary:</p> <ul> <li>One Application can contain Multiple Jobs.</li> <li>One Job can contain Multiple Stages.</li> <li>One Stage can contain Multiple Tasks</li> </ul> <p>--- What if our cluster capacity is less than the size of data to be processed?</p> <p>If your cluster memory capacity is less than the size of the data to be processed, Spark can still handle it by leveraging its ability to perform computations on disk and spilling data from memory to disk when necessary.</p> <p>Let's break down how Spark will handle a 60 GB data load with a 30 GB memory cluster:</p> <ol> <li> <p>Data Partitioning: When Spark reads a 60 GB file from HDFS, it partitions the data into manageable blocks, according to the Hadoop configuration parameter dfs.blocksize or manually specified partitions. These partitions can be processed independently.</p> </li> <li> <p>Loading Data into Memory: Spark will load as many partitions as it can fit into memory. It starts processing these partitions. The size of these partitions is much smaller than the total size of your data (60 GB), allowing Spark to work within the confines of your total memory capacity (30 GB in this case).</p> </li> <li> <p>Spill to Disk: When the memory is full, and Spark needs to load new partitions for processing, it uses a mechanism called \"spilling\" to free up memory. Spilling means writing data to disk. The spilled data is the intermediate data generated during shuffling operations, which needs to be stored for further stages.</p> </li> <li> <p>On-Disk Computation: Spark has the capability to perform computations on data that is stored on disk, not just in memory. Although computations on disk are slower than in memory, it allows Spark to handle datasets that are larger than the total memory capacity.</p> </li> <li> <p>Sequential Processing: The stages of the job are processed sequentially, meaning Spark doesn't need to load the entire dataset into memory at once. Only the data required for the current stage needs to be in memory or disk.</p> </li> </ol> <p>--- How spark perform data partitioning</p> <p></p> <ul> <li>Data Partitioning: Apache Spark partitions data into logical chunks during reading from sources like HDFS, S3, etc.</li> <li>Data Distribution: These partitions are distributed across the Spark cluster nodes, allowing for parallel processing.</li> <li>Custom Partitioning: Users can control data partitioning using Spark's repartition(), coalesce() and partitionBy() methods, optimizing data locality or skewness.</li> </ul> <p>When Apache Spark reads data from a file on HDFS or S3, the number of partitions is determined by the size of the data and the default block size of the file system. In general, each partition corresponds to a block in HDFS or an object in S3.</p> <p>Example</p> <p>If HDFS is configured with a block size of 128MB and you have a 1GB file, it would be divided into 8 blocks in HDFS. Therefore, when Spark reads this file, it would create 8 partitions, each corresponding to a block.</p> <p>--- Lazy Evaluation in Spark</p> <p>Lazy evaluation in Spark means that the execution doesn't start until an action is triggered. In Spark, transformations are lazily evaluated, meaning that the system records how to compute the new RDD (or DataFrame/Dataset) from the existing one without performing any transformation. The transformations are only actually computed when an action is called and the data is required. </p> <p>Example</p> <p>spark.read.csv()  will not actually read the data until an action like .show() or .count() is performed</p>"},{"location":"spark/spark/#spark-query-plan","title":"Spark Query Plan","text":"<p>The Spark SQL Engine is fundamentally the Catalyst Optimizer. Its primary role is to convert user code (written in DataFrames, SQL, or Datasets) into Java bytecode for execution. This conversion and optimization process occurs in four distinct phases. It's considered a compiler because it transforms your code into Java bytecode. It plays a key role in optimizing code leveraging concepts like lazy evaluation</p> <p></p> <p>--- Phase 1: Unresolved Logical Plan</p> <p>This is the initial stage where you write your code using DataFrames, SQL, or Datasets APIs.</p> <p>When you write transformations (e.g., select, filter, join), Spark creates an \"unresolved logical plan\". This plan is like a blueprint or a \"log\" of transformations, indicating what operations need to be performed in what order.</p> <p>At this stage, the plan is \"unresolved\" because Spark has not yet checked if the tables, columns, or files referenced actually exist</p> <p>--- Phase 2: Analysis</p> <p>To resolve the logical plan by checking the existence and validity of all referenced entities.</p> <p>This phase heavily relies on the Catalog. The Catalog is where Spark stores metadata (data about data). It contains information about tables, files, databases, their names, creation times, sizes, column names, and data types. For example, if you read a CSV file, the Catalog knows its path, name, and column headers.</p> <p>The Analysis phase queries the Catalog to verify if the files, columns, or tables specified in the unresolved logical plan actually exist.</p> <p>If everything is found and validated, the plan becomes a \"Resolved Logical Plan\". If any entity is not found (e.g., a non-existent file path or a misspelled column name), Spark throws an AnalysisException</p> <p>--- Phase 3: Logical Optimization</p> <p>To optimize the \"Resolved Logical Plan\" without considering the physical execution aspects. It focuses on making the logical operations more efficient.</p> <p>Note</p> <p>Predicate Pushdown: If you apply multiple filters, the optimizer might combine them or push them down closer to the data source to reduce the amount of data processed early.</p> <p>Column Pruning: If you select all columns (SELECT *) but then only use a few specific columns in subsequent operations, the optimizer will realize this and modify the plan to only fetch the necessary columns from the start, saving network I/O and processing.</p> <p>This phase benefits from Spark's lazy evaluation, allowing it to perform these optimizations before any actual computation begins. An \"Optimized Logical Plan\"</p> <p>--- Phase 4: Physical Planning</p> <p>The \"Optimized Logical Plan\" is converted into multiple possible \"Physical Plans\". Each physical plan represents a different strategy for executing the logical operations (e.g., different join algorithms).</p> <p>Spark applies a \"Cost-Based Model\" to evaluate these physical plans. It estimates the resources (memory, CPU, network I/O) each plan would consume if executed.</p> <p>The plan that offers the best resource utilization and lowest estimated cost (e.g., least data shuffling, fastest execution time) is selected as the \"Best Physical Plan\".</p> <p>Example</p> <p>For joins, if one table is significantly smaller than the other, Spark might choose a Broadcast Join. This involves sending the smaller table to all executor nodes where the larger table's partitions reside. This avoids data shuffling (expensive network operations) of the larger table across the cluster, leading to significant performance gains.</p> <p>The Best Physical Plan, which is essentially a set of RDDs (Resilient Distributed Datasets) ready to be executed on the cluster.</p> <p>--- Phase 5:Whole-Stage Code Generation</p> <p>This is the final step where the \"Best Physical Plan\" (the RDD operations) is translated into Java bytecode.</p> <p>This bytecode is then sent to the individual executors on the cluster to be executed. This direct bytecode generation improves performance by eliminating interpretation overhead and allowing the JVM to further optimize the code.</p> <p>--- In what cases will predicate pushdown not work?</p> <ul> <li>Complex Data Types</li> </ul> <p>Spark's Parquet data source does not push down filters that involve complex types, such as arrays, maps, and struct. This is because these complex data types can have complicated nested structures that the Parquet reader cannot easily filter on.</p> <p>Here's an example:</p> <pre><code>root\n |-- Name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|Name      |properties                   |\n+----------+-----------------------------+\n|Afaque    |[eye -&gt; black, hair -&gt; black]|\n|Naved     |[eye -&gt;, hair -&gt; brown]      |\n|Ali       |[eye -&gt; black, hair -&gt; red]  |\n|Amaan     |[eye -&gt; grey, hair -&gt; grey]  |\n|Omaira    |[eye -&gt; , hair -&gt; brown]     |\n+----------+-----------------------------+\n</code></pre> <pre><code>df.filter(df.properties.getItem(\"eye\") == \"brown\").show()\n</code></pre> <pre><code>== Physical Plan ==\n*(1) Filter (metadata#123[key] = value)\n+- *(1) ColumnarToRow\n   +- FileScan parquet [id#122,metadata#123] Batched: true, DataFilters: [(metadata#123[key] = value)], Format: Parquet, ...\n</code></pre> <ul> <li>Unsupported Expressions </li> </ul> <p>In Spark, <code>Parquet</code> data source does not support pushdown for filters involving a <code>.cast</code> operation.</p> <p>The reason for this behaviour is as follows: <code>.cast</code> changes the datatype of the column, and the Parquet data source may not be able to perform the filter operation correctly on the cast data.</p> <p>Note</p> <p>This behavior may vary based on the data source. For example, if you're working with a JDBC data source connected to a database that supports SQL-like operations, the <code>.cast</code> filter could potentially be pushed down to the database.</p>"},{"location":"spark/spark/#rdd","title":"RDD","text":"<p>RDDs are the building blocks of any Spark application.</p> <p>RDDs Stands for</p> <ul> <li>Resilient: Fault tolerant and is capable of rebuilding data on failure</li> <li>Distributed: Distributed data among the multiple nodes in a cluster</li> <li>Dataset: Collection of partitioned data with values</li> </ul> <p>--- Here are some key points about RDDs and their properties:</p> <ul> <li> <p>Fundamental Data Structure: RDD is the fundamental data structure of Spark, which allows it to efficiently operate on large-scale data across a distributed environment.</p> </li> <li> <p>Immutability: Once an RDD is created, it cannot be changed. Any transformation applied to an RDD creates a new RDD, leaving the original one untouched.</p> </li> <li> <p>Resilience: RDDs are fault-tolerant, meaning they can recover from node failures. This resilience is provided through a feature known as lineage, a record of all the transformations applied to the base data.</p> </li> <li> <p>Lazy Evaluation: RDDs follow a lazy evaluation approach, meaning transformations on RDDs are not executed immediately, but computed only when an action (like count, collect) is performed. This leads to optimized computation.</p> </li> <li> <p>Partitioning: RDDs are partitioned across nodes in the cluster, allowing for parallel computation on separate portions of the dataset.</p> </li> <li> <p>In-Memory Computation: RDDs can be stored in the memory of worker nodes, making them readily available for repeated access, and thereby speeding up computations.</p> </li> <li> <p>Distributed Nature: RDDs can be processed in parallel across a Spark cluster, contributing to the overall speed and scalability of Spark.</p> </li> <li> <p>Persistence: Users can manually persist an RDD in memory, allowing it to be reused across parallel operations. This is useful for iterative algorithms and fast interactive use.</p> </li> <li> <p>Operations: Two types of operations can be performed on RDDs - transformations (which create a new RDD) and actions (which return a value to the driver program or write data to an external storage system).</p> </li> </ul> <p>--- When to Use RDDs (Advantages)</p> <p>Despite the general recommendation to use DataFrames/Datasets, RDDs have specific use cases where they are advantageous:</p> <ul> <li> <p>Unstructured Data: RDDs are particularly well-suited for processing unstructured data where there is no predefined schema, such as streams of text, media, or arbitrary bytes. For structured data, DataFrames and Datasets are generally better.</p> </li> <li> <p>Full Control and Flexibility: If you need fine-grained control over data processing at a very low level and want to optimize the code manually, RDDs provide that flexibility. This means the developer has more control over how data is transformed and distributed.</p> </li> <li> <p>Type Safety (Compile-Time Errors): RDDs are type-safe. This means that if there's a type mismatch (e.g., trying to add an integer to a string), you will get an error during compile time, before the code even runs. This can help catch errors earlier in the development cycle, unlike DataFrames or SQL queries which might only show errors at runtime</p> </li> </ul> <p>--- Why You Should NOT Use RDDs (Disadvantages)</p> <p>For most modern Spark applications, especially with structured or semi-structured data, RDDs are generally discouraged due to several drawbacks:</p> <ul> <li> <p>No Automatic Optimization by Spark: Spark's powerful Catalyst Optimizer does not perform optimizations automatically for RDD operations. This means the responsibility for writing optimized and efficient code falls entirely on the developer.</p> </li> <li> <p>Complex and Less Readable Code: Writing RDD code can be complex and less readable compared to DataFrames, Datasets, or SQL. The code often requires explicit handling of data transformations and aggregations, which can be verbose.</p> </li> <li> <p>Potential for Inefficient Operations: Expensive Shuffling: Without Spark's internal optimizations, RDD operations can lead to inefficient data shuffling. In contrast, DataFrames/Datasets using the \"what to\" approach allow Spark to rearrange operations (e.g., filter first, then shuffle) to optimize performance, saving significant computational resources.</p> </li> </ul> <p>Example</p> <p>If you perform a reduceByKey (which requires shuffling data across nodes) before a filter operation, Spark will shuffle all the data first, then filter it. If the filter significantly reduces the dataset size, shuffling the larger pre-filtered dataset becomes a very expensive operation.</p> <ul> <li>Developer Burden: Because Spark doesn't optimize RDDs, the developer must have a deep understanding of distributed computing and Spark's internals to write performant RDD code. This makes development harder and slower compared to using higher-level APIs</li> </ul> <p>--- Difference</p> Criteria RDD (Resilient Distributed Dataset) DataFrame DataSet Abstraction Low level, provides a basic and simple abstraction. High level, built on top of RDDs. Provides a structured and tabular view on data. High level, built on top of DataFrames. Provides a structured and strongly-typed view on data. Type Safety Provides compile-time type safety, since it is based on objects. Doesn't provide compile-time type safety, as it deals with semi-structured data. Provides compile-time type safety, as it deals with structured data. Optimization Optimization needs to be manually done by the developer (like using <code>mapreduce</code>). Makes use of Catalyst Optimizer for optimization of query plans, leading to efficient execution. Makes use of Catalyst Optimizer for optimization. Processing Speed Slower, as operations are not optimized. Faster than RDDs due to optimization by Catalyst Optimizer. Similar to DataFrame, it's faster due to Catalyst Optimizer. Ease of Use Less easy to use due to the need of manual optimization. Easier to use than RDDs due to high-level abstraction and SQL-like syntax. Similar to DataFrame, it provides SQL-like syntax which makes it easier to use. Interoperability Easy to convert to and from other types like DataFrame and DataSet. Easy to convert to and from other types like RDD and DataSet. Easy to convert to and from other types like DataFrame and RDD."},{"location":"spark/spark/#partitioning-and-bucketing","title":"Partitioning and Bucketing","text":"<p>Partitioning and Bucketing are data optimization techniques used in Spark during the data writing process to improve performance for future read operations, joins, and filters. By deciding how data is organized on disk, you can significantly reduce the amount of data Spark needs to scan.</p> <p>--- Partitioning</p> <p>Partitioning organizes data into a hierarchical folder structure based on the distinct values of one or more columns. </p> <p>For every unique value in the partitioned column, Spark creates a separate folder. For example, partitioning by \"Address\" (Country) would create folders like <code>Address=India</code>, <code>Address=USA</code>, etc..</p> <p>When you query data using a filter (e.g., <code>WHERE Country = 'India'</code>), Spark skip all other folders and only reads the relevant one, avoiding a full table scan.</p> <p>You can partition by multiple columns. The order of columns matters; for example, partitioning by <code>Address</code> then <code>Gender</code> creates gender folders inside each country folder.</p> <pre><code># Partitioning by a single column (Address)\ndf.write \\\n  .partitionBy(\"Address\") \\\n  .format(\"csv\") \\\n  .save(\"/path/to/destination\")\n\n# Partitioning by multiple columns (Address and then Gender)\ndf.write \\\n  .partitionBy(\"Address\", \"Gender\") \\\n  .format(\"csv\") \\\n  .save(\"/path/to/destination_multi\")\n</code></pre> <p>Partitioning is inefficient for high-cardinality columns (columns with many unique values, like a User ID). If you partition by ID, Spark might create millions of tiny files, which degrades performance.</p> <p>--- Bucketing</p> <p>Bucketing is used when partitioning is not suitable, particularly for high-cardinality columns or to optimize joins.</p> <p>Spark uses a hash function on a specific column to distribute data into a fixed number of \"buckets\" (files).</p> <p>Unlike partitioning, bucketing is not supported by the standard <code>.save()</code> method on file systems; it requires using <code>saveAsTable</code> because the metadata must be stored in the Hive Metastore.</p> <p>If two tables are bucketed on the same column with the same number of buckets, Spark can perform a join without \"shuffling\" data across the network, which is a very expensive operation.</p> <p>Spark knows exactly which bucket contains a specific value (e.g., a specific Aadhaar ID), allowing it to search only a small fraction of the data.</p> <pre><code># Bucketing by ID into 3 buckets\ndf.write \\\n  .bucketBy(3, \"ID\") \\\n  .saveAsTable(\"bucketed_table\")\n</code></pre> <p>If you have 200 tasks running and you ask for 5 buckets, Spark might create \\(200 \\times 5 = 1,000\\) files. To prevent this, repartition the data to match the number of buckets before writing.</p> <pre><code># Optimize by matching partitions to buckets\ndf.repartition(5) \\\n  .write \\\n  .bucketBy(5, \"ID\") \\\n  .saveAsTable(\"optimized_bucket_table\")\n</code></pre> <p>--- Summary Comparison</p> Feature Partitioning Bucketing Logic Groups data into folders based on column values. Groups data into a fixed number of files via hashing. Best For Low-cardinality columns (e.g., Country, Gender). High-cardinality columns (e.g., ID) and Join optimization. Output Created as directory structures on the file system. Created as specific bucketed files within a table. Constraint Can lead to \"too many small files\" if cardinality is high. Must be saved as a table (<code>saveAsTable</code>)."},{"location":"spark/spark/#sparksession-vs-sparkcontext","title":"SparkSession vs SparkContext","text":"<p>Both Spark Session and Spark Context serve as the entry point into a Spark cluster, similar to how a <code>main</code> method serves as the entry point for code execution in languages like C++ or Java. This means that to run any Spark code, you first need to establish one of these entry points.</p> <p>--- Spark Session</p> <p>The Spark Session is the unified entry point introduced in Spark 2.0. It is now the primary way to interact with Spark.</p> <p>Prior to Spark 2.0 (specifically up to Spark 1.4), if you wanted to work with different Spark functionalities like SQL, Hive, or Streaming, you had to create separate contexts for each (e.g., <code>SQLContext</code>, <code>HiveContext</code>, <code>StreamingContext</code>). The Spark Session encapsulates all these different contexts, providing a single object to access them. This simplifies development as you only need to create a Spark Session to gain access to all necessary functionalities.</p> <p>When you create a Spark Session, you can pass configurations for resources needed, such as the amount of memory or the number of executors. The Spark Session takes these values and communicates with the Resource Manager (like YARN or Mesos) to request and allocate the necessary driver memory and executors. Once these resources are secured, the Spark Session facilitates the execution of your Spark jobs within that allocated environment.</p> <p>If you've been using Databricks notebooks, you might have implicitly been using a Spark Session without realizing it. Databricks typically provides a default <code>spark</code> object, which is an instance of <code>SparkSession</code>, allowing you to directly write code like <code>spark.read.format(...)</code>. This is why the local setup is demonstrated in the source, as the default session is not automatically provided outside environments like Databricks.</p> <p>You can configure properties like <code>spark.driver.memory</code> by using the <code>.config()</code> method when building the Spark Session.</p> <p>--- Spark Context</p> <p>The Spark Context (<code>SparkContext</code>) was the original entry point for Spark applications before Spark 2.0.</p> <p>n earlier versions of Spark (up to Spark 1.4), <code>SparkContext</code> was the primary entry point for general Spark operations. However, for specific functionalities like SQL, you needed additional context objects like <code>SQLContext</code>.</p> <p>While Spark Session has become the dominant entry point, <code>SparkContext</code> is still relevant for RDD (Resilient Distributed Dataset) level operations. If you need to perform low-level transformations directly on RDDs (e.g., <code>flatMap</code>, <code>map</code>), you would typically use the Spark Context. An example provided is writing a word count program using RDDs, where <code>SparkContext</code> comes into use.</p> <p>With the advent of Spark Session, you do not create a <code>SparkContext</code> directly as a separate entry point anymore. Instead, you can access the <code>SparkContext</code> object through the <code>SparkSession</code> instance. This means that the <code>SparkContext</code> is now encapsulated within the <code>SparkSession</code>.</p> <p>--- Code Example</p> <p>Here\u2019s an example demonstrating how to create a Spark Session and then obtain a Spark Context from it, based on the provided transcript:</p> <pre><code># First, import the SparkSession class from pyspark.sql\nfrom pyspark.sql import SparkSession\n\n# Create a SparkSession builder\n# The .builder() method is used to construct a SparkSession instance\nspark_builder = SparkSession.builder\n\n# Configure the SparkSession\n# .master(\"local\"): Specifies that Spark should run in local mode.\n#                  This means Spark will use your local machine's resources.\n# .appName(\"Testing\"): Sets the name of your Spark application.\n#                     This can be any descriptive name for your project.\n# .config(\"spark.driver.memory\", \"12g\"): An optional configuration to request specific resources,\n#                                       here requesting 12GB for the driver memory.\nspark_session_config = spark_builder.master(\"local\").appName(\"Testing\").config(\"spark.driver.memory\", \"12g\")\n\n# Get or Create the SparkSession\n# .getOrCreate(): This is a crucial method. If a SparkSession with the specified\n#                name and configuration already exists, it will retrieve it.\n#                Otherwise, it will create a new one.\nspark = spark_session_config.getOrCreate()\n\n# Print the SparkSession object to verify it's created\nprint(spark)\n\n# Access the SparkContext from the created SparkSession\n# The SparkContext is encapsulated within the SparkSession object.\n# This is how you get the sc (SparkContext) object in modern Spark applications.\nsc = spark.sparkContext\n\n# Print the SparkContext object\nprint(sc)\n\n# Example of using SparkSession to read data (common operation)\n# This is what you often do in Databricks without explicitly creating a session.\n# employee_df = spark.read.format(\"csv\").load(\"path/to/employee_data.csv\")\n</code></pre>"},{"location":"spark/spark/#repartition-vs-coaelesce","title":"Repartition vs Coaelesce","text":"<p>The need for repartition and coalesce arises from issues faced when processing large datasets in Spark, particularly concerning data partitioning.</p> <p></p> <p>When a DataFrame is created, it's often divided into multiple partitions. Sometimes, these partitions can be of uneven sizes (e.g., 10MB, 20MB, 40MB, 100MB).</p> <p>Processing smaller partitions (e.g., 10MB) takes less time than larger ones (e.g., 100MB). This leads to idle Spark executors: while one executor is busy with a large partition, others might finish their tasks quickly and then wait for the large partition to complete. This causes time delays and underutilization of allocated resources (e.g., RAM)</p> <p>This situation often arises after operations like join transformations. For instance, if a join operation is performed on a product column, and one product is a \"best-selling product\" with a high number of records, all those records might get grouped into a single partition, making it very large. This phenomenon is called data skew.</p> <p>Users often see messages like \"199 out of 200 partitions processed,\" where the last remaining partition takes a significantly longer time to complete due to its large size.</p> <p>To deal with these scenarios and optimize performance, Spark provides repartition and coalesce methods</p> <p>--- repartition</p> <p>Repartition shuffles the entire dataset across the cluster. This means data from existing partitions can be moved to new partitions.</p> <p>The primary goal of repartition is to evenly distribute data across the specified number of new partitions. For example, if you have 200MB of data across five uneven partitions and repartition it into five, it will aim for 40MB per partition.</p> <p>Repartition can increase or decrease the number of partitions. If you initially have 5 partitions but need 10, repartition is the only choice.It can be used when you want to increase the number of partitions to allow for more concurrent tasks and increase parallelism when the cluster has more resources.</p> <p>Due to the shuffling operation, repartition is generally more expensive and involves more I/O operations compared to coalesce.</p> <p>Pros and Cons of Repartition are Evenly distributed data. More I/O (Input/Output) because of shuffling. More expensive.</p> <p>In certain scenarios, you may want to partition based on a specific key to optimize your job. For example, if you frequently filter by a certain key, you might want all records with the same key to be on the same partition to minimize data shuffling. In such cases, you can use repartition() with a column name.</p> <p>--- coalesce</p> <p>Coalesce merges existing partitions to reduce the total number of partitions.</p> <p>Crucially, coalesce tries to avoid full data shuffling. It achieves this by moving data from some partitions to existing ones, effectively merging them locally on the same executor if possible. This makes it less expensive than repartition.</p> <p>Because it avoids full shuffling, coalesce does not guarantee an even distribution of data across the new partitions. It might result in an uneven distribution, especially if the original partitions were already skewed.</p> <p>Coalesce can only decrease the number of partitions. It cannot be used to increase the number of partitions. If you need more partitions, you must use repartition.</p> <p>Pros and Cons of Coalesce: No shuffling (or minimal shuffling). Not expensive (cost-effective). Uneven data distribution.</p> <p>However, it can lead to  data skew if you have fewer partitions than before, because it combines existing partitions to reduce the total number.</p> <p>--- When to Choose Which?</p> <p>The choice between repartition and coalesce is use-case dependent</p> <p>Choose repartition when:</p> <p>You need to evenly distribute data across partitions, which is crucial for balanced workload across executors.</p> <p>You need to increase the number of partitions (e.g., if you have too few partitions or want to process data in smaller chunks in parallel).</p> <p>You are okay with the overhead of a full shuffle, as the benefit of even distribution outweighs the cost.</p> <p>Dealing with severe data skew is a primary concern.</p> <p>Choose coalesce when:</p> <p>You primarily need to decrease the number of partitions (e.g., after filter operations drastically reduce data, or before writing to a single file).</p> <p>You want to minimize shuffling and I/O costs.</p> <p>You can tolerate slightly uneven data distribution across partitions, or the data skew is minimal and won't significantly impact performance.</p> <p>You want to save processing time and cost by avoiding a full shuffle</p> <p>--- Why doesn't <code>.coalesce()</code> explicitly show the partitioning scheme?</p> <p><code>.coalesce</code> doesn't show the partitioning scheme e.g. <code>RoundRobinPartitioning</code> because the operation only minimizes data movement by merging into fewer partitions, it doesn't do any shuffling. Because no shuffling is done, the partitioning scheme remains the same as the original DataFrame and Spark doesn't include it explicitly in it's plan as the partitioning scheme is unaffected by <code>.coalesce</code></p>"},{"location":"spark/spark/#spark-strategy-joins","title":"Spark Strategy Joins","text":"<p>It is important to distinguish between Join Types and Join Strategies.</p> <p>Join Types refers to the logical result of the join (e.g., Left Join, Right Join, Inner Join, etc.).</p> <p>Join Strategies refers to the internal implementation or \"strategy\" Spark uses to execute the join across a cluster (e.g., Shuffle Sort Merge Join, Broadcast Join).</p> <p>--- Why Joins are Expensive: The Shuffling Process</p> <p>Joins are considered \"expensive\" operations in Spark because they often require shuffling (or \"saapling\" as referred to in the source), which involves moving data across the network between executors.</p> <p>If you have two DataFrames of 500MB each with a default HDFS block size of 128MB, Spark will create 4 partitions for each DataFrame. These partitions are distributed across different executors/worker nodes.</p> <p>To join data on a specific key (e.g., <code>ID</code>), the data for that same key must reside on the same executor. If <code>ID 1</code> is on Executor A and its corresponding match is on Executor B, Spark must move that data to a common location.</p> <p>For wide transformations like joins, Spark defaults to creating 200 partitions.</p> <p>Spark uses a hash-based approach to determine where data goes. For example, it might calculate <code>ID % 200</code> to determine the partition number. This ensures that the same ID from both DataFrames always ends up in the same partition.</p> <p>Conceptual Code Example (Shuffling): <pre><code># Spark defaults to 200 partitions for shuffle operations\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n\n# Performing a join triggers shuffling to align keys across the cluster\ndf_joined = df1.join(df2, df1.id == df2.id, \"inner\")\n</code></pre></p> <p>Spark utilizes five primary strategies to perform joins:</p> <ol> <li>Shuffle Sort Merge Join (SSMJ)</li> <li>Shuffle Hash Join (SHJ)</li> <li>Broadcast Hash Join (BHJ)</li> <li>Cartesian Join</li> <li>Broadcast Nested Loop Join (BNLJ)</li> </ol> <p>--- Shuffle Sort Merge Join (SSMJ)</p> <p>This is the default join strategy in Spark.</p> <p>Data with the same keys are moved to the same partitions. The data within each partition is sorted by the join key. Spark iterates through the sorted data and merges matching keys. The sorting phase typically takes \\(O(n \\log n)\\) time.It is CPU-intensive due to sorting but very stable for large datasets because it doesn't require the entire table to fit in memory.</p> <p>--- Shuffle Hash Join (SHJ)</p> <p>Data is moved to the same partitions. Spark builds a hash table of the smaller DataFrame in memory. It then probes this hash table using keys from the larger DataFrame. The join/lookup phase is \\(O(1)\\). It is Memory-intensive. If the hash table exceeds the executor's memory, it will trigger an Out of Memory (OOM) exception.</p> <p>--- Broadcast Nested Loop Join (BNLJ)</p> <p>This is considered the most expensive join strategy. It is used when there is no equality condition (non-equi joins), such as <code>df1.id &gt; df2.id</code>. It operates at \\(O(n^2)\\) because it essentially involves two nested loops to compare every row of one table with every row of the other.</p> <p>Conceptual Code Example (Non-Equi Join): <pre><code># A non-equi join often forces a Broadcast Nested Loop Join\ndf_non_equi = df1.join(df2, df1.id &gt; df2.id, \"inner\")\n</code></pre></p> <p>--- Summary of Trade-offs</p> Strategy Resource Focus Best For Sort Merge Join CPU (Sorting) Large datasets; Very stable; Default Shuffle Hash Join Memory (Hash Table) When one table is small enough to fit in memory Broadcast Nested Loop CPU/Memory (Nested Loop) Non-equi joins (e.g., <code>&gt;</code>, <code>&lt;</code>); Very slow <p>--- How Broadcast Join Works</p> <p>Broadcast Hash Join is a specialized join strategy used to optimize performance by eliminating the need for data shuffling,. While standard joins like Shuffle Sort Merge Join (the Spark default) move data across the network to align keys, a Broadcast Join sends the entire smaller dataset to every worker node,.</p> <p>The core mechanism involves the Driver and the Executors - The Spark Driver identifies a table that is small enough to be broadcast. It must have sufficient memory to store this table locally before distributing it. The Driver sends a complete copy of the small table to every Executor in the cluster.</p> <p>Once the small table is residing on every Executor, each Executor can perform the join locally using its own partition of the large table. This makes the Executors \"self-sufficient\" because they no longer need to fetch data from other nodes via shuffling.</p> <p>--- When to Use Broadcast Joins</p> <p>It is ideal when you have one large table (e.g., 1GB) and one small table (e.g., less than 10MB). Use it to prevent \"cluster choking\" caused by moving massive amounts of data across the network. While not the primary focus of this source, the transcript notes that avoiding shuffling is the main goal of this strategy.</p> <p>--- Checking and Setting the Broadcast Threshold</p> <p>Spark uses a default threshold to decide if a table should be automatically broadcast. This is typically 10 MB.</p> <pre><code># To get the current broadcast threshold (returns value in bytes)\ncurrent_threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\nprint(current_threshold) # Default is 10485760 (10 MB)\n\n# To change the threshold (e.g., to 20 MB)\n# You must convert MB to bytes\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"20971520\")\n\n# To disable automatic broadcasting\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n</code></pre> <p>--- Forcing a Broadcast Join with Hints</p> <p>If Spark does not automatically choose a broadcast join, you can provide a hint in your code to force it.</p> <pre><code>from pyspark.sql.functions import broadcast\n\n# Standard join might result in a Sort Merge Join\n# df_joined = df_large.join(df_small, \"id\")\n\n# Using the broadcast hint to force a Broadcast Hash Join\ndf_joined = df_large.join(broadcast(df_small), df_large.id == df_small.id, \"inner\")\n\n# Inspecting the physical plan to verify the join strategy\ndf_joined.explain() \n</code></pre> <p>--- Identifying Joins in the Spark Web UI</p> <p>In the Spark Web UI (SQL tab), you can distinguish between join types by looking at the execution graph: Will show an <code>Exchange</code> (Shuffle), followed by a <code>Sort</code>, and then a <code>SortMergeJoin</code>. Will show a <code>BroadcastExchange</code> and then a <code>BroadcastHashJoin</code>, with no <code>Exchange</code> (shuffle) for the large table.</p> <p>--- Potential Risks and Failure Points</p> <p>Despite its efficiency, Broadcast Join can fail in the following scenarios:</p> <p>Since the small table must first be collected by the Driver, if the table is too large for the Driver's memory, the application will crash. If the Executor's memory is already near capacity, adding a broadcasted table (even a 100MB one) can trigger an OOM during the join operation. Attempting to broadcast a very large file (e.g., 1GB) will saturate the network because that file must be sent to every single executor.</p> <p>--- Summary Table</p> Feature Shuffle Sort Merge Join (SSMJ) Broadcast Hash Join (BHJ) Data Movement Shuffles both tables Broadcasts only the small table Network Cost High (Shuffle) Low (if table is small) Default Size Any size &lt; 10 MB (Default) Stability High Risky if small table is too large"},{"location":"spark/spark/#spark-memory-management","title":"Spark Memory Management","text":"<p>When the Spark application is launched, the Spark cluster will start two processes \u2014 Driver and Executor.</p> <p>The driver is a master process responsible for creating the Spark context, submission of Spark jobs, and translation of the whole Spark pipeline into computational units \u2014 tasks. It also coordinates task scheduling and orchestration on each Executor.</p> <p>Driver memory management is not much different from the typical JVM process.</p> <p>The executor is responsible for performing specific computational tasks on the worker nodes and returning the results to the driver, as well as providing storage for RDDs. And its internal memory management is very interesting.</p>"},{"location":"spark/spark/#executor-memory","title":"Executor memory","text":"<p>A Spark executor container has three major components of memory:</p> <p>--- On-Heap Memory</p> <p>This occupies the largest block and is where most of Spark's operations run .The On-Heap memory is managed by the JVM (Java Virtual Machine). Even though Spark is written in Scala and you might write code in Python using PySpark (which uses a wrapper around Java APIs), the underlying execution still happens on the JVM.</p> <p>The On-Heap memory is further divided into four sections:</p> <ul> <li> <p>Execution Memory:      It is mainly used to store temporary data in the shuffle, join, sort, aggregation, etc. Most likely, if your pipeline runs too long, the problem lies in the lack of space here.</p> <p>Note</p> <p>Execution Memory = usableMemory * spark.memory.fraction * (1 - spark.memory.storageFraction).</p> <p>As Storage Memory, Execution Memory is also equal to 30% of all system memory by default (1 * 0.6 * (1 - 0.5) = 0.3).</p> </li> <li> <p>Storage Memory:      This is where caching (for RDDs or DataFrames) occurs, and it's also used for storing broadcast variables.     Storage Memory is used for caching and broadcasting data. </p> <p>Note</p> <p>Storage Memory = usableMemory * spark.memory.fraction * spark.memory.storageFraction</p> <p>Storage Memory is 30% of all system memory by default (1 * 0.6 * 0.5 = 0.3).</p> </li> <li> <p>User Memory:     Used for storing user objects such as variables, collections (lists, sets, dictionaries) defined in your program, or User Defined Functions (UDFs).     It is mainly used to store data needed for RDD conversion operations, such as lineage. You can store your own data structures there that will be used inside transformations. It's up to you what would be stored in this memory and how. Spark makes completely no accounting on what you do there and whether you respect this boundary or not.</p> <p>Note</p> <p>User Memory = usableMemory * (1 - spark.memory.fraction)</p> <p>It is 1 * (1 - 0.6) = 0.4 or 40% of available memory by default.</p> </li> <li> <p>Reserved Memory:      This is the memory Spark needs for running itself and storing internal objects     The most boring part of the memory. Spark reserves this memory to store internal objects. It guarantees to reserve sufficient memory for the system even for small JVM heaps.</p> <p>Note</p> <p>Reserved Memory is hardcoded and equal to 300 MB (value RESERVED_SYSTEM_MEMORY_BYTES in source code). In the test environment (when spark.testing set) we can modify it with spark.testing.reservedMemory.</p> <p>usableMemory = spark.executor.memory - RESERVED_SYSTEM_MEMORY_BYTES</p> </li> <li> <p>Unified memory:</p> <p>Unified memory refers to the Execution memory and Storage memory combined.</p> <ul> <li>Why it's \"Unified\": It's due to Spark's dynamic memory management strategy.This means if execution memory needs more space, it can use some of the storage memory, and vice-versa. There is a priority given to execution memory because critical operations like joins, shuffles, sorting, and group by happen there. The division between execution and storage is represented as a movable \"slider\".</li> </ul> <p>Evolution of Unified Memory (Pre-Spark 1.6 vs. Post-Spark 1.6):</p> <ul> <li>Before Spark 1.6: The space allocated to execution and storage memory was fixed.     If execution needed more memory but its fixed allocation was full, it could not use available space in storage memory, leading to wasted memory.</li> <li>After Spark 1.6 (&gt;= Spark 1.6): The \"slider\" became movable, allowing dynamic allocation based on needs.</li> </ul> </li> <li> <p>Rules for Slider Movement (Dynamic Allocation):</p> <p>Execution needs more memory, and Storage has vacant space: If storage is not using all its allocated space, execution can simply use that vacant portion of memory.</p> <p>Execution needs more memory, and Storage is occupied: If storage is using its blocks, it will evict some of its blocks (least recently used or LRU algorithm) to make room for execution memory.</p> <p>Storage needs more memory: In this case, because execution has priority, none of the execution blocks will be evicted. Storage must evict its own blocks (based on LRU) to free up space for new cached data</p> </li> </ul> <p>--- Off-Heap Memory</p> <p>Off-Heap memory is often the least talked about and least used, but it can be very useful in certain situations. - Default State: It is disabled by default (spark.memory.offHeap.enabled is set to zero).</p> <ul> <li>Enabling and Sizing: You can enable it by setting spark.memory.offHeap.enabled to true and specify its size using spark.memory.offHeap.size. A good starting point for its size is 10% to 20% of your executor memory.</li> <li>Structure: Similar to unified memory, off-heap memory also has two parts: execution and storage.</li> <li>Purpose/Use Case: It becomes useful when the on-heap memory is full.<ul> <li>When on-heap memory is full, a garbage collection (GC) cycle occurs, which pauses the program's operation to clean up unwanted objects. These GC pauses can negatively impact program performance.</li> <li>Off-Heap memory is managed by the Operating System, not the JVM. Therefore, it is not subject to the JVM's GC cycles.</li> </ul> </li> <li>Developer Responsibility: Since it's not subject to GC, the Spark developer is responsible for both the allocation and deallocation of memory in the off-heap space. This adds complexity and requires caution to avoid memory leaks.</li> <li>Performance: Off-heap memory is slower than on-heap memory. However, if Spark had to choose between spilling data to disk or using off-heap memory, using off-heap memory would be a better choice because writing to disk is several orders of magnitude slower</li> </ul> <p>---  Overhead Memory</p> <p>Used for internal system-level operations</p> <p>Example</p> <p>Calculation: The overhead memory is defined as the maximum of 384 MB or 10% of the spark.executor.memory. If spark.executor.memory is 10 GB, 10% of it is 1 GB. max(384 MB, 1 GB) = 1 GB. So, the overhead memory would be 1 GB.</p> <p>It's important to note that the spark.executor.memory parameter only allocates for on-heap memory. When Spark requests memory from a cluster manager (like YARN), it adds the executor memory and the overhead memory. If off-heap memory is enabled, it will also add that amount to the request.</p> <p>Example</p> <p>If spark.executor.memory is 10 GB, and overhead is 1 GB (and off-heap is disabled), Spark will request 10 GB + 1 GB = 11 GB from the cluster manager for that container</p> <p>--- Why Out of Memory Occurs Even When Spillage is Possible Despite the ability to spill data to disk from the Execution Memory Pool, an Out of Memory Exception can still occur, especially during operations like joins or aggregations:</p> <ul> <li> <p>The Problem of Data Skew: If data for a single key (e.g., ID=1) becomes excessively large (e.g., 3GB), exceeding the available Execution Memory Pool (e.g., 2.9GB), it cannot be processed.</p> </li> <li> <p>Impossibility of Partial Spillage: During operations like joins, all data related to a specific key must be present on the same executor for the operation to complete correctly. If a 3GB chunk of data for a single ID has to be processed, and only 2.9GB is available, it's impossible to spill just a portion of that key's data. Spilling half of the 3GB data would mean the join would not yield the correct result for that key. Therefore, if a single partition or a single key's data exceeds the physical memory capacity of the executor's Execution Memory Pool (even with potential spill), an Out of Memory Exception is inevitable.</p> </li> </ul> <p>--- Solutions to Out of Memory Exception</p> <ul> <li>Repartitioning: Redistributing data across more partitions.</li> <li>Salting: A technique to add a \"salt\" to skewed keys to distribute them more evenly during shuffles.</li> <li>Sorting: Pre-sorting data can sometimes help with certain types of joins (e.g., Sort-Merge Join) to reduce memory pressure.</li> </ul>"},{"location":"spark/spark/#driver-memory","title":"Driver memory","text":"<p>The Spark driver has its own dedicated memory. You can configure the driver's memory when starting a PySpark session.</p> <p>Requesting Driver Memory: To request a specific amount of driver memory, you can use the pyspark command with the --driver-memory flag: pyspark --driver-memory 1g</p> <p>This command requests 1 GB of driver memory from your local setup. After starting the session, you can verify the Spark driver memory configuration by navigating to localhost:4040/jobs in your web browser.</p> <p>---  Types of Driver Memory Within the Spark driver, there are two main types of memory that work together</p> <ul> <li> <p>JVM Heap Memory (spark.driver.memory):This is the primary memory allocated for the driver's Java Virtual Machine (JVM) processes. All JVM-related operations, such as scheduling tasks and handling responses from executors, primarily use this memory.This is what you configure using --driver-memory or spark.driver.memory.</p> </li> <li> <p>Memory Overload (spark.driver.memoryOverhead): This memory is dedicated to non-JVM processes.     It handles objects created by your application that are not part of the JVM heap.     It also accounts for the memory requirements of the application master container itself, which hosts the driver.</p> <p>Note</p> <p>By default, spark.driver.memoryOverhead is calculated as 10% of spark.driver.memory. However, there's a minimum threshold: if 10% of spark.driver.memory is less than 384 MB, then spark.driver.memoryOverhead will default to 384 MB. The system picks whichever value is higher.</p> <ul> <li>Example 1 (1GB driver memory): 10% of 1GB is 100 MB. Since 100 MB is less than 384 MB, the memoryOverhead will be 384 MB.</li> <li>Example 2 (4GB driver memory): 10% of 4GB is 400 MB. Since 400 MB is greater than 384 MB, the memoryOverhead will be 400 MB.</li> <li>Example 3 (20GB driver memory): 10% of 20GB is 2GB. In this case, the memoryOverhead would be 2GB memory**</li> </ul> </li> </ul> <p>--- Common Reasons for Driver Out of Memory</p> <p>Besides the collect() method, several other common scenarios can lead to driver OOM:</p> <ul> <li> <p>Using collect() Method on Large Datasets: As demonstrated, attempting to pull all data to the driver's memory will cause an OOM if the data size exceeds the driver's capacity.</p> </li> <li> <p>Broadcasting Large DataFrames/Tables:     Broadcasting is a technique used in Spark to optimize joins by sending a smaller DataFrame or table to all executors so that the larger DataFrame can be joined locally without shuffling data.      When you broadcast data (e.g., df2 and df3 in the example), the driver first merges and holds this data in its memory.      Then, the driver sends this combined data to all executors.      If you broadcast multiple large DataFrames (e.g., five 50 MB DataFrames, totaling 250 MB) and the driver doesn't have enough memory to hold them before distributing them, it will lead to a driver OOM error. This is why broadcasting is recommended only for small tables/DataFrames.</p> </li> <li>Excessive Object Creation and Heavy Non-JVM Processing:      If your Spark application creates many objects or performs heavy processing that falls under non-JVM operations, it consumes the memoryOverhead.      If the memoryOverhead is insufficient, it can lead to OOM errors often indicated as being \"due to memory overhead\".</li> <li> <p>Incorrect Memory Configuration:     Manually setting spark.driver.memory or spark.driver.memoryOverhead to values that are too low for the workload can lead to OOM.</p> <p>Example</p> <p>If you have a 20 GB driver but incorrectly set spark.driver.memoryOverhead to 1 GB when it should ideally be 2 GB (10% of 20GB), you might encounter an OOM error related to memoryOverhead</p> </li> </ul> <p>--- Handling and Solving Driver Out of Memory Based on the reasons for OOM, the solutions are often direct:</p> <ul> <li>Avoid collect() on Large Data:     For large datasets, never use df.collect() unless you are absolutely certain the data size is small enough to fit within the driver's memory.     Instead, use df.show() for quick inspection.     If you need to process all data, consider writing it to a file system (like HDFS or S3) or processing it in a distributed manner across executors.</li> <li>Manage Broadcasted Data Carefully:     Only broadcast DataFrames or tables that are genuinely small.     Before broadcasting, ensure the driver's memory (specifically the JVM heap) is large enough to hold the combined size of all dataframes you plan to broadcast.</li> <li>Increase Driver Memory and Memory Overhead:     If your application performs extensive non-JVM operations or creates many objects, you might need to increase spark.driver.memory and/or spark.driver.memoryOverhead.     If you observe \"due to memory overhead\" errors, explicitly increasing spark.driver.memoryOverhead beyond its default 10% (while respecting system limits) might resolve the issue</li> </ul>"},{"location":"spark/spark/#spark-submit","title":"Spark Submit","text":"<p>Spark Submit is a command-line tool that allows you to trigger or run your Spark applications on a Spark cluster. It packages all the required files and JARs (Java Archive files) and deploys them to the Spark cluster for execution. It is used to run jobs on various types of Spark clusters</p> <p>--- Where is Your Spark Cluster Located?</p> <p>Spark clusters can be deployed in multiple environments. When using Spark Submit, you specify the location of your master node.</p> <p>Common cluster types include:</p> <ul> <li>Standalone Cluster: A simple, self-contained Spark cluster. An example master configuration for a standalone cluster could look like spark://10.160.78.10:7077, where 7077 is the default port.</li> <li>Local Mode: For running Spark applications on your local machine, typically for development or testing. The master configuration is simply local.</li> <li>YARN (Yet Another Resource Negotiator): A popular resource management system in the Hadoop ecosystem. The master configuration is yarn.</li> <li>Kubernetes: A container orchestration system.</li> <li>Mesos: Another cluster management platform</li> </ul> <p>Example</p> <p>spark-submit --master {stanadlone,yarn.mesos,kubernetes} --deploy-mode {client/cluster} --class mainclass.scala  --jars mysql-connector.jar  --conf spark.dynamicAllocation.enabled=true   --conf spark.dynamicAllocation.minExecutors=1   --conf spark.dynamicAllocation.maxExecutors=10   --conf spark.sql.broadcastTimeout=3600   --conf spark.sql.autobroadcastJoinThreshold=100000   --conf spark.executor.cores=2   --conf spark.executor.instances=5   --conf spark.default.parallelism=20   --conf spark.driver.maxResultSize=1G   --conf spark.network.timeout=800  --conf spark.driver.maxResultSize=1G   --conf spark.network.timeout=800   --driver-memory 1G   --executor-memory 2G   --num-executors 5   --executor-cores 2   --py-files /path/to/other/python/files.zip  /path/to/your/python/wordcount.py    /path/to/input/textfile.txt </p> <ul> <li> <p>master: This is the master URL for the cluster. It can be a URL for any Spark-supported cluster manager. For example, local for local mode, spark://HOST:PORT for standalone mode, mesos://HOST:PORT for Mesos, or yarn for YARN.</p> </li> <li> <p>deploy-mode: This can be either client (default) or cluster. In client mode, the driver runs on the machine from which the job is submitted. In cluster mode, the framework launches the driver inside the cluster.</p> </li> <li> <p>class: This is the entry point for your application, i.e., where your main method runs. For Java and Scala, this would be a fully qualified class name.</p> </li> <li> <p>jars: This argument allows you to provide paths to external JAR files that your Spark application depends on. You can provide multiple JAR files as a comma-separated list. It's recommended to use absolute paths for JAR files to prevent future issues, even if they are in the same directory</p> </li> <li> <p>conf: This is used to set any Spark property. For example, you can set Spark properties like spark.executor.memory, spark.driver.memory, etc.</p> <ul> <li>spark.dynamicAllocation.enabled true: Enables dynamic memory allocation.</li> <li>spark.dynamicAllocation.minExecutors 1: Sets the minimum number of executors to 1.</li> <li>spark.dynamicAllocation.maxExecutors 10: Sets the maximum number of executors to 10. This prevents a single process from hogging all resources. This is beneficial because if a process reserves memory but doesn't use it, dynamic allocation can free up that idle memory for other processes.</li> <li>Broadcast Threshold: This configuration determines the maximum size of data that Spark will automatically broadcast to all worker nodes when performing a join. The default is 10MB.</li> <li>Broadcast Timeout: This sets the maximum time (in seconds) that a broadcast operation is allowed to take before timing out. A common general setting might be 600 seconds (10 minutes) or 1200 seconds (20 minutes), while 3600 seconds (1 hour) is considered very long and can significantly delay job completion</li> <li>spark.executor.cores=2 sets the number of cores to use on each executor.</li> <li>spark.executor.instances=5: sets the number of executor instances</li> <li>spark.default.parallelism=20: sets the default number of partitions in RDDs returned by transformations like join(), reduceByKey(), and parallelize() when not set by user.</li> <li>spark.driver.maxResultSize=1G:  limits the total size of the serialized results of all partitions for each Spark action (e.g., collect). This should be at least as large as the largest object you want to collect.</li> <li>spark.network.timeout=800:  sets the default network timeout value to 800 seconds. This configuration plays a vital role in cases where you deal with large shuffles.</li> </ul> </li> <li> <p>driver-memory: Specifies the amount of memory allocated to the Spark Driver program.</p> </li> <li> <p>executor-memory: Specifies the amount of memory allocated to each Spark Executor.</p> </li> <li> <p>num-executors: Specifies the total number of executors to launch for the application. Combined with --executor-memory, this implies the total executor memory required (e.g., 2 GB/executor  5 executors = 10 GB total executor memory).</p> </li> <li> <p>executor-cores: Specifies the number of CPU cores allocated to each executor. This determines how many parallel tasks an executor can run</p> </li> <li> <p>files: This argument is used to specify non-Python files (e.g., configuration files like .ini, .json, .csv) that your Spark application needs. Similar to --py-files, these are bundled and distributed to all worker nodes</p> </li> </ul> <p>After all the Spark Submit configurations, you provide the path to your main application script (e.g., main.py). Any values provided after the main script are treated as command-line arguments that can be accessed within your script.</p> <ul> <li> <p>main.py: This is typically accessed as sys.argv in Python.</p> </li> <li> <p>Subsequent arguments: These are sys.argv, sys.argv, and so on. They are useful for passing dynamic parameters like environment names (e.g., dev, qa, prod) to control execution flow within your script without changing the script itself</p> </li> <li> <p>application-jar: This is a path to your compiled Spark application.</p> </li> <li> <p>application-arguments: These are arguments that you need to pass to your Spark application</p> </li> </ul> <p></p> <p>--- Client Mode</p> <p></p> <p>In Client mode, the Spark Driver runs directly on the edge node (or the machine from which the spark-submit command is executed).</p> <p>The Executors, however, still run on the worker nodes within the cluster.</p> <ul> <li>Advantages:<ul> <li>Easy Debugging and Real-time Logs: Logs (STD OUT and STD ERR) are generated directly on the client machine (edge node). This makes it very easy for developers to monitor the process, see real-time output, debug issues, and observe errors as they occur. This mode is highly suitable for development and testing of small code snippets.</li> </ul> </li> <li>Disadvantages:<ul> <li>Vulnerability to Edge Node Shutdown: If the edge node is shut down, either accidentally or intentionally, the Spark Driver (running on it) will be terminated. Since the Driver coordinates the entire application, its termination will cause all associated Executors to be killed, leading to the entire Spark job stopping abruptly and incompletely.</li> <li>High Network Latency: Communication between the Driver (on the edge node) and the Executors (on worker nodes in the cluster) involves two-way communication across the network. This can introduce network latency, especially for operations like Broadcaster, where data needs to be first sent to the Driver and then distributed to Executors.</li> <li>Potential for Driver Out of Memory (OOM) Errors: If multiple users submit jobs in Client mode from the same edge node, and their collective Driver memory requirements exceed the edge node's physical memory capacity (which is typically lower than worker nodes), processes may fail to start or encounter Driver OOM errors</li> </ul> </li> </ul> <p>When u start a spark shell, application driver creates the spark session in your local machine which request to Resource Manager present in cluster to create Yarn application. YARN Resource Manager start an Application Master (AM container). For client mode Application Master acts as the Executor launcher. Application Master will reach to Resource Manager and request for further containers.  Resource manager will allocate new containers. These executors will directly communicate with Drivers which is present in the system in which you have submitted the spark application.</p> <p>--- Cluster Mode</p> <p></p> <p>For cluster mode, there\u2019s a small difference compare to client mode in place of driver. Here Application Master will create driver in it and driver will reach to Resource Manager.</p> <p>In Cluster mode, the Spark Driver (Application Master container) is launched and runs on one of the worker nodes within the Spark cluster. The Executors also run on other worker nodes in the cluster.</p> <ul> <li>Advantages:<ul> <li>Resilience and Disconnect-ability: Once a Spark job is submitted in Cluster mode, the Driver runs independently within the cluster. This means the user can disconnect from or even shut down their edge node machine without affecting the running Spark application. This makes it ideal for long-running jobs.</li> <li>Low Network Latency: Both the Driver and the Executors are running within the same cluster. This proximity significantly reduces network latency between them, leading to more efficient data transfer and communication.</li> <li>Scalability and Resource Utilization: Worker nodes are provisioned with significant memory and processing capabilities. By running the Driver on a worker node, the application can leverage the cluster's robust resources, reducing the likelihood of Driver OOM issues, even with many concurrent jobs.</li> <li>Suitable for Production Workloads: Cluster mode is the recommended deployment mode for production workloads, especially for scheduled jobs that run automatically and do not require constant real-time monitoring on the client side.</li> </ul> </li> <li>Disadvantages:     Indirect Log Access: Logs and output are not directly displayed on the client machine. When a job is submitted in Cluster mode, an Application ID is generated. Users must use this Application ID to access the Spark Web UI (User Interface) to track the job's status, progress, and logs. This adds an extra step for monitoring compared to Client mode</li> </ul> <p>--- Local Mode</p> <p></p> <p>In local mode, Spark runs on a single machine, using all the cores of the machine. It is the simplest mode of deployment and is mostly used for testing and debugging.</p> <p>--- Comparison</p> Feature Client Mode Cluster Mode Driver Location Edge Node (or client machine) Worker Node within the cluster Log Generation On client machine (STD OUT, STD ERR) Application ID generated; view via Spark Web UI Debugging Easy, real-time feedback Requires checking Spark Web UI Network Latency High (Driver &lt;-&gt; Executors across network) Low (Driver &lt;-&gt; Executors within cluster) Edge Node Shutdown Application stops (Driver killed) Application continues to run Driver Out of Memory Higher chance if many users/low edge node memory Lower chance (cluster has more resources) Use Case Development, small code snippets, debugging Production workloads, long-running jobs"},{"location":"spark/spark/#aqe","title":"AQE","text":"<p>--- Key Features of AQE</p> <p>AQE provides three main capabilities to improve performance: 1.  Dynamically Coalescing Shuffle Partitions. 2.  Dynamically Switching Join Strategies. 3.  Dynamically Optimizing Skew Joins.</p> <p>--- Dynamically Coalescing Shuffle Partitions</p> <p>When Spark shuffles data (e.g., during a <code>groupBy</code> or <code>join</code>), it creates a default number of shuffle partitions\u2014usually 200.</p> <p>If the dataset is small, 200 partitions result in many empty or tiny partitions. This wastes resources because the Spark scheduler must still manage, schedule, and monitor tasks for these empty partitions, leading to unnecessary overhead.</p> <p>AQE's Shuffle Reader observes the data size after the shuffle. If it finds many small partitions, it merges (coalesces) them into a smaller number of larger partitions at runtime.</p> <p>Example</p> <p>A 25 MB dataset originally split into 200 partitions might be coalesced by AQE into a single partition, reducing the number of tasks from 200 to 1. This saves CPU cores and scheduling time.</p> <pre><code># External Information: Enabling AQE and Coalescing\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n</code></pre> <p>--- Dynamically Switching Join Strategies</p> <p>Spark usually decides a join strategy (like Sort-Merge Join or Broadcast Hash Join) during the initial planning phase based on the estimated size of the tables.</p> <p>Initial estimates can be wrong. For example, a 10 GB table might be reduced to 5 MB after several filters and transformations. Without AQE, Spark would stick to the slower Sort-Merge Join.</p> <p>AQE monitors the actual size of the data after transformations. If one side of the join becomes small enough (e.g., less than 10 MB), AQE dynamically switches the strategy from Sort-Merge Join to Broadcast Hash Join at runtime. Broadcast joins are significantly faster as they avoid the expensive shuffling and sorting required by Sort-Merge joins.</p> <pre><code># External Information: Conceptual Spark SQL example\n# Initial tables are large (10GB and 20GB), triggering Sort-Merge Join\ndf1 = spark.table(\"fact_sales\") # 10GB\ndf2 = spark.table(\"dim_products\") # 20GB\n\n# A filter is applied that reduces dim_products to 5MB\nfiltered_df2 = df2.filter(df2.category == \"Electronics\") \n\n# With AQE enabled, Spark switches to Broadcast Join at runtime \n# because filtered_df2 is now &lt; 10MB.\nresult = df1.join(filtered_df2, \"product_id\")\n</code></pre> <p>--- Dynamically Optimizing Skew Joins</p> <p>Data skew occurs when data is distributed unevenly across partitions. For instance, if \"Sugar\" accounts for 80% of sales, the partition containing \"Sugar\" will be much larger than others.</p> <p>One large partition causes a \"199 out of 200 tasks completed\" scenario, where one task takes a very long time or fails with an Out of Memory (OOM) error.</p> <p>AQE identifies skewed partitions and splits them into smaller sub-partitions.     1.  The large partition (e.g., Sugar data) is split into multiple smaller parts.     2.  To ensure the join still works, the corresponding data on the other side of the join (the non-skewed table) is duplicated for each new sub-partition.    Thresholds for Skew: AQE identifies a partition as skewed if:     1.  The partition size is greater than 256 MB.     2.  The partition size is more than 5 times the median partition size.</p> <pre><code># External Information: AQE Skew Join Configurations\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", \"5\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"256MB\")\n</code></pre>"},{"location":"spark/spark/#cache-persist","title":"Cache &amp; Persist","text":"<p>The methods persist() and cache() in Apache Spark are used to save the RDD, DataFrame, or Dataset in memory for faster access during computation. They are effectively ways to optimize the execution of your Spark jobs, especially when you have repeated transformations on the same data. However, they differ in how they handle the storage:</p> <p>--- cache</p> <ul> <li> <p>What is Caching in Spark? Caching is an optimization technique in Spark that allows you to store intermediate results in memory. This prevents Spark from re-calculating the same data repeatedly when it is used multiple times in subsequent transformations.</p> </li> <li> <p>Where is Cached Data Stored? When data is cached, it is stored within the Storage Memory Pool of a Spark Executor. An executor's memory is divided into three parts: User Memory, Spark Memory, and Reserved Memory. Spark Memory, in turn, contains two pools: Storage Memory Pool and Execution Memory Pool. Cached data specifically resides in the Storage Memory Pool. If the Storage Memory Pool fills up, Spark might evict (remove) data that is not frequently used (using an LRU - Least Recently Used - fashion) or spill it to disk.</p> </li> <li> <p>Why Do We Need Caching? Spark operates with lazy evaluation, meaning transformations are not executed until an action is called. When an action is triggered, Spark builds a Directed Acyclic Graph (DAG) to determine the lineage of the data. If a DataFrame (DF) is used multiple times, Spark will re-calculate it from the beginning each time it's referenced, because DataFrames are immutable and executors' memories are short-lived</p> </li> <li> <p>How Caching Helps: By calling .cache() on df, its intermediate result is stored in the Storage Memory Pool. Now, whenever df is needed again, Spark directly retrieves it from memory instead of re-calculating it. This significantly reduces computation time and improves efficiency</p> </li> <li> <p>When Not to Cache? You should avoid caching data when the DataFrame is very small or when its re-calculation time is negligible. Caching consumes memory, and if the benefits of caching don't outweigh the memory consumption, it's better to avoid it.</p> </li> <li> <p>Limitations of Caching:  If the cached data's partitions are larger than the available Storage Memory Pool, the excess partitions will not be stored in memory and will either be re-calculated on the fly or spilled to disk if using a storage level that supports it. Spark does not store partial partitions; a partition is stored entirely or not at all.  If a cached partition is lost (e.g., due to an executor crash), Spark will re-calculate it using the DAG lineage</p> </li> <li> <p>How to Uncache Data - To remove data from the cache, you can use the .unpersist() method.</p> </li> </ul> <p>When you call df.cache(), it internally calls df.persist() with a default storage level of MEMORY_AND_DISK.</p> <p>persist() offers more flexibility because it allows you to specify the desired storage level as an argument</p> <p>--- persist(storageLevel)</p> <p>Storage levels define where data is stored (memory, disk, or both) and how it is stored (serialized or deserialized, and with replication). These levels provide fine-grained control over how cached data is managed, balancing performance, fault tolerance, and memory usage.</p> <p>To use StorageLevel with persist(), you need to import it: from pyspark import StorageLevel</p> <p>Here are the different storage levels explained:</p> <ul> <li> <p>MEMORY_ONLY:</p> <p>Stores data only in RAM (deserialized form). If memory is insufficient, partitions will be re-calculated when needed. Fastest processing because data is in memory and readily accessible. High memory utilization, potentially limiting other operations.</p> <p>For small to medium-sized datasets that fit entirely in memory and where re-calculation overhead is high.</p> </li> <li> <p>MEMORY_AND_DISK:</p> <p>Default for cache(). Attempts to store data in RAM first (deserialized form). If RAM is full, excess partitions are spilled to disk (serialized form). Provides a good balance of speed and resilience; data is less likely to be re-calculated.</p> <p>DisDisk access is slower than memory. Data read from disk (serialized) requires CPU to deserialize it, leading to higher CPU utilization. For larger datasets that might not fully fit in memory but where performance is still critical.</p> </li> <li> <p>MEMORY_ONLY_SER:</p> <p>Stores data in RAM only, but in a serialized form. Serialization saves memory space, allowing more data to be stored in the same amount of RAM (e.g., 5GB uncompressed might become 8GB serialized). DisData needs to be deserialized by the CPU when accessed, leading to higher CPU utilization and slightly slower access compared to MEMORY_ONLY. This serialization specifically works for Java and Scala objects, and not for Python objects (though Python has its own pickling mechanisms, the _SER storage levels in Spark are typically for JVM objects). When memory is a major constraint and you can tolerate increased CPU usage for deserialization.</p> </li> <li> <p>MEMORY_AND_DISK_SER:</p> <p>Stores data first in RAM (serialized), then spills to disk (serialized) if memory is full. Combines memory saving of serialization with resilience of disk storage. DisHigh CPU usage due to deserialization for both memory and disk reads. For very large datasets where memory constraints are severe and some CPU overhead for deserialization is acceptable.</p> </li> <li> <p>DISK_ONLY:</p> <p>Stores data only on disk (serialized form). Slowest storage level due to reliance on disk I/O. Good for extremely large datasets that don't fit in memory, or for fault tolerance where data needs to be durable across executor restarts. DisSignificantly slower than memory-based storage levels. When performance is less critical than fault tolerance or when datasets are too large for memory.</p> </li> <li> <p>Replicated Storage Levels (e.g., MEMORY_ONLY_2, DISK_ONLY_2):</p> <p>These levels store two copies (2x replicated) of each partition across different nodes. For example, MEMORY_ONLY_2 stores two copies in RAM on different executors. Provides fault tolerance. If one executor or worker node goes down, the data can still be accessed from its replica, avoiding re-calculation from the DAG. DisDoubles memory/disk consumption compared to non-replicated versions. For highly critical data that is complex to calculate and must be readily available even if a node fails. Generally, cache() (which is MEMORY_AND_DISK) is preferred unless specific fault tolerance is required</p> </li> </ul> <p>--- Choosing the Right Storage Level</p> <p>The choice of storage level depends on your specific needs:</p> <ul> <li>Start with MEMORY_ONLY: If your data fits in memory and transformations are simple, this is the fastest.</li> <li>Move to MEMORY_AND_DISK: If data is larger and might not fit entirely in memory, or if re-calculation is expensive. This is the most commonly used for general caching.</li> <li>Consider _SER options: Only if memory is a severe bottleneck and you can tolerate increased CPU usage for serialization/deserialization. Note that in Python, direct serialization using _SER levels like in Java/Scala might not provide the same benefits.</li> <li>Use _2 options: Only for critical, complex data where high fault tolerance is a must and you can afford the doubled storage cost.</li> </ul>"},{"location":"spark/spark/#salting","title":"Salting","text":"<p>Data Skew occurs when data is not distributed evenly across partitions. In a join operation, if one specific key (e.g., ID 1) has a massive number of records compared to others, all those records are sent to a single executor based on their hash.</p> <p>The executor handling the skewed key becomes a bottleneck, taking significantly longer to finish while others sit idle, or it may even crash the application.</p> <p>Example</p> <p>A \"best-selling product\" might account for 90% of sales data, causing a massive skew for that specific product ID during a join.</p> <p>--- Why Other Methods Might Fail</p> <ul> <li>Re-partitioning: Standard re-partitioning often fails to solve skew because records with the same key are still hashed to the same partition.</li> <li>Broadcasting: While broadcasting the smaller table avoids shuffling, it is only viable if the table is small (e.g., &lt;10MB). If both tables are large, broadcasting is not an option.</li> <li>AQE (Adaptive Query Execution): While Spark\u2019s AQE provides some optimizations, it may not always resolve complex skew issues manually.</li> </ul> <p>Salting involves adding a random value (the \"salt\") to the join key to break a single large partition into multiple smaller ones.</p> <ul> <li> <p>Step 1: Modifying the Skewed Table (Left Table)</p> <p>In the skewed table, you append a random number (e.g., between 1 and 10) to the join key. This forces the records for the same ID to be distributed across 10 different partitions instead of one.</p> </li> <li> <p>Step 2: Modifying the Reference Table (Right Table)</p> <p>If you only salt the left table, the join will fail because the keys no longer match (e.g., \"ID_1\" vs \"ID_1_5\"). To fix this, you must replicate (explode) every record in the right table for every possible salt value used in the left table.</p> </li> <li> <p>Before Salting: A task might show a massive gap between the minimum duration (e.g., 0.2 seconds) and the maximum duration (e.g., 6 seconds) because one executor is struggling with the skewed partition.</p> </li> <li> <p>After Salting: The workload is balanced. The average time might be 9 seconds with a maximum of 12 seconds. While the total work might slightly increase due to replication, the overall job finishes faster because executors work in parallel rather than waiting for one skewed task to finish.</p> </li> </ul>"},{"location":"spark/spark/#dynamice-resource-allocation","title":"Dynamice Resource Allocation","text":"<p>Dynamic Resource Allocation refers to the ability of a Spark application to dynamically increase or decrease the number of executors it uses based on the workload. This means resources are added when tasks are queued or existing ones need more processing power, and released when they become idle.</p> <p>To make processes run faster and ensure other processes also get sufficient resources.</p> <p>DRA is a cluster-level optimization technique, contrasting with code-level optimizations like join tuning, caching, partitioning, and coalescing.</p> <p>--- Static vs. Dynamic Resource Allocation Techniques</p> <p>Spark offers two primary resource allocation techniques:</p> <ul> <li> <p>Static Resource Allocation:     In this approach, the application requests a fixed amount of memory (e.g., 100 GB) at the start, and it retains that allocated memory for the entire duration of the application's run, regardless of whether the memory is actively used or idle.     Default behavior in Spark if DRA is not explicitly enabled.</p> <p>Can lead to resource wastage if the application doesn't constantly utilize all allocated resources, making them unavailable for other jobs. This is particularly problematic for smaller jobs that have to wait for large, static jobs to complete, even if the larger job is underutilizing its resources.</p> <p>Example</p> <p>A heavy job requests 980 GB for executors and 20 GB for the driver (total 1 TB) on a 1 TB cluster. This saturates the entire cluster, leaving no resources for other users' jobs, even small ones. The resource manager, often operating on a First-In, First-Out (FIFO) policy, will make subsequent jobs wait until resources are freed.</p> </li> <li> <p>Dynamic Resource Allocation:     Dynamically adjusts resources by acquiring more executors when needed and releasing them when they become idle.</p> <p>Optimizes cluster utilization by making resources available to other applications when they are not actively being used by a particular job.</p> </li> </ul> <p>--- How Dynamic Resource Allocation Works in Detail</p> <ul> <li> <p>Initial Resource Request and Configuration</p> <p>A Spark application typically requests resources using the spark-submit command. For DRA to work, specific configurations must be set.</p> <p>Note</p> <ul> <li>spark.dynamicAllocation.enabled=true: This is the primary configuration to enable Dynamic Resource Allocation. By default, it is set to false (disabled).</li> <li>spark.dynamicAllocation.minExecutors: Specifies the minimum number of executors that the application will always retain, even if idle. This helps prevent the process from failing if it releases too many resources and cannot re-acquire them when needed later. For example, setting it to 20 ensures at least 20 executors are always available.</li> <li>spark.dynamicAllocation.maxExecutors: Specifies the maximum number of executors that the application can acquire. This acts as an upper limit for resource consumption.</li> <li>--executor-memory 20G: Sets the memory for each executor.</li> <li>--executor-cores 4: Sets the number of CPU cores for each executor, determining how many parallel tasks each executor can run.</li> <li>--driver-memory 20G: Sets the memory for the driver program.</li> </ul> </li> <li> <p>Resource Release Mechanism</p> <p>When an application no longer needs all its allocated resources, Spark can release them.</p> <p>Resources are released when executors become idle, meaning they are not actively performing tasks.</p> <p>By default, an executor will be released if it remains idle for 60 seconds. This can be configured using spark.dynamicAllocation.executorIdleTimeout.</p> <p>Example</p> <p>Setting spark.dynamicAllocation.executorIdleTimeout=45s will release idle executors after 45 seconds.</p> <p>Spark internally manages the release of resources. The resource manager (e.g., YARN, Mesos) does not directly request Spark applications to release resources. For instance, if a process initially needed 1000 GB for a join but then transitions to a filter operation requiring only 500 GB, the extra 500 GB can be released, making them available for other processes.</p> </li> <li> <p>Resource Acquisition (Demand) Mechanism</p> <p>When an application's workload increases and it needs more resources, Spark will request them.</p> <p>The driver program identifies the need for more memory/executors (e.g., for a large join operation). Spark starts requesting more resources if it experiences a backlog of pending tasks for a certain duration. The default is 1 second. This can be configured using spark.dynamicAllocation.schedulerBacklogTimeout.</p> <p>Example</p> <p>Configuration: Setting spark.dynamicAllocation.schedulerBacklogTimeout=2s will cause Spark to request resources after 2 seconds of a task backlog.</p> <p>Spark does not request all needed resources at once. Instead, it requests them in a 2-fold manner (doubling the requested executors each time): Initially, it might request 1 additional executor. If that's not enough, it will request 2 more. Then 4, then 8, then 16, and so on, until the required resources are met or maxExecutors is reached.</p> </li> </ul> <p>--- Challenges and Solutions in Dynamic Resource Allocation</p> <p>While DRA offers significant benefits, it also presents challenges that need to be addressed:</p> <ul> <li> <p>Challenge 1: Process Failure Due to Resource Unavailability     If an application releases too many resources, and other processes quickly acquire them, the original application might not be able to get back the needed resources on demand, potentially leading to process failure, especially for memory-intensive operations like joins.</p> <p>Solution: Configure spark.dynamicAllocation.minExecutors and spark.dynamicAllocation.maxExecutors.</p> <ul> <li>By setting a minExecutors value (e.g., 20 out of 49), you ensure that a baseline number of executors is always available to your application, preventing it from completely running out of resources and failing even if it releases others.</li> <li>maxExecutors prevents over-allocation and resource monopolization.</li> </ul> </li> <li> <p>Challenge 2: Loss of Cached Data or Shuffle Output     When executors are released, any cached data or shuffle output written to the local disk of those executors would be lost. This would necessitate re-calculation of that data, negating the performance benefits of DRA.</p> <p>Solution: External Shuffle Service and Shuffle Tracking.</p> <ul> <li>External Shuffle Service (spark.shuffle.service.enabled=true): This service runs independently on worker nodes and is responsible for storing shuffle data. This ensures that even if an executor or worker node is released, the shuffle data persists and can be retrieved later by other executors or if the original executor is re-acquired.</li> <li>Shuffle Tracking (spark.dynamicAllocation.shuffleTracking.enabled=true): This configuration ensures that shuffle output data is not deleted even when an executor is released. It works in conjunction with the external shuffle service to prevent the need for re-calculation of shuffled data throughout the Spark application's lifetime.</li> </ul> </li> </ul> <p>--- When to Avoid Dynamic Resource Allocation</p> <p>While DRA is generally beneficial, there are specific scenarios where it should be avoided:</p> <ul> <li>Critical Production Processes: For critical production jobs where any delay or potential failure due to resource fluctuation is unacceptable, it is advisable to use Static Memory Allocation. This ensures predictable resource availability and minimizes risk.</li> <li>Non-Critical Processes / Development: For processes that have some bandwidth for resource fluctuations, or for development and testing environments, Dynamic Resource Allocation is highly recommended</li> </ul> <p>--- Dynamic Partition Pruning (DPP)</p> <p>Dynamic Partition Pruning (DPP) is an optimization technique in Apache Spark that enhances query performance, especially when dealing with partitioned data and join operations.</p> <ol> <li> <p>Understanding Partition Pruning</p> <p>Before diving into Dynamic Partition Pruning, it's essential to understand standard Partition Pruning.</p> <p>Partition pruning is a mechanism where Spark avoids reading unnecessary data partitions based on filter conditions. It \"prunes\" or removes data that is not relevant to the query.</p> <p>When data is partitioned on a specific column (e.g., sales_date), and a query applies a filter directly on that partitioning column, Spark can identify and read only the partitions that contain the relevant data. This significantly reduces the amount of data scanned.</p> <p>Example</p> <ul> <li>Imagine a large sales_data dataset partitioned by sales_date. Each date has its own partition.</li> <li>If you run a query like SELECT  FROM sales_data WHERE sales_date = '2019-04-19', Spark, with partition pruning enabled, will only read the data for April 19, 2019.</li> <li>Observed in Spark UI: In this example, if there are 123 total partitions, Spark will only read 1 file. The Spark UI's \"SQL\" tab details will show \"Partition Filter\" applied, indicating that the date has been cast and used for filtering.</li> </ul> </li> <li> <p>The Issue: When Standard Partition Pruning Fails</p> <p>Standard partition pruning works efficiently when the filter condition is directly applied to the partitioning column of the table being queried. However, a common scenario where it fails is when: You have two dataframes (or tables), say df1 and df2. df1 is a partitioned table (e.g., partitioned by date). You need to join df1 and df2. The filter condition originates from df2 (the non-partitioned table or the table that is not the primary partitioned table being filtered). In such a case, because the filter is applied on df2 and not directly on df1's partitioning column, Spark's optimizer (without DPP) won't know which partitions of df1 to prune at the planning stage.</p> <p>Example</p> <ul> <li>Data: df1 is sales_data (partitioned by sales_date) and df2 is a date_dimension table (containing date and week_of_year columns).</li> <li>Goal: Find sales data for a specific week, e.g., week = 16.</li> <li>Query Concept: df1 is joined with df2 (e.g., on date columns), and then df2 is filtered for week_of_year = 16.</li> <li>Configuration for Demonstration: To observe this issue, Spark's default behavior needs to be overridden by explicitly disabling Dynamic Partition Pruning (spark.sql.set('spark.sql.optimizer.dynamicPartitionPruning.enabled', 'false')) and also potentially disabling broadcast joins.</li> <li>Observed in Spark UI: When this query is run with DPP disabled, Spark will scan all 123 files of the sales_data table, even though only a few dates (and thus partitions) might be relevant for week 16. The \"Partition Filter\" section in the Spark UI for df1 will show no effective pruning related to the join condition. This leads to performance degradation.</li> </ul> </li> <li> <p>Dynamic Partition Pruning (DPP): The Solution</p> <p>Dynamic Partition Pruning (DPP) addresses the performance issue described above by enabling Spark to prune partitions at runtime.</p> <p>DPP is an optimization technique that allows Spark to update filter conditions dynamically at runtime.</p> <ul> <li>Filter Small Table: Spark first filters the smaller table (df2, e.g., date_dimension for week = 16) to identify the relevant values (e.g., specific dates that fall in week 16).</li> <li>Broadcast: The relevant values (e.g., the list of specific dates) from the filtered smaller table are broadcasted to all executor nodes. Broadcasting makes this small dataset available on all nodes where the larger table is processed.</li> <li>Subquery Injection: At runtime, Spark then uses these broadcasted values to create a subquery (similar to an IN clause) for the partitioned table (df1). For instance, it essentially transforms the query to look like: SELECT  FROM big_table WHERE sales_date IN (SELECT dates FROM small_table).</li> <li>Dynamic Pruning: This subquery allows Spark to dynamically identify and prune the irrelevant partitions of the large table (df1), reading only the necessary ones.</li> </ul> <p>Example</p> <ul> <li>Using the same sales_data (df1) and date_dimension (df2) tables, and the join with week = 16 filter.</li> <li>Configuration: DPP is enabled (by default in Spark 3.0+ or explicitly enabled) and the broadcast mechanism is active.</li> <li>Observed in Spark UI: When run with DPP enabled, Spark will only read a small subset of files (e.g., 3 files out of 123 total partitions), as only those files contain the dates relevant to week 16. The \"Partition Filter\" in the Spark UI will clearly show a \"Dynamic Pruning Expression\" applied to sales_date. You will also see \"Broadcast Exchange\" in the execution plan, indicating that the smaller table was broadcasted.</li> </ul> </li> <li> <p>Key Conditions for Dynamic Partition Pruning</p> <p>For Dynamic Partition Pruning to work effectively, two primary conditions must be met:</p> <ol> <li>Partitioned Data: The data in the larger table (df1 in our example) must be partitioned on the column used in the join and filter condition (e.g., sales_date). If the data is not partitioned, DPP cannot apply.</li> <li>Broadcastable Second Table: The second table (df2), which provides the filter condition, must be broadcastable. This means it should be small enough to fit into memory and be efficiently broadcasted to all executor nodes. If it's too large, it won't be broadcasted, and DPP might not engage. You can also adjust Spark's broadcast threshold value if needed.</li> </ol> </li> </ol>"},{"location":"spark/spark/#spark-streaming","title":"Spark Streaming","text":"<p>--- Converting Batch to Streaming Code</p> <p>One of the core benefits of Spark is that converting batch code to streaming is straightforward.</p> <ol> <li>Reading: Change <code>.read</code> to <code>.readStream</code>.</li> <li>Source: Change the format from <code>text</code> to <code>socket</code> and provide <code>host</code> and <code>port</code> options.</li> <li>Transformations: The logic for <code>split</code>, <code>explode</code>, and <code>groupBy</code> remains exactly the same.</li> <li>Writing: Instead of <code>.show()</code>, use <code>.writeStream</code> with a defined <code>format</code> (e.g., <code>console</code>) and an <code>outputMode</code>.</li> <li>Driver Connection: Add <code>.awaitTermination()</code> to ensure the driver stays active while the streaming application runs on executors.</li> </ol> <p>--- Key Streaming Concepts</p> <ul> <li> <p>Output Mode (\"complete\"): In this mode, the entire updated result table is displayed in the console every time a new batch is processed.</p> </li> <li> <p>Micro-Batches: When you type \"hello world\" into the terminal, Spark processes it as the first batch. If you type \"world\" again, it processes a second batch and updates the count for \"world\" to 2.</p> </li> <li> <p>Checkpoints: Spark automatically creates a temporary checkpoint location to store metadata for the streaming application. Checkpoints ensure that the application can track its progress and state.</p> </li> <li> <p>Schema Handling: For socket sources, Spark defaults to a schema with a single column called <code>value</code> of type string.</p> </li> </ul> <p>--- Spark Streaming Output Modes</p> <p>Spark offers three primary output modes that define how the result of a transformation is written to the output sink.</p> <ul> <li>Complete Mode</li> </ul> <p>In Complete Mode, the entire updated result table is written to the sink every time a micro-batch is processed.Even if a specific record was not present in the current micro-batch, it will still appear in the output if it was part of a previous batch.</p> <ul> <li>Update Mode</li> </ul> <p>In Update Mode, Spark only outputs the rows that were updated or newly added in the most recent micro-batch. If a record from a previous batch is not updated by the new data, it is excluded from the current output.</p> <ul> <li>Append Mode</li> </ul> <p>In Append Mode, only new rows added to the result table since the last trigger are outputted. Once data is written, it is \"locked\" and cannot be updated. This makes it ideal for logging where data is only added at the bottom. </p> <p>Note</p> <p>Append mode is often used in conjunction with watermarks to handle stateful data, which is discussed in more advanced sessions.</p> <p>So in summary:</p> <ul> <li>Complete mode gives you the entire Result Table every time. It's like taking a complete snapshot of your computation's current state after every batch.</li> <li>Append mode only gives you completely new rows. It's suitable for when your Result Table is effectively growing over time with new data, such as when you're just adding new rows and not updating existing ones.</li> <li>Update mode gives you any rows that are new or have been updated. It's a middle ground between complete and append mode, giving you a view of what's changed in your Result Table since the last batch.</li> </ul> <p>--- Lambda Architecture</p> <p>Lambda architecture is defined by having two distinct processing pipelines: a Batch Layer and a Speed (Streaming) Layer.</p> <ul> <li>How it Works:     Batch Layer: Processes raw data in large volumes at scheduled intervals (e.g., daily or weekly). This layer is used when high latency is acceptable.     Speed Layer: Also known as the streaming pipeline, it processes data in real-time for immediate insights.     Serving Layer: A single, common layer that connects to various applications (real-time, batch, or mixed) and queries the results from both the batch and speed layers.</li> <li>Challenges:     Code Duplication: Because there are two separate pipelines, you often have to write and maintain the same logic twice\u2014once for the batch code and once for the streaming code.     Latency: Data processed through the batch pipeline is subject to delays because it only runs based on a schedule.</li> </ul> <p>--- Kappa Architecture</p> <p>Kappa architecture is a simpler alternative that utilizes only one processing pipeline: the Speed Layer.</p> <ul> <li> <p>How it Works:</p> <p>Unified Pipeline: The same streaming pipeline is used to process both real-time data and historical (batch) data. Single Flow: All raw data flows through the streaming pipeline and is served through a common data serving layer to all applications. Efficiency: It solves the problem of code duplication because you only maintain a single codebase. - Challenges:</p> <p>Out-of-Order Data: Because it is solely based on speed and continuous streaming, data may arrive out of chronological order, which requires handling via \"watermarks\" (a topic for future discussion).</p> </li> </ul> <p>--- Comparison Summary</p> Feature Lambda Architecture Kappa Architecture Pipelines Two (Batch + Speed) One (Speed/Streaming) Complexity Higher (Code duplication) Lower (Single codebase) Latency High in batch layer Low (Real-time delivery) Data Order Handled by batch reprocessing Challenges with out-of-order data <p>When setting up the Spark session for streaming, a specific configuration is recommended to ensure data integrity during shutdowns.</p> <ul> <li>Graceful Shutdown: Setting <code>spark.streaming.stopGracefullyOnShutdown</code> to <code>true</code> ensures that if the application is shut down, it finishes processing any data already \"in line\" before stopping the pipeline.</li> <li>Schema Inference: For streaming, you must explicitly enable schema inference to allow Spark to identify the JSON structure at runtime.</li> </ul> <pre><code># Configuration for streaming schema inference and graceful shutdown\nspark.conf.set(\"spark.sql.streaming.schemaInference\", \"true\")\nspark.conf.set(\"spark.streaming.stopGracefullyOnShutdown\", \"true\")\n</code></pre> <p>--- Advanced File Streaming Options</p> <ul> <li><code>cleanSource</code>: Controls what happens to the input file after it is read. Options include <code>off</code> (default), <code>delete</code>, or <code>archive</code>.</li> <li><code>sourceArchiveDir</code>: Required when using <code>archive</code>; it specifies the folder where processed files are moved.</li> <li><code>maxFilesPerTrigger</code>: Determines how many files are consumed in a single micro-batch. Setting this to <code>1</code> ensures Spark processes only one file at a time, which is useful for controlling resource usage in production.</li> </ul> <p>--- Spark Streaming Trigger Types</p> <p>Triggers define the timing of streaming data processing. The sources outline three main types:</p> <ul> <li>Once (or <code>availableNow</code>)</li> </ul> <p>This trigger causes the streaming pipeline to behave like a batch job.  It consumes all data currently available in the source, processes it, and then automatically shuts down the pipeline. Ideal for Kappa architectures where you want to use the same streaming code to process historical data as a one-time batch.</p> <ul> <li>Processing Time</li> </ul> <p>This is the standard micro-batch trigger where you specify a recurring time interval. Spark triggers a micro-batch at the defined interval (e.g., every 10 seconds). It processes any new data that arrived during that window.</p> <ul> <li>Continuous (Experimental)</li> </ul> <p>Currently experimental in Spark 3.3.0, this mode offers the lowest latency. It does not use micro-batches. Instead, it processes data continuously. The time interval provided (e.g., '10 seconds') defines how often Spark records a checkpoint, not how often it processes data. It does not yet support all transformations (such as <code>explode</code>) or all sinks. For example, it is demonstrated using a memory sink.</p> <p>--- The Problem with Multiple <code>writeStream</code> Commands</p> <p>A common mistake is trying to call <code>writeStream</code> multiple times on the same streaming DataFrame to send data to  different locations. This approach causes several issues:</p> <p>Each <code>writeStream</code> command triggers its own Spark job for every micro-batch. This means the entire DAG (Directed Acyclic Graph) is re-computed, and the source data is read twice for two different streams.</p> <p>Each stream must maintain its own checkpoint location to track metadata.Because different sinks have different latencies, one stream might process data faster than the other, leading to them processing different offsets at any given time.</p> <ul> <li>The Solution: <code>foreachBatch</code></li> </ul> <p>To handle multiple sinks effectively, Spark provides the <code>foreachBatch</code> function. This function takes a Python function as input and executes it for every micro-batch.</p> <p>The data is read from the source and processed through the transformations only once per micro-batch. You only need to maintain one checkpoint location for the entire process. Within the provided Python function, you can use standard batch <code>write</code> commands to send data to as many locations as needed.</p> <p>--- Event Time vs. Processing Time</p> <p>Understanding the distinction between these two timestamps is vital for accurate streaming analytics.</p> <ul> <li> <p>Event Time:  The time at which the data was actually generated at the source (e.g., the moment a sensor in Sydney records a temperature).</p> </li> <li> <p>Processing Time:  The time at which the data arrives at the processing engine (e.g., Spark) to be ingested.</p> </li> </ul> <p>--- The Problem of Late Arrival</p> <p>Due to geographical distances or network latency, data generated at the same time might arrive at the processing center at different times.     Scenario: A device in Delhi (D1) and a device in Sydney (D2) both record a temperature at 12:04 (Event Time).    Result: D1 arrives almost instantly, while D2 arrives at 1:10 (Processing Time).    Miscalculation: If you aggregate by processing time, D1 falls into the 12:00\u20131:00 window, while D2 falls into the 1:00\u20132:00 window. This leads to incorrect averages because D2 should have been included in the 12:00 window based on its actual generation time.</p> <p>--- Stateful Processing</p> <p>To perform aggregations like \"hourly average temperature\" correctly, Spark must perform stateful processing.</p> <p>Logic: Spark holds the \"state\" (the current sum and count of temperatures) for specific time windows in its memory.    Updating State: When data generated at 12:04 arrives late (even hours later), Spark must go back into its memory, find the 12:00\u20131:00 window, and update the calculation.</p> <p>--- Managing Memory with Watermarks</p> <p>Spark cannot hold state in its memory indefinitely; otherwise, it would eventually run out of memory (OOM), especially if data arrives days or weeks late.</p> <p>Watermarks: A watermark is a threshold or \"timeout\" that tells Spark how long to keep the state for a specific window in memory.</p> <p>Discarding Late Data: If you define a watermark of 2 hours, Spark will wait for late data for up to two hours after the window closes. Any data arriving after that period is automatically discarded to free up memory.</p> <p>--- Summary Table</p> Concept Definition Importance Event Time When data was generated. Essential for accurate analytics. Processing Time When data arrived at Spark. Easier to track but can lead to miscalculations. State Data kept in Spark's memory. Allows updates to old windows when late data arrives. Watermark A \"timeout\" for late data. Prevents memory exhaustion by discarding very late data. <p>--- Overview of Window Operations</p> <p>Window operations are essential for stateful processing, allowing you to perform group aggregations (like word counts or temperature averages) over specific time segments. These operations rely on event time to ensure that data is placed into the correct chronological bucket, even if it arrives out of order.</p> <ul> <li>Tumbling Windows (Fixed Windows)</li> </ul> <p>Tumbling windows are of a fixed, constant size and do not overlap. Once a window ends, a new one begins immediately. Each event belongs to exactly one window.</p> <p>Example Logic: With a 10-minute window and a 5-minute trigger, Spark processes data in non-overlapping blocks (e.g., 12:00\u201312:10, 12:10\u201312:20).</p> <p>Results for a specific window only get updated when events with an event time falling within that window arrive.</p> <ul> <li>Sliding Windows (Overlapping Windows)</li> </ul> <p>Sliding windows are also of a fixed size but overlap each other for a specific duration. Because they overlap, a single event can fall into multiple windows simultaneously.</p> <p>Example Logic: If the window size is 10 minutes and the \"slide\" duration is 5 minutes, the windows will overlap by 5 minutes (e.g., W1: 12:00\u201312:10 and W2: 12:05\u201312:15).</p> <p>When an event arrives at 12:07, Spark must update both W1 and W2 because 12:07 falls within both ranges.</p> <ul> <li>Session Windows</li> </ul> <p>Session windows do not have a fixed size. Instead, they are defined by a session gap or a period of inactivity. A session stays \"open\" as long as events are flowing. It automatically terminates once the specified gap duration passes without any new activity.</p> <p>Example Logic: If the session gap is 5 minutes and the last event occurred at 12:09, the window will terminate at 12:14. If a new user logs in at 12:15 and has no further activity, that session terminates at 12:20.</p> <p>--- The Role of Watermarks in Windowing</p> <p>In stateful windowing, Spark must keep aggregations in memory to update them when late data arrives. Watermarks prevent memory exhaustion by defining a threshold for how long Spark should wait for late events.</p> <p>The watermark is calculated as the <code>Latest Event Time - Watermark Duration</code>.</p> <p>If the latest event is 12:17 and the watermark is 10 minutes, the threshold is 12:07. Any event generated before 12:07 that arrives at 12:20 will be ignored and will not update previous windows. This allows Spark to clear old aggregations from its memory.</p> <ul> <li>Summary Table</li> </ul> Window Type Fixed Size? Overlapping? Trigger Basis Tumbling Yes No Fixed time intervals Sliding Yes Yes Fixed time with overlap Session No No Inactivity/Session gap <p>--- Implementing Windows and Watermarks</p> <ul> <li> <p>Tumbling (Fixed) Window: A window of a fixed duration (e.g., 10 minutes) where windows do not overlap.</p> </li> <li> <p>Sliding (Overlapping) Window: Created by adding a sliding interval (e.g., a 10-minute window that slides every 5 minutes). This causes windows to overlap.</p> </li> <li> <p>Watermarks: A threshold used to handle late events. A watermark of 10 minutes tells Spark to discard any data arriving more than 10 minutes after the latest event timestamp processed.</p> </li> </ul> <p>Aggregation Code Example: <pre><code># Grouping with a 10-minute watermark and 10-minute tumbling window\nfinal_df = words_df \\\n    .withWatermark(\"event_time\", \"10 minutes\") \\\n    .groupBy(\n        window(col(\"event_time\"), \"10 minutes\"), \n        col(\"word\")\n    ).count() \\\n    .select(\"window.start\", \"window.end\", \"word\", \"count\")\n</code></pre></p> <p>--- Comparing Output Modes and Watermark Effects</p> Feature Update Mode Complete Mode Behavior Only outputs rows updated in the current batch. Rewrites the entire result table to the sink. Late Data Respects watermarks. Discards data arriving after the cut-off. Ignores watermarks. Keeps all historical data in memory to print the full table. Memory Efficient; discards old state based on watermark. Higher risk of Out of Memory (OOM) errors in stateful processing. <p>Practical Example of Late Data:    Latest Event: 12:17.    Watermark: 10 minutes (Cut-off time = 12:07).    Scenario A (Late Data): An event at 11:04 arrives. Update mode discards it, while Complete mode creates a new window for it.    Scenario B (Acceptable Delay): An event at 12:08 arrives. Because it is after the 12:07 cut-off, both modes update the counts for the 12:00\u201312:10 window.</p>"},{"location":"spark/spark/#spark-best-practises","title":"Spark Best Practises","text":"<p>--- How to deal with data skewness</p> <p>Handling data skewness is a common challenge in distributed computing frameworks like Apache Spark. </p> <p>Here are some popular techniques to mitigate it: - Salting: Salting involves adding a random component to a skewed key to create additional unique keys. After performing the operation (like a join), the extra key can be dropped to get back to the original data.</p> <ul> <li> <p>Splitting skewed data: Identify the skewed keys and process them separately. For instance, you can filter out the skewed keys and perform a separate operation on them.</p> </li> <li> <p>Increasing the number of partitions: Increasing the number of partitions can distribute the data more evenly. However, this might increase the overhead of managing more partitions.</p> </li> <li> <p>Using reduceByKey instead of groupByKey: reduceByKey performs local aggregation before shuffling the data, which reduces the data transferred over the network.</p> </li> <li> <p>Using Broadcast Variables: When joining a large DataFrame with a small DataFrame, you can use broadcast variables to send a copy of the small DataFrame to all nodes. This avoids shuffling of the large DataFrame.</p> </li> </ul> <p>--- Driver Failure</p> <p>The driver program runs the main() function of the application and creates a SparkContext. If the driver node fails, the entire application will be terminated, as it's the driver program that declares transformations and actions on data and submits such requests to the cluster.</p> <ul> <li> <p>Impact:</p> </li> <li> <p>The driver node is a single point of failure for a Spark application.</p> </li> <li> <p>If the driver program fails due to an exception in user code, the entire Spark application is terminated, and all executors are released.</p> </li> <li> <p>Handling Driver Failure:</p> </li> <li> <p>Driver failure is usually fatal, causing the termination of the application.</p> </li> <li>It's crucial to handle exceptions in your driver program to prevent such failures.</li> <li>Also, monitor the health of the machine hosting the driver program to prevent failures due to machine errors.</li> <li>In some cluster managers like Kubernetes, Spark supports mode like spark.driver.supervise to supervise and restart the driver on failure.</li> </ul> <p>--- Executor Failure</p> <p>Executors in Spark are responsible for executing the tasks. When an executor fails, the tasks that were running will fail.</p> <ul> <li> <p>Impact:</p> </li> <li> <p>Executors can fail for various reasons, such as machine errors or OOM errors in the user's application.</p> </li> <li>If an executor fails, the tasks that were running on it are lost.</li> <li> <p>The failure of an executor doesn't cause the failure of the Spark application, unless all executors fail.</p> </li> <li> <p>Handling Executor Failure:</p> </li> <li> <p>If an executor fails, Spark can reschedule the failed tasks on other executors.</p> </li> <li>There is a certain threshold for task failures. If the same task fails more than 4 times (default), the application will be terminated.</li> <li>Make sure to tune the resources allocated for each executor, as an executor might fail due to insufficient resources.</li> <li>For resilience, you can also opt to replicate the data across different executor nodes.</li> </ul> <p>--- Spark Driver OOM Scenarios</p> <ol> <li> <p>Large Collect Operations: If the data collected from executors using actions such as collect() or take() is too large to fit into the driver's memory, an OutOfMemoryError will occur.     Solution: Be cautious with actions that pull large volumes of data into the driver program. Use actions like take(n), first(), collect() carefully, and only when the returned data is manageable by the driver.</p> </li> <li> <p>Large Broadcast Variables: If a broadcast variable is larger than the amount of free memory on the driver node, this will also cause an OOM error.    Solution: Avoid broadcasting large variables. If possible, consider broadcasting a common subset of the data, or use Spark's built-in broadcast join if joining with a large DataFrame.</p> </li> <li> <p>Improper Driver Memory Configuration: If spark.driver.memory is set to a high value, it can cause the driver to request more memory than what is available, leading to an OOM error.    Solution: Set the spark.driver.memory config based on your application's need and ensure it doesn't exceed the physical memory limits.</p> </li> </ol> <p>--- Spark Executor OOM Scenarios</p> <ol> <li> <p>Large Task Results: If the result of a single task is larger than the amount of free memory on the executor node, an OutOfMemoryError will occur.    Solution: Avoid generating large task results. This is often due to a large map operation. Consider using reduceByKey or aggregateByKey instead of groupByKey when transforming data.</p> </li> <li> <p>Large RDD or DataFrame operations: Certain operations on RDDs or DataFrames, like join, groupByKey, reduceByKey, can cause data to be shuffled around, leading to a large amount of data being held in memory at once, potentially causing an OOM error.    Solution: Be cautious with operations that require shuffling large amounts of data. Use operations that reduce the volume of shuffled data, such as reduceByKey and aggregateByKey, instead of groupByKey.</p> </li> <li> <p>Persistent RDDs/DataFrames: If you're persisting many RDDs/DataFrames in memory and there isn't enough memory to store them, this will also cause an OOM error.    Solution: Unpersist unnecessary RDDs and DataFrames as soon as they are no longer needed. Tune the spark.memory.storageFraction to increase the amount of memory reserved for cached RDDs/DataFrames.</p> </li> <li> <p>Improper Executor Memory Configuration: Similar to the driver, if spark.executor.memory is set to a high value, it can cause the executor to request more memory than what is available, leading to an OOM error.    Solution: Set the spark.executor.memory config based on your application's need and ensure it doesn't exceed the physical memory limits of the executor nodes.</p> </li> </ol> <p>--- Code Level Optimization</p> <ol> <li> <p>Use DataFrames/Datasets instead of RDDs: DataFrames and Datasets have optimized execution plans, leading to faster and more memory-efficient operations than RDDs. They also have more intuitive APIs for many operations.</p> </li> <li> <p>Leverage Broadcasting: If you're performing an operation like a join between a large DataFrame and a small DataFrame, consider broadcasting the smaller DataFrame. Broadcasting sends the smaller DataFrame to all worker nodes, so they have a local copy and don't need to fetch the data across the network.</p> </li> <li> <p>Avoid Shuffling: Operations like groupByKey cause shuffling, where data is transferred across the network, which can be slow. Operations like reduceByKey or aggregateByKey reduce the amount of data that needs to be shuffled, and can be faster.</p> </li> <li> <p>Avoid Collecting Large Data: Be careful with operations like collect() that bring a large amount of data into the driver program, which could cause an out of memory error.</p> </li> <li> <p>Repartitioning and Coalescing: Depending on your use case, you might want to increase or decrease the number of partitions. If you have too many small partitions, use coalesce to combine them. If you have too few large partitions, use repartition to split them.</p> </li> <li> <p>Persist/Cache Wisely: Persist or cache the DataFrames or RDDs that you'll reuse. However, keep in mind that these operations consume memory, so use them judiciously.</p> </li> </ol> <p>--- Resource Configuration Optimization</p> <ol> <li> <p>Tune Memory Parameters: Make sure to set spark.driver.memory, spark.executor.memory, spark.memory.fraction, and spark.memory.storageFraction based on the memory requirements of your application and the capacity of your hardware.</p> </li> <li> <p>Control Parallelism: Use spark.default.parallelism and spark.sql.shuffle.partitions to control the number of tasks during operations like join, reduceByKey, etc. Too many tasks can cause a lot of overhead, but too few tasks might not fully utilize your cluster.</p> </li> <li> <p>Dynamic Allocation: If your cluster manager supports it, use dynamic resource allocation, which allows Spark to dynamically adjust the resources your application occupies based on the workload. This means that if your application has stages that require lots of resources, they can be allocated dynamically.</p> <pre><code>spark.dynamicAllocation.enabled true \nspark.dynamicAllocation.initialExecutors 2 \nspark.dynamicAllocation.minExecutors 1 \nspark.dynamicAllocation.maxExecutors 20\nspark.dynamicAllocation.schedulerBacklogTimeout 1m \nspark.dynamicAllocation.sustainedSchedulerBacklogTimeout 2m \nspark.dynamicAllocation.executorIdleTimeout 2min\nspark.dynamicAllocation.enabled is set to true to enable dynamic allocation.\nspark.dynamicAllocation.initialExecutors is set to 2 to specify that initially, two executors will be allocated.\nspark.dynamicAllocation.minExecutors and spark.dynamicAllocation.maxExecutors control the minimum and maximum number of executors, respectively.\nspark.dynamicAllocation.schedulerBacklogTimeout and spark.dynamicAllocation.sustainedSchedulerBacklogTimeout control how long a backlog of tasks Spark will tolerate before adding more executors.\nspark.dynamicAllocation.executorIdleTimeout controls how long an executor can be idle before Spark removes it.\n</code></pre> </li> </ol> <p>--- Resource Configuration Optimization</p> <ol> <li> <p>Tune Garbage Collection: Spark uses the JVM, so the garbage collector can significantly affect performance. You can use spark.executor.extraJavaOptions to pass options to the JVM to tune the garbage collection.</p> </li> <li> <p>Use Appropriate Data Structures: Parquet and Avro are both columnar data formats that are great for analytical queries and schema evolution. If your data processing patterns match these, consider using these formats.</p> </li> </ol>"},{"location":"sql/Mongo/","title":"Mongo","text":""},{"location":"sql/Mongo/#what-is-nosql-database","title":"What is NoSQL Database?","text":"<p>NoSQL databases, also known as \"non-SQL\" or \"not only SQL\", are databases that provide a mechanism to store and retrieve data modeled in ways other than the tabular format used in relational databases. They are typically used in large-scale or real-time web applications where the ability to scale quickly and handle large, diverse types of data is critical.</p> <p>Here are some key characteristics and features of NoSQL databases:</p> <ol> <li>Schema-less: NoSQL databases do not require a fixed schema, which gives you the flexibility to store different types of data entities together.</li> <li>Scalability: NoSQL databases are designed to expand easily to handle more traffic. They are horizontally scalable, meaning you can add more servers to handle larger amounts of data and higher loads.</li> <li>Diverse Data Models: NoSQL databases support a variety of data models including key-value pairs, wide-column, graph, or document. This flexibility allows them to handle diverse types of data and complex data structures.</li> <li>Distributed Architecture: Many NoSQL databases are designed with a distributed architecture, which can improve fault tolerance and data availability.</li> <li>Performance: Without the need for data relationships and joins as in relational databases, NoSQL databases can offer high-performance reads and writes.</li> </ol>"},{"location":"sql/Mongo/#types-of-nosql-databases","title":"Types of NoSQL Databases","text":"<p>NoSQL databases are categorized into four basic types based on the way they organize data. Let's go through each type and provide examples:</p> <ol> <li>Document Databases: These store data in documents similar to JSON (JavaScript Object Notation) objects. Each document contains pairs of fields and values, and the values can typically be a variety of types including strings, numbers, booleans, arrays, or objects. Each document is unique and can contain different data from other documents in the collection. This structure makes document databases flexible and adaptable to various data models.     Example: MongoDB, CouchDB.</li> <li>Key-Value Databases: These are the simplest type of NoSQL databases. Every single item in the database is stored as  an attribute name (or 'key') and its value. The main advantage of a key-value store is the ability to read and write operations using a simple key. This type of NoSQL database is typically used for caching and session management.     Example: Redis, Amazon DynamoDB</li> <li>Wide-Column Stores: These databases store data in tables, rows, and dynamic columns. Wide-column stores offer high performance and a highly scalable architecture. They're ideal for analyzing large datasets and are capable of storing vast amounts of data (Big Data).     Example: Apache Cassandra, Google BigTable.</li> <li>Graph Databases: These are used to store data whose relations are best represented as a graph. Each node of the graph represents an entity, and the relationship between nodes is stored directly, which allows the data to be retrieved in one operation. They're ideal for storing data with complex relationships, like social networks or a network of IoT devices.     Example: Neo4j, Amazon Neptune</li> </ol>"},{"location":"sql/Mongo/#difference-between-transactional-nosql-database","title":"Difference between Transactional &amp; NoSQL Database","text":""},{"location":"sql/Mongo/#nosql-databases-are-good-fit-for-analytical-queries","title":"NoSQL Databases are Good fit for Analytical Queries?","text":"<p>While NoSQL databases can handle certain analytical tasks, their primary purpose is not for heavy analytical queries. Traditional relational databases and data warehousing solutions, such as Hive, Redshift, Snowflake or BigQuery, are often better suited for complex analytical queries due to their ability to handle operations like joins and aggregations more efficiently</p>"},{"location":"sql/Mongo/#nosql-databases-in-bigdata-ecosystem","title":"NoSQL Databases in BigData Ecosystem","text":"<p>The strength of NoSQL databases lies in their flexibility, scalability, and speed for certain types of workloads, making them ideal for specific use-cases in the Big Data ecosystem:</p> <ol> <li>Handling Large Volumes of Data at Speed: NoSQL databases are designed to scale horizontally across many servers, which enables them to handle large volumes of data at high speed. This is particularly useful for applications that need real-time read/write operations on Big Data.</li> <li>Variety of Data Formats: NoSQL databases can handle a wide variety of data types (structured, semi-structured, unstructured), making them ideal for Big Data scenarios where data formats are diverse and evolving.</li> <li>Fault Tolerance and Geographic Distribution: NoSQL databases have a distributed architecture that provides high availability and fault tolerance, critical for applications operating on Big Data.</li> <li>Real-time Applications: Many Big Data applications require real-time or near-real-time functionality. NoSQL databases, with their high-speed read/write capabilities and ability to handle high volumes of data, are often used for real-time analytics, IoT data, and other similar scenarios. That said, the choice between SQL, NoSQL, or other database technologies should be based on the specific needs of the use-case at hand.</li> </ol>"},{"location":"sql/Mongo/#cap-theorem","title":"CAP Theorem","text":"<p>The CAP theorem is a concept that a distributed computing system is unable to simultaneously provide all three of the following guarantees:</p> <ol> <li>Consistency (C): Every read from the system receives the most recent write or an error. This implies that all nodes see the same data at the same time. It's the idea that you're always reading fresh data.</li> <li>Availability (A): Every request receives a (non-error) response, without the guarantee that it contains the most recent write. It's the idea that you can always read or write data, even if it's not the most current data.</li> <li>Partition Tolerance (P): The system continues to operate despite an arbitrary number of network or message failures (dropped, delayed, scrambled messages). It's the idea that the system continues to function even when network failures occur between nodes. Now, the key aspect of the CAP theorem, proposed by computer scientist Eric Brewer, is that a distributed system can satisfy any two of these three guarantees at the same time, but not all three. Hence the term \"CAP\" - Consistency, Availability, and Partition tolerance.</li> </ol> <p></p> <p>Here's how the three dichotomies look like:</p> <ol> <li>CA (Consistent and Available) systems prioritize data consistency and system availability but cannot tolerate network partitions. In such a system, if there is a partition between nodes, the system won't work as it doesn't support partition tolerance.</li> <li>CP (Consistent and Partition-tolerant) systems prioritize data consistency and partition tolerance. If a network partition occurs, the system sacrifices availability to ensure data consistency across all nodes.</li> <li> <p>AP (Available and Partition-tolerant) systems prioritize system availability and partition tolerance. If a network partition occurs, all nodes may not immediately reflect the same data, but the system remains available. Remember, real-world systems must tolerate network partitions (P), so the practical choice is between consistency (C) and availability (A) when partitions occur.</p> </li> <li> <p>NoSQL Database: MongoDB is a NoSQL database, meaning it does not use traditional table-based relational database structures. It's designed for large scale data storage and for handling diverse data types.</p> </li> <li>Document-Oriented: It stores data in JSON-like documents (BSON format), which allows for varied, dynamic schemas. This is in contrast to SQL databases which use a fixed table schema.</li> <li>Schema-less: MongoDB is schema-less, meaning that the documents in the same collection (equivalent to tables in SQL) do not need to have the same set of fields or structure, and the common field in different documents can hold different types of data.</li> <li>Scalability: It offers high scalability through sharding, which distributes data across multiple machines.</li> <li>Replication: MongoDB provides high availability with replica sets. A replica set consists of two or more copies of the data. Each replica set member may act in the role of primary or secondary replica at any time. The primary replica performs all write operations, while secondary replicas maintain a copy of the data of the primary using built-in replication.</li> <li>Querying: Supports a rich set of querying capabilities, including document-based queries, range queries, regular expression searches, and more.</li> <li>Indexing: Any field in a MongoDB document can be indexed, which improves the performance of search operations.</li> </ol>"},{"location":"sql/Mongo/#mongodb-architecture","title":"MongoDB Architecture","text":"<ol> <li> <p>Document Model: BSON Format: MongoDB stores data in BSON (Binary JSON) documents, which are JSON-like structures. This format supports a rich variety and complexity of data types. Unlike relational databases, MongoDB does not require a predefined schema. The structure of documents can change over time.</p> </li> <li> <p>Collections: Similar to Tables: Collections in MongoDB are analogous to tables in relational databases. They hold sets of documents. Schema-less: Each document in a collection can have a completely different structure.</p> </li> <li> <p>Database: Multiple Collections: A MongoDB instance can host multiple databases, each containing their own collections.</p> </li> <li> <p>Sharding: a shard typically refers to a group of multiple nodes (machines), especially in production environments.  Each shard is often a replica set, which is a group of mongod instances that hold the same data set. In this setup, each shard consists of multiple machines - one primary and multiple secondaries.</p> <p>Shard Key: The distribution of data across shards is determined by a shard key. MongoDB partitions data in the collection based on this  key, and different partitions (or chunks of data) are stored on different shards.</p> <p>Primary Node: Within each shard (replica set), there is one primary node that handles all write operations. All data changes are first written to the primary.</p> <p>Secondary Nodes: The secondary nodes replicate data from the primary node, providing redundancy and increasing data availability. They can also serve read operations to distribute the read load.</p> </li> <li> <p>Query Router Role in Sharded Clusters:The Query Router is typically a mongos instance in MongoDB. It acts as an intermediary between client applications and the MongoDB sharded cluster.</p> <p>Query Distribution: The Query Router receives queries from client applications and determines the appropriate shard(s) that hold the relevant data. It routes the query to the correct shard(s) based on the shard key and the cluster\u2019s current configuration.</p> <p>Aggregation of Results: After receiving responses from the shards, the Query Router aggregates these results and returns them to the client application. This process is transparent to the client, which interacts with the Query Router as if it were a single MongoDB server.</p> <p>Load Balancing: Query Routers can help distribute read and write loads across the shards, enhancing the overall performance of the database system.In larger deployments, multiple Query Routers can be used to balance the load and provide redundancy.</p> <p>Shard Management: The Query Router communicates with the cluster\u2019s config servers to keep track of the metadata about the cluster's current state, including the distribution of data across shards.</p> <p>Simplifies Client Interaction: By abstracting the complexity of the sharded cluster, Query Routers simplify how clients interact with the database. Clients do not need to know the details of data distribution across shards.</p> <p>Write Operations: For write operations, the Query Router forwards the request to the primary replica set member of the appropriate shard.</p> <p>Caching: Query Routers cache the cluster\u2019s metadata to quickly route queries without needing to frequently access config servers.</p> </li> </ol>"},{"location":"sql/Mongo/#mongodb-indexes","title":"MongoDB Indexes","text":"<p>Indexing in MongoDB is a critical feature that improves the performance of database operations, particularly in querying and sorting data. </p> <p>Purpose: Indexes in MongoDB are used to efficiently fetch data from a database. Without indexes, MongoDB must perform a full scan of a collection to select those documents that match the query statement.     Default Index: Every MongoDB collection has an automatic index created on the _id field. The _id index is the primary key and ensures the uniqueness of each document in the collection.     Index Types: MongoDB supports various types of indexes, catering to different types of data and queries.</p> <p>Types of Indexes</p> <ol> <li>Single Field Index: Indexes a single field of a document in either ascending or descending order. Besides the default _id index, you can create custom single field indexes.</li> <li>Compound Index: Indexes multiple fields within a document. The order of fields listed in a compound index is significant. It determines the sort order and query capability of the index.</li> <li>Multikey Index: Created automatically for fields that hold an array. If you index a field that contains an array, MongoDB creates separate index entries for each element of the array.</li> <li>Text Index: Used for searching text strings. A text index stores the content of a field tokenized as words, optimized for text search operations.</li> <li>Hashed Index: Stores the hash of the value of a field. These are primarily used in sharding scenarios to evenly distribute data across shards.</li> <li>Partial Index: Indexes only the documents in a collection that meet a specified filter expression. This can be more efficient and consume less space than indexing all documents.</li> <li>TTL (Time-To-Live) Index: Automatically deletes documents from a collection after a certain amount of time. This is useful for data that needs to expire, like sessions or logs.</li> </ol>"},{"location":"sql/Mongo/#use-cases-of-mongodb","title":"Use cases of MongoDB","text":"<ol> <li> <p>Content Management Systems:     Flexible schema accommodates various content types and changing data structures.     Efficiently stores and retrieves diverse and complex data sets.</p> </li> <li> <p>Mobile Apps:     Scales easily with user growth.     Offers real-time data synchronization and integration capabilities.</p> </li> <li> <p>Internet of Things (IoT):     Handles high volumes of time-series data from sensors and devices. Supports geospatial queries and real-time analytics.</p> </li> <li> <p>E-commerce Applications:     Manages diverse and evolving product catalogs.     Offers personalized customer experiences through robust data handling.</p> </li> <li> <p>Gaming Industry:     Provides high performance for real-time analytics.     Scales dynamically to handle fluctuating user loads.</p> </li> <li> <p>Real-Time Analytics:     Facilitates real-time data processing and aggregation. Offers quick insights from live data.</p> </li> <li> <p>Catalogs and Inventory Management:     Easily manages complex and varied product data.     Supports fast queries for efficient inventory tracking.</p> </li> <li> <p>Log Data Storage and Analysis:     Stores large volumes of log data for analysis.     Offers time-to-live (TTL) indexes for expiring old logs.</p> </li> <li> <p>Document and Asset Management:     Ideal for storing, retrieving, and managing document-based information. Supports rich document structures and metadata.</p> </li> <li> <p>Social Networks:     Manages dynamic and large-scale user-generated data.     Handles complex friend networks and social graph data efficiently.</p> </li> </ol>"},{"location":"sql/mongoiq/","title":"MongoDB","text":"<p>What is MongoDB?</p> <p>MongoDB is a NoSQL database that stores data in flexible,JSON-like documents, meaning fields can vary from document to document and data structure can be changed over time.</p> <p>What are Collections in MongoDB?</p> <p>Collections in MongoDB are analogous to tables in relational databases and are used to store a set of documents.</p> <p>What is a Document in MongoDB?</p> <p>A document is the basic unit of data in MongoDB and is similar to a JSON object, consisting of field-value pairs.</p> <p>How does MongoDB differ from SQL databases?</p> <p>MongoDB is a NoSQL database that does not require a fixed schema, allows horizontal scaling, and uses a document-based data model, unlike structured, table-based SQL databases.</p> <p>What is the role of _id in MongoDB?</p> <p>The _id field acts as a primary key in MongoDB documents, uniquely identifying each document in a collection.</p> <p>What are Indexes in MongoDB?</p> <p>Indexes support the efficient execution of queries in MongoDB. Without indexes, MongoDB must scan every document in a collection to select those that match the query statement.</p> <p>Can you change an _id field of a document?</p> <p>No, the _id field of a document is immutable and cannot be changed once set.</p> <p>What is a Replica Set in MongoDB?</p> <p>A replica set in MongoDB is a group of mongod instances that maintain the same data set, providing redundancy and high availability.</p> <p>What is Sharding in MongoDB?</p> <p>Sharding is the process of storing data records across multiple machines and is MongoDB\u2019s approach to meeting the demands of data growth.</p> <p>What are Aggregations in MongoDB?</p> <p>Aggregations in MongoDB process data records and return computed results, similar to GROUP BY in SQL. They provide a way to perform complex data processing and transformations.</p> <p>How do you back up a MongoDB database?</p> <p>MongoDB can be backed up using mongodump, a utility for creating binary export of the contents of a database.</p> <p>What is BSON in MongoDB?</p> <p>BSON (Binary JSON) is a binary-encoded serialization of JSON-like documents used by MongoDB.</p> <p>What is a Namespace in MongoDB?</p> <p>A namespace in MongoDB is the concatenation of the database name and the collection name, used to uniquely identify collections across databases.</p> <p>What is Mongoose in the context of MongoDB?</p> <p>Mongoose is an Object Data Modeling (ODM) library for MongoDB and Node.js, managing relationships between data and providing schema validation.</p> <p>How does MongoDB provide concurrency?</p> <p>MongoDB uses reader-writer locks that allow concurrent readers shared access to a resource, such as a database or collection, but give exclusive access to a single write operation.</p> <p>What are some common commands in MongoDB?</p> <p>Common commands include find() for retrieving documents, insert() for adding new documents, update() for modifying existing documents, and delete() for removing documents.</p> <p>What is Journaling in MongoDB?</p> <p>Journaling in MongoDB is used to safeguard data in case of a crash by recording changes before they are written to the database.</p> <p>What is GridFS and when is it used?</p> <p>GridFS is used in MongoDB for storing and retrieving large files like images, audio files, or video files.</p> <p>How do you scale MongoDB?</p> <p>MongoDB can be scaled horizontally by sharding, distributing data across multiple servers, or vertically by adding more resources to the existing machines.</p> <p>What is the default port for MongoDB?</p> <p>The default port for MongoDB is 27017</p> <p>How does MongoDB handle transaction management?</p> <p>MongoDB supports multi-document ACID transactions, similar to relational databases. Transactions in MongoDB can be used to perform a series of read and write operations atomically.</p> <p>Explain the concept of 'upsert' in MongoDB.</p> <p>'Upsert' is a combination of 'update' and 'insert'. If the specified document exists, MongoDB updates it with the new values; if it does not exist, MongoDB inserts it as a new document.</p> <p>What are the differences between embedded documents and references in MongoDB?</p> <p>Embedded documents are stored directly within a parent document, providing fast read access. References are links to documents stored in another collection, requiring an additional query to retrieve but are better for data normalization and avoiding data duplication.</p> <p>How do you ensure data integrity in MongoDB?</p> <p>Data integrity in MongoDB can be ensured through proper schema design, using transactions for complex operations, and implementing validation rules in the database layer.</p> <p>Scenario: How would you design a MongoDB schema for a blogging platform?</p> <p>A blogging platform schema might involve collections for users, posts, and comments. Posts can have embedded comments or reference them. User documents can reference their posts.</p> <p>What is MapReduce in MongoDB and when would you use it?</p> <p>MapReduce is a data processing paradigm in MongoDB used for batch processing of data and aggregation operations. It's useful for large datasets and complex aggregations.</p> <p>How does MongoDB ensure high availability?</p> <p>MongoDB ensures high availability through replica sets, which provide redundancy and automatic failover in case of primary node failure.</p> <p>Can you change the shard key after sharding a collection?</p> <p>No, once a shard key is chosen and sharding is implemented, you cannot change the shard key of a collection.</p> <p>What is a Covered Query in MongoDB?</p> <p>A covered query is a query in which all the fields in the query, including the sort and projection fields, are part of an index. Covered queries can be much faster as they avoid fetching documents.</p> <p>Explain Write Concern in MongoDB.</p> <p>Write concern in MongoDB describes the level of acknowledgment requested from MongoDB for write operations, determining the guarantee of writing data to the database.</p> <p>What is the Aggregation Pipeline in MongoDB?</p> <p>The Aggregation Pipeline is a framework in MongoDB for data aggregation, modeled as a pipeline through which documents pass and are transformed into aggregated results.</p> <p>Scenario: How would you optimize a slow query in MongoDB?</p> <p>To optimize a slow query, first identify the query, examine the execution plan, create appropriate indexes, and consider redesigning the schema for more efficient querying.</p> <p>What is the role of the Profiler in MongoDB?</p> <p>The Profiler in MongoDB is used to monitor database operations, helping in identifying slow queries and performance bottlenecks.</p> <p>How can you achieve pagination in MongoDB queries?</p> <p>Pagination can be achieved using the skip() and limit() methods in MongoDB. However, for large datasets, a range-based pagination using _id or another indexed field is more efficient.</p> <p>Explain the role of the WiredTiger storage engine in MongoDB.</p> <p>WiredTiger, the default storage engine in MongoDB, offers advantages like support for transactions, compression, and cache management, leading to improved performance and storage efficiency.</p> <p>Scenario: How would you handle a scenario where your MongoDB database is hitting memory limits?</p> <p>Addressing memory limits involves optimizing indexes, queries, and schema design; considering sharding for horizontal scaling; and potentially increasing the server's memory capacity.</p> <p>What are TTL Indexes in MongoDB?</p> <p>Time-To-Live (TTL) indexes are used to automatically remove documents from a collection after a certain amount of time, useful for data that only needs to be stored temporarily.</p> <p>What is the difference between $lookup and DBRef in MongoDB?</p> <p>$lookup is an aggregation pipeline stage that lets you join documents from different collections, while DBRef is a convention for representing a reference to another document.</p> <p>Scenario: Describe how you would migrate data from a SQL database to MongoDB.</p> <p>Migrating data involves exporting data from the SQL database, transforming it into a format suitable for MongoDB (like JSON), and then importing it using tools like mongoimport.</p> <p>What is a Capped Collection in MongoDB?</p> <p>Capped collections are fixed-size collections that automatically overwrite their oldest entries when they reach their maximum size. They are ideal for logging and caching purposes.</p> <p>Explain the use of the $facet stage in the aggregation pipeline.</p> <p>The $facet stage allows for performing multiple aggregation pipelines within a single stage. It is useful for creating multi-faceted aggregations that categorize data into different metrics in a single query.</p> <p>How does MongoDB handle large-scale join operations?</p> <p>MongoDB isn't designed for large-scale join operations like traditional RDBMS. However, it can perform left outer joins using the $lookup stage in the aggregation pipeline.</p> <p>What are the limitations of using transactions in MongoDB?</p> <p>Transactions in MongoDB can affect performance due to increased latency and have limitations like a 60-second transaction timeout and increased storage space requirements for the WiredTiger engine.</p> <p>Scenario: Design a MongoDB schema for a real-time analytics dashboard.</p> <p>For a real-time analytics dashboard, a schema that supports fast reads is crucial. This might involve denormalizing data, using pre-aggregated metrics, and optimizing indexes for common queries.</p> <p>Explain the role of oplog in MongoDB Replica Sets.</p> <p>The oplog (operations log) is a special capped collection that keeps a rolling record of all operations that modify the data storedin databases. It's used in replica sets for replication purposes.</p> <p>How does MongoDB ensure data durability?</p> <p>MongoDB ensures data durability through journaling and replication. Journaling writes operations to disk to prevent data loss, and replication ensures data is copied across multiple servers.</p> <p>What are the strategies for handling hotspots in sharded clusters?</p> <p>Handling hotspots involves choosing an effective shard key, ensuring even data distribution, using compound shard keys if necessary, and monitoring and redistributing chunks when needed.</p> <p>Scenario: How would you optimize a sharded MongoDB cluster with uneven shard loads?</p> <p>To optimize an unevenly loaded sharded cluster, analyze the shard key and chunk distribution, use zone sharding for better control, and possibly reshard the data with a more appropriate shard key.</p> <p>Discuss the impact of document size on MongoDB's performance.</p> <p>Larger documents consume more memory and can lead to slower read/write operations. Optimizing document size by avoiding large, complex documents or unnecessary fields can improve performance.</p> <p>How does MongoDB handle network partitions in a sharded environment?</p> <p>In the event of a network partition, MongoDB maintains consistency by ensuring that each shard remains independently consistent. However, the cluster might not be able to achieve overall consistency until the partition is resolved.</p> <p>Explain how the WiredTiger cache works in MongoDB.</p> <p>The WiredTiger cache holds recently read and written data in memory. When the cache becomes full, older or less frequently accessed data is written to disk. Proper cache management is crucial for performance.</p> <p>What are the best practices for securing a MongoDB database?</p> <p>Best practices include enabling authentication, using role-based access control, encrypting data at rest and in transit, regularly updating MongoDB, and securing the underlying server.</p> <p>Scenario: Implement a strategy to handle time-series data in MongoDB.</p> <p>For time-series data, use a schema that groups data into buckets (documents) based on time intervals. This approach optimizes storage and query efficiency for time-based data.</p> <p>Discuss the use of Change Streams in MongoDB.</p> <p>Change streams allow applications to access real-time data changes without polling the database. They are useful for triggering actions, updating caches, and synchronizing data with external systems.</p> <p>Explain how MongoDB\u2019s query planner selects indexes for executing queries.</p> <p>MongoDB's query planner evaluates various query plans using available indexes and chooses the one with the lowest estimated cost based on factors like index selectivity and document counts.</p> <p>What are the considerations for selecting a shard key in MongoDB?</p> <p>A good shard key should provide even distribution of data, support the query patterns of your application, and minimize the need for resharding.</p> <p>Scenario: How do you migrate a large collection in MongoDB with minimal downtime?</p> <p>For minimal downtime, perform the migration in stages: start by syncing data to the new collection or database, redirect read/write traffic, and then finalize the migration.</p> <p>How does MongoDB handle write operations in sharded clusters?</p> <p>Write operations in sharded clusters are directed to the primary shard responsible for the data's shard key. The primary shard then applies the write locally and replicates it to secondary members.</p> <p>What is a Write Amplification in MongoDB and how can it be minimized?</p> <p>Write amplification occurs when more writes are performed than necessary, often due to updates that cause document relocations. It can be minimized by schema design that reduces document growth and by using update operators efficiently.</p> <p>How do you monitor and tune the performance of a MongoDB cluster?</p> <p>Performance can be monitored using tools like MongoDB Atlas, mongostat, and mongotop. Tuning involves analyzing query patterns, optimizing indexes, adjusting server configurations, and ensuring adequate resources.</p>"},{"location":"sql/sql/","title":"SQL","text":""},{"location":"sql/sql/#what-is-a-database","title":"What is a database?","text":"<p>Database is structured, organised set of data.Think of it as a filecabinet whre you store data in different sections called tables.</p>"},{"location":"sql/sql/#what-is-dbms","title":"What is DBMS?","text":"<p>A software which allows users to interact with data. Stores data in structured format. A schema defines the structure of data.</p>"},{"location":"sql/sql/#acid-properties","title":"Acid Properties","text":"<ul> <li> <p>Atomicity: Each transaction is either properly carried out or the database reverts back to the state before the transaction has started.Atomicity enforces the \"all or nothing\" principle, ensuring that an entire transaction either completes successfully or is rolled back, preventing partial, inconsistent states. In a financial transfer, if a crash occurs after deducting funds from one account but before adding them to another, an atomic system would detect the failure before the final COMMIT and roll back the entire transaction.</p> </li> <li> <p>Consistency: Database must be in a consistent state before and after the transaction.Consistency guarantees that a transaction moves the database from one valid state to another, enforcing rules (like checking for sufficient funds) to prevent inconsistent results.</p> </li> <li> <p>Isolation: Multiple transactions occur independently without interference.Isolation is the \"no interference policy,\" meaning multiple concurrent transactions operate independently without seeing each other's uncommitted changes; Delta achieves this using. Optimistic Concurrency Control (OCC).</p> </li> <li> <p>Durability: Successful transactions are persisted even in case of failure.Durability ensures that once a transaction is committed, it permanently remains recorded in the system, even in the event of crashes or power outages. This is achieved by logging the start of the operation and the final COMMIT status in a transaction log (ledger); if the system restarts and the log shows no COMMIT, the changes are rolled back</p> </li> </ul>"},{"location":"sql/sql/#the-basics-of-a-query","title":"The Basics of a Query","text":"<p>--- SELECT and FROM</p> <p>A query is a way to ask the database a question to retrieve data without modifying the table structure or its contents.</p> <ul> <li>SELECT: Specifies which columns you want to retrieve. Use a star () to select all columns.</li> <li>FROM: Specifies the table where the data is located.</li> </ul> <p>Example</p> <p><code>SELECT  FROM customers;</code> retrieves every column and row from the \"customers\" table.</p> <p>--- Selecting Specific Columns and Using Aliases</p> <p>Instead of retrieving everything, you can specify a list of columns separated by commas.</p> <ul> <li>Syntax: <code>SELECT column1, column2 FROM table_name;</code>.</li> </ul> <p>Note</p> <p>Do not place a comma after the last column in your list, or SQL will return an error. Aliases (AS): You can temporarily rename a column in your result set for better readability using the <code>AS</code> keyword.</p> <p>Example</p> <p><code>SELECT country AS customer_country, SUM(score) AS total_score FROM customers GROUP BY country;</code>.</p> <p>--- Filtering Data: WHERE</p> <p>The <code>WHERE</code> clause filters rows based on a specific condition. Only data that meets the condition is included in the output.</p> <ul> <li>Syntax: <code>SELECT  FROM customers WHERE score != 0;</code> (Retrieves customers whose score is not zero).</li> <li>Handling Strings: If your condition involves text (characters), the value must be enclosed in single quotes.</li> </ul> <p>Example</p> <p><code>SELECT  FROM customers WHERE country = 'Germany';</code>.</p> <p>--- Sorting Data: ORDER BY</p> <p>Use <code>ORDER BY</code> to sort your results in a specific order.</p> <ul> <li>Mechanisms: Use <code>ASC</code> for ascending (lowest to highest, default) or <code>DESC</code> for descending (highest to lowest).</li> <li>Nested Sorting: You can sort by multiple columns. SQL prioritizes the first column listed; the second column is used to \"refine\" the sorting if there are duplicate values in the first column.</li> </ul> <p>Example</p> <p><code>SELECT  FROM customers ORDER BY country ASC, score DESC;</code> sorts by country alphabetically, then by highest score within each country.</p> <p>--- Aggregation and Grouping: GROUP BY</p> <p>The <code>GROUP BY</code> clause combines rows with the same values into summary rows. It is typically used with aggregate functions like <code>SUM</code>, <code>COUNT</code>, or <code>AVG</code>.</p> <ul> <li>Rule: Any non-aggregated column in your <code>SELECT</code> statement must also be included in the <code>GROUP BY</code> clause, or you will receive an error.</li> </ul> <p>Example</p> <p><code>SELECT country, SUM(score) FROM customers GROUP BY country;</code> provides the total score for each unique country.</p> <p>--- Filtering Aggregated Data: HAVING</p> <p><code>HAVING</code> is used to filter data after it has been aggregated by a <code>GROUP BY</code> clause.</p> <ul> <li>Difference from WHERE: <code>WHERE</code> filters individual rows before they are grouped; <code>HAVING</code> filters the results after the grouping logic is applied.</li> </ul> <p>Example</p> <p>To find countries with an average score greater than 430: <code>SELECT country, AVG(score) FROM customers GROUP BY country HAVING AVG(score) &gt; 430;</code>.</p> <p>--- Distinct and Top Keywords</p> <ul> <li>DISTINCT: Placed immediately after <code>SELECT</code>, it removes duplicate rows so that each value in the result is unique. Note that it is an \"expensive\" operation for the database, so it should only be used when necessary.</li> <li>TOP (or LIMIT): Restricts the number of rows returned based on their row number in the result set.</li> </ul> <p>Example</p> <p><code>SELECT TOP 3  FROM customers ORDER BY score DESC;</code> retrieves the three customers with the highest scores.</p> <p>--- Coding Order vs. Execution Order</p> <p>The order in which you write a query is different from the order in which the database executes it. Understanding this helps in building correct queries.</p> Step Coding Order (Syntax) Execution Order (Logic) 1 <code>SELECT</code> (with <code>DISTINCT</code> and <code>TOP</code>) <code>FROM</code> (Find the data) 2 <code>FROM</code> <code>WHERE</code> (Filter original rows) 3 <code>WHERE</code> <code>GROUP BY</code> (Aggregate/combine rows) 4 <code>GROUP BY</code> <code>HAVING</code> (Filter aggregated results) 5 <code>HAVING</code> <code>SELECT</code> / <code>DISTINCT</code> (Pick columns/unique values) 6 <code>ORDER BY</code> <code>ORDER BY</code> (Sort the final list) 7 <code>TOP</code> (Limit the number of rows) <p>--- Comments in SQL</p> <p>Comments are notes for the coder that the database engine ignores.</p> <ul> <li>Inline: Use two dashes (<code>--</code>).</li> <li>Multi-line: Wrap the text in <code>/</code> and <code>/</code>.</li> </ul>"},{"location":"sql/sql/#sql-ddl","title":"SQL DDL","text":"<p>--- The CREATE Command</p> <p>The <code>CREATE</code> command is used to build a new object, like a table, within the database. Usually, a newly created table is empty.</p> <ul> <li>Syntax &amp; Logic: You must define the table name, column names, data types, and constraints.</li> <li>Data Types: Common types include <code>INT</code> (numbers), <code>VARCHAR</code> (characters/text), and <code>DATE</code>.</li> <li>Constraints: These are rules for the data, such as <code>NOT NULL</code> (the field must have a value) or <code>PRIMARY KEY</code> (a unique identifier for each row to ensure integrity).</li> </ul> <p>Example</p> <p>Creating a \"Persons\" Table <pre><code>    CREATE TABLE persons (\n    ID INT NOT NULL,\n    person_name VARCHAR(50) NOT NULL,\n    birth_date DATE, -- This is optional (nulls allowed)\n    phone VARCHAR(15) NOT NULL,\n    CONSTRAINT PK_persons PRIMARY KEY (ID) -- Setting the ID as the primary key\n    );\n</code></pre></p> <p>--- The ALTER Command</p> <p>The <code>ALTER</code> command is used to edit or change the definition of an existing table, such as adding or removing columns or changing data types.</p> <ul> <li>Adding a Column: When you add a new column, it is automatically placed at the end of the table. If you want a column in the middle, you must drop the whole table and recreate it from scratch.</li> <li>Removing a Column: You only need to specify the column name; you do not need to provide the data type because the database already knows it. Warning: Deleting a column also deletes all data stored in that column.</li> </ul> <p>Example</p> <p>Adding and Removing Columns</p> <p>To Add: <code>ALTER TABLE persons ADD email VARCHAR(50) NOT NULL;</code></p> <p>To Remove: <code>ALTER TABLE persons DROP COLUMN phone;</code></p> <p>--- The DROP Command</p> <p>The <code>DROP</code> command is used to completely remove a table and all its data from the database.</p> <ul> <li>Risk Level: This is described as the simplest but most risky command because it destroys the table and all its contents instantly.</li> <li>Comparison: Destroying a table with <code>DROP</code> is much easier than building one with <code>CREATE</code>.</li> </ul> <p>Example</p> <p>Deleting a Table <pre><code>DROP TABLE persons;\n</code></pre></p>"},{"location":"sql/sql/#sql-dml","title":"SQL DML","text":"<p>--- The INSERT Command</p> <p>The <code>INSERT</code> command is used to add new rows to a table. These rows are typically appended to the end of the existing data.</p> <p>Method A: Manual Insertion</p> <p>In this method, you manually specify the values to be added via a script.</p> <ul> <li>Syntax: <code>INSERT INTO table_name (column_list) VALUES (value_list);</code>.</li> <li>Key Rules:        The number of columns and values must match.        The order of values must match the order of defined columns.        Specifying columns is optional; if skipped, SQL expects a value for every column in the table in its default order.        Nulls: Use <code>NULL</code> to indicate unknown data. Note that columns with constraints (like Primary Keys) will not allow null values.</li> </ul> <p>Example</p> <p>(Multiple Rows): <pre><code>INSERT INTO customers (ID, first_name, country, score)\nVALUES (6, 'Anna', 'USA', NULL),\n    (7, 'Sam', NULL, 100);\n(This adds two customers in a single execution; note the comma separating the value lists).\n</code></pre></p> <p>Method B: Inserting from Another Table</p> <p>You can move data from a Source Table to a Target Table without writing manual values.</p> <ol> <li> <p>Select: Write a <code>SELECT</code> query to gather the desired data from the source.</p> </li> <li> <p>Insert: Wrap that query in an <code>INSERT INTO</code> statement.</p> </li> </ol> <p>Note</p> <p>Column names do not have to match between tables, but the data types and structure must be compatible.</p> <p>--- The UPDATE Command</p> <p>The <code>UPDATE</code> command modifies existing rows. Unlike <code>INSERT</code>, it does not create new records; it changes the content of what is already there.</p> <ul> <li>Syntax: <code>UPDATE table_name SET column1 = value1, column2 = value2 WHERE condition;</code>.</li> <li>The Risk of WHERE: If you omit the <code>WHERE</code> clause, every single row in the table will be updated with the new value.</li> <li>Best Practice: Test your <code>WHERE</code> clause with a <code>SELECT</code> statement first to ensure you are targeting the correct records before running the update.</li> </ul> <p>Example</p> <p>(Updating Multiple Columns): To change customer 10's score and country simultaneously: <pre><code>UPDATE customers \nSET score = 0, country = 'UK' \nWHERE ID = 10;\n(This targets only the specific row where ID is 10).\n</code></pre></p> <p>--- The DELETE and TRUNCATE Commands</p> <p>These commands are used to remove data from your tables.</p> <p>DELETE:</p> <p>Removes specific rows based on a condition.</p> <ul> <li>Syntax: <code>DELETE FROM table_name WHERE condition;</code>.</li> <li>Warning: Just like <code>UPDATE</code>, forgetting the <code>WHERE</code> clause will delete all data in the table.</li> </ul> <p>Example</p> <pre><code>DELETE FROM customers WHERE ID &gt; 5;\n(This removes all recently added customers with an ID higher than 5).\n</code></pre> <p>TRUNCATE:</p> <p>If you want to empty a table completely while keeping its structure intact, <code>TRUNCATE</code> is the preferred method.</p> <ul> <li>Efficiency: It is much faster than <code>DELETE FROM table_name</code> because it does not use the same logging and background protocols.</li> <li>Behavior: It resets the table to an empty state and does not return the number of rows affected.</li> </ul> Feature DROP TRUNCATE DELETE Purpose Completely removes the entire table structure from the database Removes all rows from a table, but the table structure remains Removes specific rows based on a condition or all rows from a table, but the table structure remains Transaction Control Cannot be rolled back Cannot be rolled back Can be rolled back Space Reclaiming Releases the object and its space Frees the space containing the table Doesn't free up space, but leaves empty space for future use Speed Fastest as it removes all data and structure Faster than DELETE as it doesn't log individual row deletions Slowest as it logs individual row deletions Referential Integrity Not checked Checked Checked Where Clause Not applicable Not applicable Applicable Command Type DDL (Data Definition Language) DDL (Data Definition Language) DML (Data Manipulation Language)"},{"location":"sql/sql/#sql-where","title":"SQL Where","text":"<p>--- Core Logic of SQL Conditions</p> <p>In SQL, the <code>WHERE</code> clause is used to filter data based on specific logic. A condition typically follows this formula: Expression -&gt; Operator -&gt; Expression.</p> <p>You can compare several types of data</p> <ul> <li>Column to Column: e.g., comparing first name to last name.</li> <li>Column to Static Value: e.g., <code>first_name = 'John'</code>.</li> <li>Functions: Applying a function like <code>UPPER</code> to a column before comparing.</li> <li>Math Expressions: e.g., <code>price  quantity = 1000</code>.</li> <li>Subqueries: Comparing a column to the result of another complete query.</li> </ul> <p>When a condition is applied, SQL evaluates the data row by row. If a row meets the condition (True), it is kept; if not (False), it is removed from the final result.</p> <p>--- Comparison Operators</p> <p>These are the most basic operators used to compare two values.</p> <ul> <li>Equal (<code>=</code>): Retrieves exact matches.</li> </ul> <p>Example</p> <p><code>SELECT  FROM customers WHERE country = 'Germany';</code>.</p> <ul> <li>Not Equal (<code>!=</code> or <code>&lt;&gt;</code>): Retrieves everything except the specified value.</li> </ul> <p>Example</p> <p><code>SELECT  FROM customers WHERE country != 'Germany';</code>.</p> <ul> <li>Greater Than (<code>&gt;</code>) and Less Than (<code>&lt;</code>): Used for numerical ranges.</li> </ul> <p>Example</p> <p><code>WHERE score &gt; 500;</code>.</p> <ul> <li>Greater Than or Equal to (<code>&gt;=</code>) and Less Than or Equal to (<code>&lt;=</code>): These are \"mixed\" operators that include the boundary value itself.</li> </ul> <p>Example</p> <p><code>WHERE score &gt;= 500;</code> (Includes rows where the score is exactly 500).</p> <p>--- Logical Operators</p> <p>Logical operators combine multiple conditions within a single <code>WHERE</code> clause.</p> <ul> <li>AND: This is restrictive. All conditions must be True for a row to be included.</li> </ul> <p>Example</p> <p><code>WHERE country = 'USA' AND score &gt; 500;</code> (Only keeps customers who meet both criteria).</p> <ul> <li>OR: This is less restrictive. A row is kept if at least one of the conditions is True.</li> </ul> <p>Example</p> <p><code>WHERE country = 'USA' OR score &gt; 500;</code>.</p> <ul> <li>NOT: This is a reverse operator. it switches True to False and vice versa to exclude matching values.</li> </ul> <p>Example</p> <p><code>WHERE NOT score &lt; 500;</code> (This will return all scores that are 500 or higher).</p> <p>--- Range Operator: BETWEEN</p> <p>The <code>BETWEEN</code> operator checks if a value falls within a specific range defined by a lower and upper boundary.</p> <ul> <li>Inclusivity: The boundaries are inclusive, meaning values exactly equal to the boundaries are considered inside the range.</li> </ul> <p>Example</p> <p><code>SELECT  FROM customers WHERE score BETWEEN 100 AND 500;</code>. Alternative: You can achieve the same result using comparison operators: <code>WHERE score &gt;= 100 AND score &lt;= 500;</code>.</p> <p>--- Membership Operators: IN and NOT IN</p> <p>These operators check if a value exists within a specified list.</p> <ul> <li>Efficiency: Using <code>IN</code> is cleaner and more performant than writing multiple <code>OR</code> conditions for the same column.</li> </ul> <p>Example</p> <p><code>SELECT  FROM customers WHERE country IN ('Germany', 'USA');</code>. NOT IN: Reverses the logic to find values not present in the list.</p> <p>--- Search Operator: LIKE</p> <p>The <code>LIKE</code> operator is used for pattern matching within text strings. It relies on two special wildcards:</p> <ol> <li>Percentage (<code>%</code>): Represents \"anything\"\u2014zero, one, or multiple characters.</li> <li>Underscore (<code>_</code>): Represents exactly one character.</li> </ol> <p>Example</p> <p>Starts with: <code>WHERE first_name LIKE 'M%';</code> (Finds Maria, Martin, etc.).</p> <p>Ends with: <code>WHERE first_name LIKE '%n';</code> (Finds John, Martin, etc.).</p> <p>Contains: <code>WHERE first_name LIKE '%r%';</code> (Finds any name with an 'r' anywhere in it).</p> <p>Specific Position: <code>WHERE first_name LIKE '__r%';</code> (Uses two underscores to find names where 'r' is exactly the third character).</p>"},{"location":"sql/sql/#sql-joins","title":"SQL Joins","text":"<p>--- Combining Tables: Joins vs. Set Operators</p> <p>When you want to combine data from two tables (Table A and Table B), you must first decide whether you want to combine columns or rows.</p> <ul> <li>Joins: Used to combine columns side-by-side. This makes the resulting table wider.</li> <li>Set Operators (e.g., UNION): Used to stack rows on top of each other. This makes the resulting table longer.</li> <li>Requirement for Joins: You must define key columns (usually IDs) that exist in both tables to connect them.</li> </ul> <p>--- Why Use Joins?</p> <p>There are three primary reasons to use joins:</p> <ol> <li>Recombine Data: In professional databases, data is often spread across multiple tables (e.g., Customers, Addresses, Orders). Joins allow you to see the \"big picture\" in one result.</li> <li>Data Enrichment: Using a \"lookup\" or reference table to add extra information to a master table (e.g., adding zip codes to a customer list).</li> <li>Check Existence: Joining tables to filter data based on whether a record exists (or doesn't exist) in another table (e.g., finding customers who have never placed an order).</li> </ol> <p>--- The Four Basic Join Types*</p> <p>A. Inner Join (The Default)</p> <p>An Inner Join returns only the rows where there is a match in both tables.</p> <ul> <li>Logic: It represents the \"overlapping\" area of two circles. If a row in the left table has no match in the right table (or vice versa), it is excluded from the results.</li> <li>Syntax: <code>SELECT columns FROM tableA INNER JOIN tableB ON tableA.key = tableB.key;</code></li> </ul> <p>Note</p> <p>The order of tables does not matter; you will get the same result whether you start with Table A or Table B.</p> <p>B. Left Join A Left Join returns all rows from the left table and only the matching rows from the right table.</p> <ul> <li>Logic: The left table is your \"primary\" source. If there is no match in the right table, SQL will still show the left table's data but will display NULL for the missing right-side values.</li> <li>Syntax: <code>SELECT columns FROM tableA LEFT JOIN tableB ON tableA.key = tableB.key;</code></li> </ul> <p>Example</p> <p>To see all customers including those without orders, use a Left Join starting with the customers table.</p> <p>C. Right Join The Right Join is the exact opposite of the Left Join. It returns all rows from the right table and only the matching rows from the left table.</p> <p>Note</p> <p>Most developers prefer Left Joins. You can achieve a Right Join effect by simply switching the order of the tables in a Left Join.</p> <p>Example</p> <p><code>SELECT columns FROM tableA RIGHT JOIN tableB...</code> is the same as <code>SELECT columns FROM tableB LEFT JOIN tableA...</code>.</p> <p>D. Full Join A Full Join returns everything from both tables, regardless of whether there is a match.</p> <ul> <li>Logic: It combines the effects of both Left and Right joins. You will see matching rows side-by-side, and unmatching rows from either table will show NULL for the missing side.</li> <li>Order: Like the Inner Join, the order of tables does not matter.</li> </ul> <p>--- Best Practices for Writing Joins</p> <ul> <li>Specify the Type: While <code>JOIN</code> often defaults to <code>INNER JOIN</code>, it is best practice to explicitly write <code>INNER</code>, <code>LEFT</code>, or <code>FULL</code> to avoid confusion.</li> <li>Use Table Aliases: When tables have long names, use the <code>AS</code> keyword to give them short nicknames (e.g., <code>customers AS C</code>). This makes the query easier to read.</li> <li>Prefix Column Names: When joining tables, always specify which table a column belongs to (e.g., <code>C.ID</code> vs <code>O.ID</code>) to avoid \"ambiguous column\" errors if both tables have columns with the same name.</li> </ul> <p>--- Summary Table of Execution Logic</p> Join Type Left Table Data Right Table Data If No Match Exists... Inner Matching Only Matching Only Row is excluded. Left All Rows Matching Only Right columns show NULL. Right Matching Only All Rows Left columns show NULL. Full All Rows All Rows Missing sides show NULL. <p>--- Left Anti-Join</p> <p>A Left Anti-Join returns only the rows from the left table that have no match in the right table. It effectively filters out any overlapping data, leaving you with only the unique records in the primary (left) table.</p> <ul> <li>Syntax &amp; Logic: There is no specific \"ANTI-JOIN\" keyword in SQL Server. Instead, you create this effect in two steps:<ol> <li>Perform a standard LEFT JOIN.</li> <li>Use a WHERE clause to filter for rows where the right table's key IS NULL.</li> </ol> </li> </ul> <p>Example</p> <p>To find \"inactive\" customers who have never placed an order <pre><code>SELECT  FROM customers AS C\nLEFT JOIN orders AS O ON C.ID = O.customer_id\nWHERE O.customer_id IS NULL;\nThis returns only the customers who are in the database but have no corresponding entries in the orders table.\n</code></pre></p> <p>--- Right Anti-Join</p> <p>The Right Anti-Join is the exact opposite of the left version. It returns rows from the right table that have no match in the left table.</p> <ul> <li>Logic: The right table acts as the primary source, and the left table is used as a filter.</li> <li>The \"Better\" Way: While you can use a <code>RIGHT JOIN ... WHERE left_key IS NULL</code>, it is often cleaner to simply switch the table order and use a Left Join. Starting with the main table of interest on the left makes the query easier to read.</li> </ul> <p>Example</p> <p>To find \"orphaned\" orders that do not have a valid customer associated with them: <pre><code>SELECT  FROM orders AS O\nLEFT JOIN customers AS C ON O.customer_id = C.ID\nWHERE C.ID IS NULL;\n</code></pre></p> <p>--- ***Full Anti-Join**</p> <p>A Full Anti-Join returns all rows that do not match in either table. This is the opposite of an Inner Join; instead of showing only the overlap, it shows everything except the overlap.</p> <ul> <li>Syntax: Use a FULL JOIN combined with a WHERE clause using an OR operator to check for nulls on both sides.</li> </ul> <p>Example</p> <p>To find all \"strange cases\" (customers with no orders AND orders with no customers) simultaneously: <pre><code>SELECT  FROM customers AS C\nFULL JOIN orders AS O ON C.ID = O.customer_id\nWHERE C.ID IS NULL OR O.customer_id IS NULL;\n</code></pre></p> <p>--- Mimicking an Inner Join (Bonus Tip)</p> <p>You can achieve the effect of an Inner Join by using a Left Join combined with a <code>WHERE ... IS NOT NULL</code> condition.</p> <p>Example</p> <p><code>SELECT  FROM customers AS C LEFT JOIN orders AS O ON C.ID = O.customer_id WHERE O.customer_id IS NOT NULL;</code> This keeps only the rows where a match was found, effectively filtering out the non-matching left-table rows.</p> <p>--- Cross Join</p> <p>A Cross Join (or Cartesian Join) combines every row from the left table with every row from the right table.</p> <ul> <li>Key Characteristics:        It generates all possible combinations.        The total number of rows returned is the product of the row counts of both tables (e.g., 5 customers \u00d7 4 orders = 20 rows).        No \"ON\" condition: Unlike other joins, you do not use a key to match rows because you are combining everything regardless of whether they match.</li> <li>Use Cases: It is rarely used in daily operations but is helpful for simulations, generating test data, or creating full matrices (e.g., combining every product with every available color).</li> </ul> <p>Example</p> <pre><code>SELECT  FROM customers\nCROSS JOIN orders;\n</code></pre> <p>--- Decision Tree for Joining Tables</p> <p>When deciding how to combine tables, your choice depends on whether you are looking for matching data, all data, or unmatching data.</p> <ul> <li> <p>Matching Data Only: Use an Inner Join; this is the only type used when you strictly want the overlapping data between tables.</p> </li> <li> <p>All Data (No Missing Records):     Left Join: Use this when you have a master table (a \"main\" table) that is more important than the others and you want to keep all its records.     Full Join: Use this if all tables are equally important and you want to see all data from every side.</p> </li> <li> <p>Unmatching Data (Checkups):     Left Anti-Join: Use this to see unmatching data from one important side.     Full Anti-Join: Use this when both tables are important and you want to see unmatching data from both.</p> </li> </ul>"},{"location":"sql/sql/#sql-set","title":"SQL SET","text":"<p>--- Core Rules for Set Operators</p> <p>For a set operator to function, the participating queries must follow these strict structural rules:</p> <ul> <li>Rule 1: One Order By: You can only use the <code>ORDER BY</code> clause once, and it must be placed at the very end of the entire query.</li> <li>Rule 2: Matching Column Count: Both queries must select the exact same number of columns.</li> <li>Rule 3: Compatible Data Types: The data types of the columns must match or be compatible. SQL uses the first query to set the data type; if the second query's data cannot be converted (e.g., trying to map text to an integer), the query will fail.</li> <li>Rule 4: Identical Column Order: SQL maps columns by their position (the first column of Query A with the first column of Query B). You must ensure the data in those positions matches logically.</li> <li>Rule 5: First Query Controls Aliases: The column names (aliases) shown in the final output are determined solely by the first query. Any aliases in subsequent queries are ignored.</li> <li>Rule 6: Logical Mapping: SQL only checks if data types match, not if the data makes sense. It is the user's responsibility to ensure they aren't accidentally mapping \"first names\" to \"last names\".</li> </ul> <p>--- The Four Set Operators</p> <p>A. UNION</p> <p>Returns all distinct, unique rows from both queries. It combines the data and removes any duplicates found in the overlapping sets.</p> <p>Example</p> <p>Combining a customer list and an employee list to find every unique person. If \"Kevin Brown\" is both a customer and an employee, he will appear only once in the result.</p> <p>B. UNION ALL</p> <p>Returns all rows from both queries, including duplicates.</p> <ul> <li>Performance: <code>UNION ALL</code> is significantly faster than <code>UNION</code> because it skips the extra step of searching for and removing duplicates.</li> </ul> <p>Example</p> <p>Combining the same lists as above, \"Kevin Brown\" would appear twice in the output.</p> <p>C. EXCEPT (or MINUS)</p> <p>Returns distinct rows from the first query that are not found in the second query. This is the only set operator where the order of queries changes the result.</p> <p>Example</p> <p><code>SELECT name FROM employees EXCEPT SELECT name FROM customers;</code> finds employees who are not also customers.</p> <p>D. INTERSECT</p> <p>Returns only the rows that are common to both queries (the overlap) and removes duplicates.</p> <p>Example</p> <p><code>SELECT name FROM employees INTERSECT SELECT name FROM customers;</code> finds only the people who are both employees and customers.</p> <p>--- 3. Professional Use Cases and Examples</p> <ul> <li> <p>Use Case 1: Consolidating Similar Tables Instead of writing four separate reports for Employees, Customers, Suppliers, and Students, you can use <code>UNION</code> to combine them into one \"Persons\" table. This allows you to write a single analytical query on the combined data rather than repeating logic for four different tables.</p> </li> <li> <p>Use Case 2: Handling Split Tables Databases often split large tables by year (e.g., <code>Orders_2023</code>, <code>Orders_2024</code>). You can use <code>UNION</code> to recreate one master <code>Orders</code> table for multi-year analysis.</p> </li> <li> <p>Use Case 3: Data Migration and Quality Checks</p> </li> </ul> <p>To ensure two tables are identical after a migration:</p> <ol> <li>Run <code>TableA EXCEPT TableB</code>.</li> <li>Run <code>TableB EXCEPT TableA</code>.</li> <li>If both results are empty, the tables are 100% identical.</li> </ol> <p>--- Best Practices for Set Operators</p> <ul> <li> <p>Avoid <code>SELECT</code>: Explicitly list your columns. This prevents errors if one table's schema changes (e.g., adding or reordering columns) in the future.</p> </li> <li> <p>Add a \"Source\" Column: When combining tables, add a static string to identify where each row originated.</p> </li> </ul> <p>Example</p> <pre><code>SELECT 'Orders' AS SourceTable, OrderID FROM Orders\nUNION\nSELECT 'Archive' AS SourceTable, OrderID FROM Orders_Archive;\nThis helps users distinguish between active and archived data in the final report.\n</code></pre>"},{"location":"sql/sql/#sql-functions","title":"SQL Functions","text":"<p>--- What is an SQL Function?</p> <p>A function is a built-in code block that accepts an input value, processes it through transformations, and returns a result as an output value. Functions are essential for four main types of tasks:</p> <ul> <li>Data Manipulation: Changing values to solve specific tasks.</li> <li>Aggregations &amp; Analysis: Finding insights and building reports.</li> <li>Data Cleansing: Identifying and fixing \"bad\" data within tables.</li> <li>Data Transformation: Converting data into a more usable format.</li> </ul> <p>--- The Two Main Categories</p> <p>Functions are divided into two high-level groups based on how they handle rows:</p> <ul> <li>Single-Row Functions: These follow a \"one value in, one value out\" logic.</li> </ul> <p>Example</p> <p>If you input the name \"Maria,\" the function processes that specific string and returns a single modified value (like \"MARIA\" or \"ma\").</p> <ul> <li>Multi-Row (Aggregate) Functions: These take multiple rows as input and return a single summary value.</li> </ul> <p>Example</p> <p>The <code>SUM</code> function can take a column of values (e.g., 30, 10, 20, 40) and return the single total of 100.</p> <p>--- Nesting Functions (The \"Factory\" Logic)</p> <p>In SQL, you can nest functions, meaning you use multiple functions together to manipulate a single value. The sources compare this to a factory where material is processed through multiple stations.</p> <ul> <li>How it Works: The output of the first function becomes the input for the next function.</li> </ul> <p>Example</p> <ol> <li>LEFT: Extract the first two characters from \"Maria\" \u2192 Output: \"Ma\".</li> <li>LOWER: Take \"Ma\" and convert it to lowercase \u2192 Output: \"ma\".</li> <li>LEN (Length): Measure the characters in \"ma\" \u2192 Output: 2.</li> </ol> <ul> <li>Execution Order: SQL always executes from the inner function to the outer function. In the example above, <code>LEFT</code> runs first, followed by <code>LOWER</code>, and finally <code>LEN</code>.</li> </ul> <p>--- Functional Subcategories</p> <p>Within the two main categories, functions are further grouped by the type of data they handle:</p> Single-Row Functions (Data Prep) Multi-Row Functions (Analysis) String: Manipulating text values. Basic Aggregates: Standard summaries like SUM or AVG. Numeric: Handling math and numbers. Window/Analytical: Advanced functions for complex data analysis. Date &amp; Time: Formatting and calculating time. Null Handling: Managing missing data."},{"location":"sql/sql/#sql-string-functions","title":"SQL String Functions","text":"<p>--- Data Manipulation Functions</p> <p>These functions are used to transform or combine existing string values into a new format.</p> <ul> <li>CONCAT (Concatenation): Combines multiple string values into a single column. This is useful for merging separated data, such as first and last names, into a \"Full Name\" field.</li> </ul> <p>Example</p> <p><code>SELECT CONCAT(first_name, ' ', country) AS name_country FROM customers;</code>.</p> <ul> <li>UPPER and LOWER: These change the case of a string. <code>UPPER</code> capitalizes every character, while <code>LOWER</code> converts everything to lowercase.</li> </ul> <p>Example</p> <p><code>SELECT LOWER(first_name) AS low_name, UPPER(first_name) AS up_name FROM customers;</code>,.</p> <ul> <li>TRIM: Removes \"leading\" (start) and \"trailing\" (end) spaces from a string. Spaces are often considered \"evil\" in data because they are hard to see but can cause errors in queries. To find rows with hidden spaces, compare the column to its trimmed version: <code>WHERE first_name != TRIM(first_name)</code>.</li> <li>REPLACE: Substitutes a specific \"old\" character or string with a \"new\" one. To remove a character entirely, replace the old value with an empty string (<code>''</code>),.</li> </ul> <p>Example</p> <p>Changing a file extension from <code>.txt</code> to <code>.csv</code>: <code>REPLACE('reports.txt', '.txt', '.csv')</code>.</p> <p>--- Calculation Functions</p> <p>LEN (Length): Counts the total number of characters, digits, or symbols in a value.</p> <ul> <li>Versatility: It works on strings, numbers, and dates (including separators like underscores or dashes).</li> </ul> <p>Example</p> <p><code>SELECT LEN(first_name) FROM customers;</code>.</p> <p>--- Extraction Functions These are used to pull out specific parts of a string based on position,.</p> <ul> <li>LEFT and RIGHT: Extracts a specific number of characters starting from either the beginning (<code>LEFT</code>) or the end (<code>RIGHT</code>) of the string,. It is often best to <code>TRIM</code> a value before using <code>LEFT</code> or <code>RIGHT</code> to ensure you aren't accidentally extracting empty spaces.</li> </ul> <p>Example</p> <p><code>SELECT LEFT(first_name, 2)</code> retrieves the first two characters.</p> <ul> <li>SUBSTRING: Extracts a part of a string starting from any specified position. It requires three arguments: the value, the starting position, and the number of characters to extract.</li> </ul> <p>Example</p> <p>To extract two characters starting after the second character, you would set the starting position to 3.</p> <p>--- Advanced Concept: Building Dynamic Queries</p> <p>In professional SQL, functions are often nested to solve complex tasks. A common \"trick\" for the <code>SUBSTRING</code> function is to make it dynamic so it works on strings of varying lengths,.</p> <ul> <li>Dynamic Extraction: If you want to extract \"everything after the first character,\" you can start at position 2 and use the <code>LEN</code> function as the third argument,. This ensures that whether a name is 4 characters or 20 characters long, the query will always capture the remainder of the string.</li> </ul> <p>Example</p> <pre><code>SELECT SUBSTRING(TRIM(first_name), 2, LEN(first_name)) FROM customers;\n</code></pre>"},{"location":"sql/sql/#sql-number-functions","title":"SQL Number Functions","text":"<p>--- The ROUND Function</p> <p>The <code>ROUND</code> function is used to round a numeric value to a specified number of decimal places.</p> <ul> <li>How it Works: SQL looks at the digit immediately following your specified decimal place to determine whether to round the number up or leave it as is.    The \"5 or Higher\" Rule:        If the deciding digit is 5 or higher, SQL rounds the number up,.        If the deciding digit is less than 5, the number does not round up and stays as it is.</li> <li>Resetting Digits: Any digits remaining after the rounding point are reset to zero,.</li> </ul> <p>Examples</p> <p>Rounding to 2 Decimal Places: <code>ROUND(3.516, 2)</code> results in 3.52. The third digit (6) is higher than 5, so the second digit (1) rounds up to 2. Rounding to 1 Decimal Place: <code>ROUND(3.516, 1)</code> results in 3.5. The second digit (1) is less than 5, so the first digit (5) remains unchanged. Rounding to 0 Decimal Places: <code>ROUND(3.516, 0)</code> results in 4. Because the first digit after the decimal (5) is 5 or higher, the integer (3) is rounded up to 4.</p> <p>--- The ABS (Absolute) Function</p> <p>The <code>ABS</code> function is used to determine the absolute value of a number, effectively converting any negative number into a positive number.</p> <ul> <li>Behavior:        If the input is negative, it returns the positive version.        If the input is already positive, the function does nothing and returns the same value.</li> <li>Professional Application: This is highly useful for correcting database errors. For example, if a database accidentally contains \"negative sales\" (which is logically impossible), you can use <code>ABS</code> to transform those errors into valid positive numbers.</li> </ul> <p>Examples</p> <p><code>ABS(-10)</code> returns 10. <code>ABS(10)</code> returns 10.</p>"},{"location":"sql/sql/#sql-date-time-functions","title":"SQL Date Time Functions","text":"<p>--- Three Sources of Dates</p> <p>You can query date information from three different sources. 1. Stored Columns: Data already saved in database tables (e.g., <code>order_date</code> or <code>creation_time</code>). 2. Hardcoded Strings: Static dates added manually to a query (e.g., <code>'2025-08-20'</code>). 3. GETDATE() Function: A fundamental function that returns the current system date and time at the moment the query is executed.</p> <p>--- Part Extraction Functions</p> <p>These functions extract specific components from a date.</p> <ul> <li>Basic Extraction (Day, Month, Year) These functions are quick and return the result as an integer.</li> </ul> <p>Example</p> <p><code>SELECT MONTH(creation_time) AS month_num</code> returns <code>1</code> for January, <code>2</code> for February, etc..</p> <ul> <li>DATEPART Returns a specific part of a date as an integer. It is more powerful than the basic functions because it can extract parts like weeks, quarters, and hours.</li> </ul> <p>Example</p> <p><code>DATEPART(quarter, creation_time)</code> returns <code>1</code> for dates in January through March.</p> <ul> <li>DATENAME Similar to <code>DATEPART</code>, but it returns the name of the part as a string (text), making it ideal for human-readable reports. !!! Example:     <code>DATENAME(month, '2025-08-20')</code> returns \"August\" (string), whereas <code>DATEPART</code> would return 8 (integer).     Usage Tip: Use <code>DATENAME(weekday, date)</code> to get the full name of the day, such as \"Wednesday\".</li> </ul> <p>--- Truncation and Calculations</p> <ul> <li>DATETRUNC This function \"truncates\" a date to a specific level in the hierarchy by resetting all lower levels to their minimum values (0 for time, 01 for days). If you truncate at the Month level, the Year and Month are kept, but the Day is reset to <code>01</code> and the Time is reset to <code>00:00:00</code>.</li> </ul> <p>Example</p> <p><code>DATETRUNC(year, creation_time)</code> resets everything to January 1st of that year.</p> <ul> <li>EOMONTH (End of Month) Returns the last day of the month for a given date.</li> </ul> <p>Example</p> <p>If the input is <code>'2025-02-01'</code>, <code>EOMONTH</code> returns <code>'2025-02-28'</code>. Trick for Start of Month: While there is no \"Start of Month\" function, you can use <code>DATETRUNC</code> at the month level to get the first day of any month.</p> <p>--- Professional Applications and Best Practices</p> <ul> <li>Data Aggregation Extracting parts allows you to group data for reporting, such as calculating \"Total Sales by Year\" or \"Orders per Month\".</li> </ul> <p>Example</p> <p>To count orders per month: <code>SELECT MONTH(order_date), COUNT() FROM sales_orders GROUP BY MONTH(order_date);</code>.</p> <ul> <li>Filtering Data You can use extracted parts in a <code>WHERE</code> clause to filter for specific timeframes.</li> </ul> <p>Example</p> <p>To see only February orders: <code>WHERE MONTH(order_date) = 2;</code>. Performance Tip: Always prefer filtering by integers (using <code>MONTH</code> or <code>DATEPART</code>) rather than strings (using <code>DATENAME</code>). Searching for numbers is faster for the database.</p> <p>---  Formatting vs. Casting</p> <ul> <li>Formatting: Changing how a value looks. For example, displaying a date with slashes instead of dashes, or showing a number with a currency symbol.</li> <li>Casting: Changing the data type from one to another. For example, converting a string of text (\"123\") into an actual integer (123) so you can perform math on it.</li> </ul> <p>--- Date Format Specifiers</p> <p>To format dates, SQL uses \"format specifiers\" which act as a code for different components. These are case-sensitive.</p> <ul> <li>Year: <code>yyyy</code> (4 digits) or <code>yy</code> (2 digits).</li> <li>Month: <code>MM</code> (2 digits), <code>MMM</code> (abbreviation), or <code>MMMM</code> (full name).</li> </ul> <p>Note</p> <p>Big M is for Month; small m is for minutes.</p> <ul> <li>Day: <code>dd</code> (2 digits), <code>ddd</code> (abbreviated name), or <code>dddd</code> (full name).</li> <li>Time: <code>HH</code> (24-hour), <code>hh</code> (12-hour), <code>mm</code> (minutes), <code>ss</code> (seconds), and <code>tt</code> (AM/PM designator).</li> </ul> <p>--- The <code>FORMAT</code> Function</p> <p>This function is used primarily to change the look of dates and numbers into a string value.</p> <ul> <li>Syntax: <code>FORMAT(value, format, [culture])</code></li> </ul> <p>Examples</p> <p>Date Formatting: <code>FORMAT(creation_time, 'dd-MM-yyyy')</code> changes the international standard (yyyy-MM-dd) to a European style. Number Formatting:    'N': Adds commas to large numbers (Numeric).    'C': Adds a currency symbol (Currency).    'P': Converts the number to a percentage (Percentage). Custom Strings: You can combine static text with <code>FORMAT</code> to create complex labels, such as \"Day: Monday Jan Q1 2025\".</p> <p>--- The <code>CONVERT</code> Function</p> <p><code>CONVERT</code> is a versatile function that can perform both casting and formatting for date and time values.</p> <ul> <li>Syntax: <code>CONVERT(data_type, value, [style_number])</code></li> </ul> <p>Examples</p> <p>Casting Only: <code>CONVERT(int, '123')</code> turns a string into an integer. Casting and Formatting: <code>CONVERT(varchar, creation_time, 32)</code> converts a date-time value into a string formatted as <code>MM-dd-yyyy</code>.</p> <p>--- The <code>CAST</code> Function</p> <p><code>CAST</code> is the most straightforward function, used strictly for changing data types. It does not allow for custom styling or formatting; it always returns the SQL standard format.</p> <ul> <li>Syntax: <code>CAST(value AS data_type)</code></li> </ul> <p>Examples</p> <p>String to Number: <code>CAST('123' AS int)</code>. DateTime to Date: <code>CAST(creation_time AS date)</code> removes the time information and keeps only the date. Integer to String: <code>CAST(123 AS varchar)</code>.</p> <p>--- Function Comparison Summary</p> Feature <code>CAST</code> <code>CONVERT</code> <code>FORMAT</code> Primary Goal Change data type Change type &amp; format Change look (to string) Formatting No formatting Date/Time styles only Date/Time and Numbers Output Type Any type Any type String only Best For... Quick type changes Date standard styles Custom reports/Numbers <p>Tip</p> <p><code>FORMAT</code> is highly useful for data preparation before aggregation. You can format dates as 'MMM-yy' (e.g., \"Jan-25\") and then group by that column to create clean, readable monthly sales reports.</p> <p>--- DATEADD Function</p> <p>The DATEADD function is used to add or subtract a specific time interval (years, months, days, etc.) to or from a date,.</p> <ul> <li>Syntax: <code>DATEADD(part, interval, date)</code>.     Part: The specific portion of the date you want to manipulate (e.g., <code>year</code>, <code>month</code>, <code>day</code>).     Interval: The amount you want to change. A positive number adds time, while a negative number subtracts time.     Date: The original date value you are starting from.</li> </ul> <p>Examples</p> <p>Adding Years: <code>DATEADD(year, 2, order_date)</code> results in a date exactly two years later than the original. Adding Months: <code>DATEADD(month, 3, order_date)</code> moves the date three months forward (e.g., January becomes April). Subtracting Days: <code>DATEADD(day, -10, order_date)</code> retrieves a date 10 days before the original.</p> <p>--- DATEDIFF Function</p> <p>The DATEDIFF function calculates the difference between two dates and returns the result as a number,.</p> <ul> <li> <p>Syntax: <code>DATEDIFF(part, start_date, end_date)</code>.     Part: The unit of measurement for the result (e.g., how many <code>years</code>, <code>months</code>, or <code>days</code> are between them).     Start Date: The earlier (younger) date.     End Date: The later (older) date.</p> </li> <li> <p>Professional Use Cases:</p> </li> <li> <p>Calculating Age: To find an employee's age, you calculate the years between their birth date and the current date: <code>DATEDIFF(year, birth_date, GETDATE())</code>,.</p> </li> <li>Shipping Duration: To find how long it took to ship an order, you calculate the days between the order date and the shipping date: <code>DATEDIFF(day, order_date, ship_date)</code>.</li> <li>Time Gap Analysis: You can use <code>DATEDIFF</code> alongside the <code>LAG</code> window function to find the number of days between a current order and the previous order placed by a customer,.</li> </ul> <p>--- ISDATE Function</p> <p>The ISDATE function is a validation tool used to check if a string or value is a valid date.</p> <ul> <li>Return Values: It returns 1 (True) if the value is a valid date and 0 (False) if it is not.</li> <li>Logic: SQL evaluates whether the value follows standard database date formats.</li> </ul> <p>Examples</p> <p><code>ISDATE('2025-08-20')</code> returns 1. <code>ISDATE('123')</code> returns 0. <code>ISDATE('2025')</code> returns 1 (SQL is smart enough to interpret a single year as the first of January of that year). Non-standard formats: If you provide a date in a format SQL does not recognize (e.g., <code>day/month/year</code> when the system expects <code>year/month/day</code>), it will return 0.</p> <p>--- Advanced Use Case: Data Cleansing</p> <p>A major use for <code>ISDATE</code> is handling data quality issues where a string column contains mostly dates but some \"corrupt\" or invalid text.</p> <ul> <li>The Problem: Attempting to <code>CAST</code> a string column directly to a <code>DATE</code> type will result in a query error if even one row contains invalid data.</li> <li>The Solution: Use a CASE WHEN statement combined with <code>ISDATE</code> to safely convert the data.</li> </ul> <p>Example</p> <pre><code>CASE\n    WHEN ISDATE(order_date) = 1 THEN CAST(order_date AS DATE)\n    ELSE NULL -- Or a dummy value like '9999-12-31'\nEND AS New_Order_Date\nThis approach allows you to successfully convert valid strings into dates while turning invalid strings into `NULL` (or a identifiable dummy value) instead of crashing the entire query.\n</code></pre>"},{"location":"sql/sql/#sql-null-functions","title":"SQL NULL Functions","text":"<p>--- Understanding NULLs</p> <p>In SQL, a NULL represents a missing, unknown, or nonexistent value.    It is not zero: NULL is not equivalent to the number 0.    It is not an empty string: It is not a blank space or an empty set of quotes ('').    Logical Meaning: A NULL simply tells us that there is \"nothing\" there or the value is currently unknown.</p> <p>--- Functions to Replace NULLs</p> <p>If you want to remove a NULL from your results and replace it with a meaningful value, you use <code>ISNULL</code> or <code>COALESCE</code>.</p> <p>A. ISNULL (SQL Server Specific)</p> <p>This function checks a value and replaces it if it is NULL. It is limited to two arguments.    Syntax: <code>ISNULL(check_expression, replacement_value)</code>.</p> <p>Example</p> <p>(Static Replacement): <code>ISNULL(shipping_address, 'Unknown')</code> replaces any missing address with the word \"Unknown\". (Column Replacement): <code>ISNULL(shipping_address, billing_address)</code> uses the billing address as a backup if the shipping address is missing.</p> <p>B. COALESCE (Standard SQL)</p> <p><code>COALESCE</code> is more flexible and powerful because it accepts a list of multiple values. It returns the first non-null value in that list, checking from left to right.</p> <p>Example</p> <p><code>COALESCE(shipping_address, billing_address, 'N/A')</code>. SQL checks the shipping address; if that's null, it checks the billing address; if both are null, it returns \"N/A\". Advantage: It is a database standard and works across SQL Server, Oracle, and MySQL, making scripts easier to migrate.</p> <p>--- Function to Create NULLs: NULLIF</p> <p><code>NULLIF</code> does the opposite of the functions above; it replaces a real value with a NULL.</p> <ul> <li>Logic: It returns NULL if the two values are equal. If they are not equal, it returns the first value. Use Case (Data Cleansing): If a price is entered as -1 (an error), you can turn it into a NULL: <code>NULLIF(price, -1)</code>. Use Case (Preventing Errors): To avoid \"Divide by Zero\" errors, you can wrap the divisor: <code>Sales / NULLIF(quantity, 0)</code>.</li> </ul> <p>--- Keywords for Checking NULLs</p> <p>To filter or check for the existence of NULLs without changing them, use these boolean keywords.</p> <ul> <li>IS NULL: Returns true if the value is missing.</li> </ul> <p>Example</p> <p><code>WHERE score IS NULL</code> finds customers with no scores.</p> <ul> <li>IS NOT NULL: Returns true if a value exists.</li> </ul> <p>Example</p> <p><code>WHERE score IS NOT NULL</code> retrieves a clean list of customers with scores.</p> <p>--- Professional Use Cases</p> <ul> <li> <p>Data Aggregations Standard aggregate functions (SUM, AVG, MIN, MAX) totally ignore NULLs.    The Risk: If you have sales of 15, 25, and NULL, <code>AVG</code> will return 20 (it divides by 2 instead of 3).    The Fix: If the business considers a NULL to be a zero, handle it first: <code>AVG(COALESCE(score, 0))</code>.</p> </li> <li> <p>Mathematical &amp; String Operations Any operation involving a NULL (using <code>+</code>, <code>-</code>, etc.) results in a NULL because you cannot perform math on an unknown value.    String Concatenation: <code>First_Name + ' ' + Last_Name</code> will return NULL if the last name is missing.    The Fix: Use <code>COALESCE(last_name, '')</code> to replace the NULL with an empty string so the first name still displays.</p> </li> <li> <p>Joining Tables SQL cannot compare NULLs in join keys. If Table A has a NULL in the key and Table B also has a NULL, an Inner Join will ignore them, and you will lose data.    The Fix: Handle the NULLs directly in the <code>ON</code> clause: <code>ON ISNULL(T1.Type, '') = ISNULL(T2.Type, '')</code>.</p> </li> <li> <p>Sorting Data By default, <code>ORDER BY</code> places NULLs at the start of an ascending list.</p> </li> </ul> <p>Professional Solution: To force NULLs to the end while keeping the rest in ascending order, create a \"flag\" column</p> <pre><code>```sql\nORDER BY (CASE WHEN score IS NULL THEN 1 ELSE 0 END), score;\nThis pushes the NULLs (flag 1) to the bottom regardless of the actual score values.\n```\n</code></pre> <ul> <li>Advanced: Left Anti-Join This technique finds records in the left table that have no match in the right table (e.g., customers who have never placed an order).    How-To: Perform a <code>LEFT JOIN</code> and then use <code>WHERE [Right_Table_Key] IS NULL</code> to filter for the unmatching rows.</li> </ul>"},{"location":"sql/sql2/","title":"Sql2","text":""},{"location":"sql/sql2/#sql-data-types","title":"SQL Data Types","text":""},{"location":"sql/sql2/#types-of-commands-in-sql","title":"Types of commands in SQL","text":"<ol> <li>Data Definition language</li> <li>Data Manipulation language</li> <li>Data Query language</li> <li>Data Control language</li> </ol>"},{"location":"sql/sql2/#sql-constraints","title":"SQL Constraints","text":"<p>SQL constraints are used to specify rules for the data in a table. Constraints are used to limit the type of data that can go into a table. This ensures the accuracy and reliability of the data in the table. If there is any violation between the constraint and the data action, the action is aborted. Constraints can be column level or table level.</p> <p>The following constraints are commonly used in SQL:</p> <ol> <li>NOT NULL - Ensures that a column cannot have a NULL value</li> <li>UNIQUE - Ensures that all values in a column are different</li> <li>PRIMARY KEY - A combination of a NOT NULL and UNIQUE. Uniquely identifies each row in a table</li> <li>FOREIGN KEY - Prevents actions that would destroy links between tables</li> <li>CHECK - Ensures that the values in a column satisfies a specific condition</li> <li>DEFAULT - Sets a default value for a column if no value is specified</li> </ol> <p>Primary Key</p> <ol> <li>Uniqueness: Each primary key value must be unique per table row.</li> <li>Immutable: Primary keys should not change once set.</li> <li>Simplicity: Ideal to keep primary keys as simple as possible.</li> <li>Non-Intelligent: They shouldn't contain meaningful information.</li> <li>Indexed: Primary keys are automatically indexed for faster data retrieval.</li> <li>Referential Integrity: They serve as the basis for foreign keys in other tables.</li> <li>Data Type: Common types are integer or string.</li> </ol> <p>Foreign Key</p> <ol> <li>Referential Integrity: Foreign keys link records between tables, maintaining data consistency.</li> <li>Nullable: Foreign keys can contain null values unless specifically restricted.</li> <li>Match Primary Keys: Each foreign key value must match a primary key value in the parent table, or be null.</li> <li>Ensure Relationships: They define the relationship between tables in a database.</li> <li>No Uniqueness: Foreign keys don't need to be unique.</li> </ol>"},{"location":"sql/sql2/#functions","title":"Functions","text":"<p>Functions in MySQL are reusable blocks of code that perform a specific task and return a single value.</p> <p>Purpose: Simplify complex calculations, enhance code reusability, and improve query performance.</p> <p>Types: Built-in Functions and User-Defined Functions (UDFs).</p> <ol> <li> <p>Built-in Functions</p> <p>a. String Functions (e.g., CONCAT, LENGTH, SUBSTRING)</p> <p>b. Numeric Functions (e.g., ABS, ROUND, CEIL)</p> <p>c. Date and Time Functions (e.g., NOW, DATE_FORMAT, DATEDIFF)</p> <p>d. Aggregate Functions (e.g., COUNT, SUM, AVG)</p> </li> <li> <p>User-Defined Functions (UDFs) </p> </li> </ol> <p>Custom functions created by users to perform specific operations. It is customizable, reusable, and encapsulate complex logic.</p> <pre><code>    DELIMITER $$\n    CREATE FUNCTION function_name(parameter(s))\n    RETURNS data_type\n    DETERMINISTIC\n    BEGIN\n    -- function body\n    RETURN value;\n    END $$\n    DELIMITER ;\n</code></pre>"},{"location":"sql/sql2/#joins","title":"JOINS","text":"<p>SQL joins are used to combine rows from two or more tables, based on a related column between them.</p> <p>Here are the main types of SQL joins:</p> <p></p> <p>Inner Join</p> <p>Returns records that have matching values in both tables. </p> <pre><code>SELECT Customers.customer_id,\nCustomers.first_name,\nOrders.amount \nFROM Customers\nINNER JOIN Orders\nON Orders.customer = Customers.customer_id;\n</code></pre> <p>Left Join</p> <p>Returns all records from the left table (table1), and the matched records from the right table(table2). If no match, the result is NULL on the right side. </p> <pre><code>SELECT Customers.customer_id,\nCustomers.first_name,\nOrders.amount\nFROM Customers\nLEFT JOIN Orders\nON Orders.customer = Customers.customer_id;\n</code></pre> <p>Right Join</p> <p>Returns all records from the right table (table2), and the matched records from the left table(table1). If no match, the result is NULL on the left side.</p> <p></p> <pre><code>SELECT Customers.customer_id,\nCustomers.first_name,\nOrders.amount\nFROM Customers\nRIGHT JOIN Orders\nON Orders.customer = Customers.customer_id;\n</code></pre> <p>Full Join</p> <p>Returns all records when there is a match in either left (table1) or right (table2) table records.</p> <p></p> <pre><code>SELECT Customers.customer_id,\nCustomers.first_name,\nOrders.amount\nFROM Customers\nFULL OUTER JOIN Orders\nON Orders.customer = Customers.customer_id;\n</code></pre> <p>Cross Join</p> <p>Returns the Cartesian product of the sets of records from the two or more joined tables when no WHERE clause is used with CROSS JOIN. </p> <p></p> <pre><code>SELECT Model.car_model,\nColor.color_name\nFROM Model\nCross JOIN Color;\n</code></pre> <p>Self Join</p> <p>A regular join, but the table is joined with itself. </p> <p>Now, to show the name of the manager for each employee in the same row, we can run the following query:</p> <pre><code>SELECT\nemployee.Id,\nemployee.FullName,\nemployee.ManagerId,\nmanager.FullName as ManagerName\nFROM Employees employee\nJOIN Employees manager\nON employee.ManagerId = manager.Id;\n</code></pre> <p>Group By WITH ROLLUP</p> <p>The GROUP BY clause in MySQL is used to group rows that have the same values in specified columns into aggregated data. The WITH ROLLUP option allows you to include extra rows that represent subtotals and grand totals.</p> <p></p> <pre><code>SELECT\nSUM(payment_amount),\nYEAR(payment_date) AS 'Payment Year',\nstore_id AS 'Store'\nFROM payment\nGROUP BY YEAR(payment_date), store_id WITH ROLLUP\nORDER BY YEAR(payment_date), store_id;\n</code></pre>"},{"location":"sql/sql2/#views","title":"Views","text":"<p>A view in SQL is a virtual table based on the result-set of an SQL statement. It contains rows and columns, just like a real table. The fields in a view are fields from one or more real tables in the database.</p> <p>Here are some key points about views:</p> <ol> <li>You can add SQL functions, WHERE, and JOIN statements to a view and display the data as if the data were coming from one single table.</li> <li>A view always shows up-to-date data. The database engine recreates the data every time a user queries a view.</li> <li>Views can be used to encapsulate complex queries, presenting users with a simpler interface to the data.</li> <li>They can be used to restrict access to sensitive data in the underlying tables, presenting only non-sensitive data to users.</li> </ol> <p></p> <pre><code>CREATE VIEW View_Products AS SELECT ProductName, Price FROM Products\nWHERE Price &gt; 30;\n</code></pre>"},{"location":"sql/sql2/#window-functions","title":"Window Functions","text":"<ol> <li>Window functions: These are special SQL functions that perform a calculation across a set of related rows.</li> <li>How it works: Instead of operating on individual rows, a window function operates on a group or 'window' of rows that are somehow related to the current row. This allows for complex calculations based on these related rows.</li> <li>Window definition: The 'window' in window functions refers to a set of rows. The window can be defined using different criteria depending on the requirements of your operation.</li> <li>Partitions: By using the PARTITION BY clause, you can divide your data into smaller sets or 'partitions'. The window function will then be applied individually to each partition.</li> <li>Order of rows: You can specify the order of rows in each partition using the ORDER BY clause. This order influences how some window functions calculate their result.</li> <li>Frames: The ROWS/RANGE clause lets you further narrow down the window by defining a 'frame' or subset of rows within each partition.</li> <li>Comparison with Aggregate Functions: Unlike aggregate functions that return a single result per group, window functions return a single result for each row of the table based on the group of rows defined in the window.</li> <li>Advantage: Window functions allow for more complex operations that need to take into account not just the current row, but also its 'neighbours' in some way.</li> </ol>"},{"location":"sql/sql2/#syntax","title":"syntax","text":"<pre><code>function_name (column) OVER (\n[PARTITION BY column_name_1, ..., column_name_n]\n[ORDER BY column_name_1 [ASC | DESC], ..., column_name_n [ASC | DESC]]\n)\n</code></pre> <ol> <li>function_name: This is the window function you want to use. Examples include ROW_NUMBER(), RANK(), DENSE_RANK(), SUM(), AVG(), and many others.</li> <li>(column): This is the column that the window function will operate on. For some functions like SUM(salary)</li> <li>OVER (): This is where you define the window. The parentheses after OVER contain the specifications for the window.</li> <li>PARTITION BY column_name_1, ..., column_name_n: This clause divides the result set into partitions upon which the window function will operate independently. For example, if you have PARTITION BY salesperson_id, the window function will calculate a result for each salesperson independently.</li> <li>ORDER BY column_name_1 [ASC | DESC], ..., column_name_n [ASC | DESC]: This clause specifies the order of the rows in each partition. The window function operates on these rows in the order specified. For example, ORDERBY sales_date DESC will make the window function operate on rows with more recent dates first.</li> </ol>"},{"location":"sql/sql2/#types-of-windows-function","title":"Types of Windows Function","text":"<p>There are three main categories of window functions in SQL: Ranking functions, Value functions, and Aggregate functions. Here's a brief description and example for each:</p>"},{"location":"sql/sql2/#ranking-functions","title":"Ranking Functions:","text":"<ol> <li>ROW_NUMBER(): Assigns a unique row number to each row, ranking start from 1 and keep increasing till the end of last row</li> </ol> <pre><code>SELECT Studentname,\nSubject,\nMarks,\nROW_NUMBER() OVER(ORDER BY Marks desc)\nRowNumber\nFROM ExamResult;\n</code></pre> <ol> <li>RANK(): Assigns a rank to each row. Rows with equal values receive the same rank, with the next row receiving a rank which skips the duplicate rankings.</li> </ol> <pre><code>SELECT Studentname,\nSubject,\nMarks,\nRANK() OVER(ORDER BY Marks DESC) Rank\nFROM ExamResult\nORDER BY Rank;\n</code></pre> <ol> <li>DENSE_RANK(): Similar to RANK(), but does not skip rankings if there are duplicates.</li> </ol> <pre><code>SELECT Studentname,\nSubject,\nMarks,\nDENSE_RANK() OVER(ORDER BY Marks DESC) Rank\nFROM ExamResult\n</code></pre>"},{"location":"sql/sql2/#value-functions","title":"Value Functions:","text":"<p>These functions perform calculations on the values of the window rows.</p> <ol> <li>FIRST_VALUE(): Returns the first value in the window.</li> </ol> <p></p> <pre><code>SELECT\nemployee_name,\ndepartment,\nhours,\nFIRST_VALUE(employee_name) OVER (\nPARTITION BY department\nORDER BY hours\n) least_over_time\nFROM\novertime;\n</code></pre> <ol> <li>LAST_VALUE(): Returns the last value in the window.</li> </ol> <p></p> <pre><code>SELECT employee_name, department,salary,\nLAST_VALUE(employee_name)\nOVER (\nPARTITION BY department ORDER BY\nsalary\n) as max_salary\nFROM Employee;\n</code></pre> <ol> <li>LAG(): Returns the value of the previous row.</li> </ol> <p></p> <pre><code>SELECT\nYear,\nQuarter,\nSales,\nLAG(Sales, 1, 0) OVER(\nPARTITION BY Year\nORDER BY Year,Quarter ASC)\nAS NextQuarterSales\nFROM ProductSales;\n</code></pre> <ol> <li>LEAD(): Returns the value of the next row.</li> </ol> <p></p> <pre><code>SELECT Year,\nQuarter,\nSales,\nLEAD(Sales, 1, 0) OVER(\nPARTITION BY Year\nORDER BY Year,Quarter ASC)\nAS NextQuarterSales\nFROM ProductSales;\n</code></pre>"},{"location":"sql/sql2/#aggregation-functions","title":"Aggregation Functions:","text":"<p>These functions perform calculations on the values of the window rows.</p> <ol> <li>SUM()</li> <li>MIN()</li> <li>MAX()</li> <li>AVG()</li> </ol>"},{"location":"sql/sql2/#frame-clause","title":"Frame Clause","text":"<p>The frame clause in window functions defines the subset of rows ('frame') used for calculating the result of the function for the current row.</p> <p>It's specified within the OVER() clause after PARTITION BY and ORDER BY.</p> <p>The frame is defined by two parts: a start and an end, each relative to the current row.</p> <p>Generic syntax for a window function with a frame clause:     function_name (expression) OVER (     [PARTITION BY column_name_1, ..., column_name_n]     [ORDER BY column_name_1 [ASC | DESC], ..., column_name_n [ASC | DESC]]     [ROWS|RANGE frame_start TO frame_end]     )</p> <p>The frame start can be:  1. UNBOUNDED PRECEDING (starts at the first row of the partition)  2. N PRECEDING (starts N rows before the current row)  3. CURRENT ROW (starts at the current row)</p> <p>The frame end can be:  1. UNBOUNDED FOLLOWING (ends at the last row of the partition)  2. N FOLLOWING (ends N rows after the current row)  3. CURRENT ROW (ends at the current row)</p> <p>For ROWS, the frame consists of N rows coming before or after the current row.</p> <p>For RANGE, the frame consists of rows within a certain value range relative to the value in the current row.</p> <p></p> <p>ROWS BETWEEN Example:</p> <p></p> <pre><code>SELECT date, revenue,\nSUM(revenue) OVER (\nORDER BY date\nROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) running_total\nFROM sales\nORDER BY date;\n</code></pre> <p>RANGE BETWEEN Example:</p> <p></p> <pre><code>SELECT\nshop,\ndate,\nrevenue_amount,\nMAX(revenue_amount) OVER (\nORDER BY DATE\nRANGE BETWEEN INTERVAL '3' DAY PRECEDING\nAND INTERVAL '1' DAY FOLLOWING\n) AS max_revenue\nFROM revenue_per_shop;\n</code></pre>"},{"location":"sql/sql2/#common-table-expression","title":"Common Table Expression","text":"<p>A Common Table Expression (CTE) in SQL is a named temporary result set that exists only within the execution scope of a single SQL statement. Here are some important points to note about CTEs:</p> <p>CTEs can be thought of as alternatives to derived tables, inline views, or subqueries.</p> <p>They can be used in SELECT, INSERT, UPDATE, or DELETE statements.</p> <p>CTEs help to simplify complex queries, particularly those involving multiple subqueries or recursive queries.</p> <p>They make your query more readable and easier to maintain.</p> <p>A CTE is defined using the WITH keyword, followed by the CTE name and a query. The CTE can then be referred to by its name elsewhere in the query.</p> <p>Here's a basic example of a CTE:     WITH sales_cte AS (     SELECT sales_person, SUM(sales_amount) as total_sales     FROM sales_table     GROUP BY sales_person     )     SELECT sales_person, total_sales     FROM sales_cte     WHERE total_sales &gt; 1000;</p> <p>Recursive CTE: </p> <p>This is a CTE that references itself. In other words, the CTE query definition refers back to the CTE name, creating a loop that ends when a certain condition is met. Recursive CTEs are useful for working with hierarchical or tree-structured data.</p> <p></p> <p>Example: </p> <pre><code>WITH RECURSIVE number_sequence AS (\nSELECT 1 AS number\nUNION ALL\nSELECT number + 1\nFROM number_sequence\nWHERE number &lt; 10\n)\nSELECT * FROM number_sequence;\n</code></pre>"},{"location":"sql/sql2/#indexing","title":"Indexing","text":"<p>Indexing in databases involves creating a data structure that improves the speed of data retrieval operations on a database table.</p> <p>Indexes are used to quickly locate data without having to search every row in a table each time a database table is accessed.</p> <p></p>"},{"location":"sql/sql2/#why-is-indexing-important","title":"Why is Indexing Important?","text":"<p>Indexes are crucial for enhancing the performance of a database by:</p> <ol> <li>Speeding up Query Execution: Indexes reduce the amount of data that needs to be scanned for a query, significantly speeding up data retrieval operations.</li> <li>Optimizing Search Operations: Indexes help in efficiently searching for records based on the indexed columns.</li> <li>Improving Sorting and Filtering: Indexes assist in sorting and filtering operations by providing a structured way to access data.</li> <li>Enhancing Join Performance: Indexes on join columns improve the performance of join operations between tables.</li> </ol>"},{"location":"sql/sql2/#advantages-of-indexing","title":"Advantages of Indexing","text":"<ol> <li>Faster Data Retrieval: Indexes make search queries faster by providing a quick way to locate rows in a table.</li> <li>Efficient Use of Resources: Reduced query execution time translates to more efficient use of CPU and memory resources.</li> <li>Improved Performance for Large Tables: Indexes are particularly beneficial for large tables where full table scans would be time-consuming.</li> <li>Better Sorting and Filtering: Indexes can improve the performance of ORDER BY, GROUP BY, and WHERE clauses.</li> </ol>"},{"location":"sql/sql2/#how-to-choose-the-right-indexing-column","title":"How to Choose the Right Indexing Column","text":"<ol> <li>Primary Key and Unique Constraints: Always index columns that are primary keys or have unique constraints, as they uniquely identify rows.</li> <li>Frequently Used Columns in WHERE Clauses: Index columns that are frequently used in WHERE clauses to filter data.</li> <li>Columns Used in Joins: Index columns that are used in join conditions to speed up join operations.</li> <li>Columns Used in ORDER BY and GROUP BY: Index columns that are used in ORDER BY and GROUP BY clauses for faster sorting and grouping.</li> <li>Selectivity of the Column: Choose columns with high selectivity (columns with many unique values) to maximize the performance benefits of the index.</li> </ol>"},{"location":"sql/sql2/#query-optimizations","title":"Query Optimizations","text":"<ol> <li>Use Column Names Instead of * in a SELECT Statement</li> </ol> <p>Avoid including a HAVING clause in SELECT statements</p> <p>The HAVING clause is used to filter the rows after all the rows are selected and it is used like a filter. It is quite useless in a SELECT statement. It works by going through the final result table of the query parsing out the rows that don\u2019t meet the HAVING condition.</p> <p>Example:</p> <pre><code>Original query:\nSELECT s.cust_id,count(s.cust_id)\nFROM SH.sales s\nGROUP BY s.cust_id\nHAVING s.cust_id != '1660' AND s.cust_id != '2';\n\nImproved query:\nSELECT s.cust_id,count(cust_id)\nFROM SH.sales s\nWHERE s.cust_id != '1660'\nAND s.cust_id !='2'\nGROUP BY s.cust_id;\n</code></pre> <ol> <li>Eliminate Unnecessary DISTINCT Conditions</li> </ol> <p>Considering the case of the following example, the DISTINCT keyword in the original query is unnecessary because the table_name contains the primary key p.ID, which is part of the result set.</p> <p>Example:</p> <pre><code>Original query:\nSELECT DISTINCT * FROM SH.sales s\nJOIN SH.customers c\nON s.cust_id= c.cust_id\nWHERE c.cust_marital_status = 'single';\n\nImproved query:\nSELECT * FROM SH.sales s JOIN\nSH.customers c\nON s.cust_id = c.cust_id\nWHERE c.cust_marital_status='single';\n</code></pre> <ol> <li>Consider using an IN predicate when querying an indexed column</li> </ol> <p>The IN-list predicate can be exploited for indexed retrieval and also, the optimizer can sort the IN-list to match the sort sequence of the index, leading to more efficient retrieval. Example:</p> <pre><code>Original query:\nSELECT s.*\nFROM SH.sales s\nWHERE s.prod_id = 14\nOR s.prod_id = 17;\n\nImproved query:\nSELECT s.*\nFROM SH.sales s\nWHERE s.prod_id IN (14, 17);\n</code></pre> <ol> <li>Try to use UNION ALL in place of UNION</li> </ol> <p>The UNION ALL statement is faster than UNION, because UNION ALL statement does not consider duplicate s, and UNION statement does look for duplicates in a table while selection of rows, whether or not they exist. Example:</p> <pre><code>Original query:\nSELECT cust_id\nFROM SH.sales\nUNION\nSELECT cust_id\nFROM customers;\n\nImproved query:\nSELECT cust_id\nFROM SH.sales\nUNION ALL\nSELECT cust_id\nFROM customers;\n</code></pre>"},{"location":"systemdesign/consistenthashing/","title":"Consistent Hashing","text":""},{"location":"systemdesign/consistenthashing/#the-problem-consistent-hashing-solves","title":"The Problem Consistent Hashing Solves","text":"<p>The limitation of simple hashing (as discussed in the conversation history) is not merely load balancing, but the management of servers. The key challenge is that adding and removing servers completely changes the local data stored in each server.</p>"},{"location":"systemdesign/consistenthashing/#introducing-the-consistent-hashing-ring-architecture","title":"Introducing the Consistent Hashing Ring Architecture","text":"<p>To overcome the issues caused by changing the modulo base (N), consistent hashing uses a circular structure called a ring.</p> <p>Instead of an array, the architecture uses a ring that contains hash positions ranging from 0 up to \\(M-1\\). The value \\(M-1\\) wraps around to 0, forming the circle. This ring represents the entire possible search space for the hash function.</p>"},{"location":"systemdesign/consistenthashing/#mapping-requests-and-servers-to-the-ring","title":"Mapping Requests and Servers to the Ring","text":"<p>Both requests and servers are mapped onto this hash ring using hash functions.</p> <p>Request Mapping</p> <p>Requests still have IDs, and these request IDs are hashed. The hash function output (a number between 0 and \\(M-1\\)) maps the request to a specific point on the ring. Multiple requests can be mapped as points around the ring.</p> <p>Server Mapping</p> <p>Servers also have IDs (e.g., 0, 1, 2, 3, 4).</p> <p>These server IDs are also hashed using the same or a different hash function.</p> <p>The hash result is taken modulo \\(M\\) (the search space size) to place the server onto a specific position on the ring.</p> <p>Example</p> <p>If hashing server ID 0 results in 49, and \\(M\\) is 30, \\(49 \\text{ mod } 30 = 19\\). Server 1 would be mapped to hash position 19. In the example architecture, four servers (SS1, SS2, SS3, SS4) are mapped to different points on the ring.</p> <p>Request Assignment (The Clockwise Rule)</p> <p>Once both requests and servers are placed on the ring, the assignment follows a simple algorithm.</p> <p>When a request arrives at its hashed position, the system looks clockwise around the ring.</p> <p>The request is served by the nearest server encountered in the clockwise direction.</p> <p>Example</p> <p>Requests are mapped to S1, S2, S3, or S4 based on which server is nearest clockwise to their hashed location.</p>"},{"location":"systemdesign/consistenthashing/#load-distribution-and-efficiency","title":"Load Distribution and Efficiency","text":"<p>Theoretical Uniform Load</p> <p>The expectation is that the hashes (for both requests and servers) are uniformly random.</p> <p>Due to uniform randomness, the distance between server points on the ring is also expected to be uniform.</p> <p>Consequently, the load (the number of requests mapped between servers) is expected to be uniform.</p> <p>The expected load factor for each server remains \\(1/N\\) (where N is the number of servers), which was also true for simple hashing.</p> <p>Minimum Change When Scaling (Adding/Removing Servers)</p> <p>The critical advantage of the ring architecture is maintaining minimum change when the number of servers changes.</p> <p>If a fifth server (SS5) is added to the ring, it is mapped to a new point. This new server only takes load from the server immediately counter-clockwise to its position.</p> <p>Example</p> <p>If SS4 is added, it sits between two existing servers. Any requests that previously mapped clockwise to SS3, but now map clockwise to the new SS4, are reassigned. SS4 gains load, and SS3's load decreases.</p> <p>In this scenario, only S3 is affected; S1, S2, and S4 are not affected. The change in load for each existing server is \"much less\" than what occurred with simple hashing.</p> <p>If a server, like S1, crashes and is removed, its load is shifted to the next clockwise server (e.g., SS4). The load is absorbed only by the nearest successor server on the ring.</p> <p>The system is designed to achieve the minimum theoretical change when adding or removing servers.</p>"},{"location":"systemdesign/consistenthashing/#practical-challenge-skewed-distribution-and-virtual-servers","title":"Practical Challenge: Skewed Distribution and Virtual Servers","text":"<p>While consistent hashing promises minimum change, a practical problem arises when the number of servers is small (e.g., four servers).</p> <p>Even with the ring architecture, having few servers means the distribution of requests can become skewed, potentially resulting in half of the total load falling on a single server, which is \"terrible\". </p> <p>Theoretically, the load should be \\(1/N\\), but practically, it can be uneven. This happens because the server points might not be evenly spaced if there are not enough servers.</p> <p>The Solution: Virtual Servers (Replicas)</p> <p>To solve the load skew and guarantee uniform distribution, system design engineers use the concept of virtual servers.</p> <p>Virtual servers do not involve buying more expensive physical servers or virtual boxes. Instead, they involve using multiple hash functions (\\(K\\) hash functions).</p> <p>Each server ID is passed through \\(K\\) different hash functions (\\(H_1, H_2, \\dots H_K\\)). This results in \\(K\\) points being mapped onto the ring for a single physical server.</p> <p>Example</p> <p>If \\(K=3\\) and there are four physical servers, the ring will have \\(4 \\times 3 = 12\\) server points. By appropriately choosing the value of \\(K\\) (e.g., \\(\\log N\\) or \\(\\log M\\)), the likelihood of a skewed load on one server is dramatically reduced, making the distribution much more even.</p> <p>Behavior with Virtual Servers</p> <p>When a server is added, it adds \\(K\\) points to the ring, taking small amounts of load from multiple existing servers.</p> <p>When a server is removed, \\(K\\) points are removed. The load that those \\(K\\) points served is then reassigned, increasing the load \"expected uniformly\" across multiple neighboring servers.</p> <p>The use of virtual servers ensures the system is efficient and maintains the expected minimum change.</p>"},{"location":"systemdesign/consistenthashing/#applications-of-consistent-hashing","title":"Applications of Consistent Hashing","text":"<p>Consistent hashing is a fundamental concept used extensively in distributed systems and scaling.</p> <p>It provides flexibility and efficient load balancing.</p> <p>It is used by web caches.</p> <p>It is used by databases.</p>"},{"location":"systemdesign/consistenthashing/#analogy","title":"Analogy","text":"<p>Consistent hashing with virtual servers can be visualized as dividing a large circular pizza (the total load) equally among chefs (servers). If you only have four chefs, one might accidentally get a disproportionately large slice. By using virtual servers, you effectively slice each chef into several 'mini-chefs' spread around the edge of the pizza. If the pizza is cut based on these numerous 'mini-chef' markers (virtual points), the total share (load) collected by any one physical chef (server) is guaranteed to be nearly identical, ensuring true load balance even when a new chef is added or an old one leaves.</p>"},{"location":"systemdesign/loadbalancing/","title":"Load Balancing","text":"<p>A server is essentially a computer running a program or algorithm. A server is defined as something that serves requests.</p> <p>When a client (e.g., using a mobile phone) connects, they are technically sending a request to use the algorithm.</p> <p>The server receives the request, runs the algorithm (e.g., facial recognition), and sends back a response (e.g., an image).</p> <p>Initially, a single computer might handle requests happily. However, if the service becomes popular and receives thousands of requests, the original computer will be unable to handle the volume. When a system can no longer handle the load, the solution is to add new servers.</p>"},{"location":"systemdesign/loadbalancing/#concept-of-load-balancing","title":"Concept of Load Balancing","text":"<p>Once multiple servers (N servers) are operating, the system must decide where to send incoming requests.</p> <p>Load refers to the requests which the server needs to process. The server carries this load.</p> <p>The primary objective is to balance the load evenly on all N servers.</p> <p>Load balancing is the concept of taking N servers and trying to distribute the load evenly on all of them. The aim is to evenly distribute the weight (load) across all available servers.</p> <p>Load Balancing Algorithms:</p> <ol> <li>Round Robin</li> <li>Weighted Round Robin</li> <li>Least Connections</li> <li>Random</li> <li>Hashing</li> </ol>"},{"location":"systemdesign/loadbalancing/#the-initial-approach-simple-hashing-modulo-arithmetic","title":"The Initial Approach: Simple Hashing (Modulo Arithmetic)","text":"<p>A simple approach to load distribution involves using a hash function and the modulo operation based on the number of servers (N).</p> <ol> <li>Request ID: Each request comes with a request ID (r or I), which is expected to be uniformly random, generated by the client between zero and M-1.</li> <li>Hashing: The request ID is hashed (H(r)) to obtain a number (M1).</li> <li>Mapping to Server: This number (M1) is mapped to one of the N servers by calculating the remainder: M1 mod N. The remainder corresponds to the server index (e.g., S0, S1, S2, etc.).</li> <li>Uniform Distribution: Because both the request ID and the hash function are generally considered uniformly random, this approach results in a uniform load distribution.        If there are X total requests, each of the N servers is expected to have X/N load.        The load factor for each server is 1/N.</li> </ol> <p>Example</p> <p>using 4 servers (N=4):</p> <p>Request R1 = 10; H(10) = 3. \\(3 \\text{ mod } 4 = 3\\). R1 goes to server S3.</p> <p>Request R2 = 20; H(20) = 15. \\(15 \\text{ mod } 4 = 3\\). R2 goes to server S3.</p> <p>Request R3 = 35; H(35) = 12. \\(12 \\text{ mod } 4 = 0\\). R3 goes to server S0.</p>"},{"location":"systemdesign/loadbalancing/#the-limitation-adding-more-servers","title":"The Limitation: Adding More Servers","text":"<p>The simple hashing approach fails when the number of servers (N) changes, specifically when more servers are added (e.g., adding S4).</p> <p>When a fifth server is added, the modulus operation changes from mod 4 to mod 5. This change causes most request assignments to shift dramatically.</p> <p>Example</p> <p>R2 (Hash result 15) previously went to S3 (\\(15 \\text{ mod } 4 = 3\\)). Now, it must go to S0 (\\(15 \\text{ mod } 5 = 0\\)).</p> <p>R3 (Hash result 12) previously went to S0 (\\(12 \\text{ mod } 4 = 0\\)). Now, it must go to S2 (\\(12 \\text{ mod } 5 = 2\\)).</p> <p>When transitioning from 4 to 5 servers, the cost of the change\u2014meaning the total number of reassignments\u2014can be 100. </p> <p>In this scenario, the change involves the entire sort space (100% of the assignments change). This massive shift means that the requests currently being served are \"completely bamboozled\".</p>"},{"location":"systemdesign/loadbalancing/#cache-invalidation-and-the-need-for-minimal-change","title":"Cache Invalidation and The Need for Minimal Change","text":"<p>The massive shift caused by adding a server is problematic primarily because of how real-world applications utilize the request ID and caching.</p> <p>In practice, the request ID is rarely random; it typically encapsulates information about the user, such as the user ID.</p> <p>Because the hash function is constant, hashing a specific user ID (e.g., \"Gaurav\") will repeatedly yield the same result, sending that user to the same server.</p> <p>This server stickiness is utilized for efficiency: if a user is repeatedly sent to the same server, that server can store relevant information (like a user profile) in its local cache, avoiding constant fetching from a database.</p> <p>When the system changes (e.g., mod 4 to mod 5), all users are sent to new, different places. Consequently, the entire system changes, and almost all the useful cached information that was stored on the old servers is dumped or rendered useless.</p> <p>To preserve cache efficacy, the system must avoid a huge change in the range of numbers (or buckets) being served. A small or tiny change is what is needed.</p>"},{"location":"systemdesign/loadbalancing/#the-solution-consistent-hashing","title":"The Solution: Consistent Hashing","text":"<p>The standard way of hashing (modulo N) fails in scaling scenarios due to the catastrophic assignment changes.</p> <p>Consistent hashing is an advanced approach designed to address this failure.</p> <p>If a new server is added (e.g., requiring a 20% share of the load), consistent hashing aims to take only a small amount of load from each existing server. The sum of these small losses across the old servers must equal the 20% assigned to the new server, but the overall change must be minimum.</p>"},{"location":"systemdesign/loadbalancing/#analogy","title":"Analogy","text":"<p>To understand the difference between simple hashing and consistent hashing, imagine a library where books (requests) are sorted onto shelves (servers) based on the last digit of the book's ID number.</p> <p>If you have 10 shelves, a book ending in '3' goes to shelf 3. If you decide to add an 11th shelf, under simple hashing, every single book in the library must be re-shelved according to the new \"mod 11\" rule, essentially destroying the order and making all previous organizational memory (cache) useless. </p> <p>Consistent hashing, however, would only require a small, localized shift of books (requests) from existing shelves to fill the new shelf, leaving the vast majority of the established assignments untouched.</p>"},{"location":"systemdesign/messagingqueue/","title":"Messaging Queue","text":""},{"location":"systemdesign/messagingqueue/#introduction-to-asynchronous-processing-and-queues","title":"Introduction to Asynchronous Processing and Queues","text":"<p>Handling Orders in a Pizza Shop</p> <p>The shop continues taking new orders even while pizzas are being made. Clients do not receive the final product (the pizza) immediately. Clients are relieved from expecting an immediate response by being given a confirmation (e.g., \"Please sit down\" or \"Can you come back after sometime?\") that the order has been placed.</p> <p>The List/Queue</p> <p>The shop maintains a list or queue that tracks orders (e.g., order no. 1, order no. 2, etc.).As new orders come in, they are added to this queue.When a pizza is done, it is removed from this queue.The client pays and is \"entirely relieved\".</p> <p>Asynchronous Nature</p> <p>The entire process is asynchronous, meaning the client did not have to wait for the final response (the pizza or the payment transaction).</p> <p>Client Benefit: The client is able to do other tasks during the wait time (e.g., checking their phone), allowing the client to be happier and distribute their resources elsewhere.</p> <p>Server/Maker Benefit: The pizza maker (or server) can order tasks according to priority. Tasks that are very easy (like filling a coke can) or those that need to be made immediately can be prioritized.</p> <p>This manipulation of the queue based on priority allows clients to spend time more judiciously.</p>"},{"location":"systemdesign/messagingqueue/#challenges-of-scaling-and-persistence","title":"Challenges of Scaling and Persistence","text":"<p>The architecture becomes complex when the system scales, such as having multiple pizza shop outlets (e.g., Pizza Shop No. 1, No. 2, No. 3, similar to Dominos).</p> <p>Handling Shop Failure (Power Outage)</p> <p>If a shop goes down (e.g., power outage).Takeaway orders might be dumped. Delivery orders can potentially be sent to the other operational shops to complete the work. If Pizza Shop No. 3 crashes, its clients need to be connected to the remaining shops, and their orders must be rerouted.</p> <p>Maintaining the list of orders solely in memory won't work because if the shop loses electricity, the computer shuts down, and the data is lost.</p> <p>Therefore, the system requires persistence in its data, which means the list must be stored in a database.</p>"},{"location":"systemdesign/messagingqueue/#the-complicated-architecture-servers-database-and-failure-management","title":"The Complicated Architecture: Servers, Database, and Failure Management","text":"<p>A more complex system includes a set of servers (e.g., 1 to 4) and a database storing the list of all orders (Order ID, contents, and completion status).</p> <p>Rerouting Orders Upon Server Crash</p> <p>If a server (e.g., S3) handling specific orders (e.g., 9 and 11) crashes, those orders need to be rerouted.</p> <p>Initial Solution Idea (Too Complicated): One approach is to have the database note which server ID is handling the order, and then check the database when a server fails.</p> <p>A better method involves a notifier checking for a heartbeat from each server (e.g., every 15 seconds).</p> <p>If a server does not respond, the notifier assumes it is dead.The notifier then queries the database to find all orders that are not done and distributes them to the remaining servers.</p> <p>The Duplication Problem</p> <p>Rerouting failed orders introduces the risk of duplication.</p> <p>If an order (e.g., Order No. 3) is picked up by the database query for reassignment (because its original server crashed), but Server No. 2 is already processing Order No. 3, the duplicated assignment means both Server 1 and Server 2 will end up making the pizza.</p> <p>This results in a \"big loss and lots of confusion\".</p> <p>Solution: Load Balancing and Consistent Hashing</p> <p>To prevent duplication and manage reassignment efficiently, the system needs load balancing.</p> <p>Load Balancing Principles: Load balancing is defined as sending the right amount of load to each server, but crucially, its principles ensure no duplicate requests are sent to the same server.</p> <p>Consistent hashing (a technique described in the conversation history) can be used to eliminate duplicates and balance the load.The principle ensures that Server S1 handles one set of \"buckets\" and S2 handles another.</p> <p>When a server crashes, the remaining servers (e.g., S1 and S2) do not lose their current buckets; they only receive new buckets that were previously handled by the crashed server.</p> <p>Example</p> <p>if Order 3 belonged to S2, it continues to belong to S2, preventing duplication when the notifier reroutes failed orders.</p>"},{"location":"systemdesign/messagingqueue/#message-queues-as-the-encapsulation-of-complexity","title":"Message Queues as the Encapsulation of Complexity","text":"<p>The need for assignment, notification, load balancing, a heartbeat mechanism, and persistence leads directly to the concept of a message queue.</p> <p>Definition</p> <p>The message queue (or task queue in this context) encapsulates all this complexity into one thing.</p> <p>Functionality</p> <p>A task queue:</p> <ol> <li> <p>Takes tasks (orders).</p> </li> <li> <p>Persists them (stores them permanently).</p> </li> <li> <p>Assigns them to the correct server.</p> </li> <li> <p>Waits for the server to complete the task.</p> </li> <li> <p>If a server takes too long to acknowledge completion, the queue assumes the server is dead and reassigns the task to the next server.</p> </li> </ol> <p>Importance</p> <p>Task queues are an important concept in system design for handling work and encapsulating complexity.</p> <p>Example</p> <p>RabbitMQ, Java Messaging Service (JMS), and zeroMQ (a library that allows one to write a messaging queue easily). Amazon also offers messaging queues.</p>"},{"location":"systemdesign/overview/","title":"Overview","text":""},{"location":"systemdesign/overview/#system-design-using-the-restaurant-analogy","title":"System Design Using the Restaurant Analogy","text":"<p>When opening a new restaurant the initial challenge arises when one chef cannot handle all the orders from new customers.</p>"},{"location":"systemdesign/overview/#scaling-the-resource-vertical-scaling-and-optimization","title":"Scaling the Resource: Vertical Scaling and Optimization","text":"<p>The goal is to optimize processes and increase throughput using the same resource (the chef/computer).</p> <p>Vertical Scaling: Thinking like a manager, the first step is to ask the chef (analogous to a computer) to work harder by being paid more, leading to more output. This is termed vertical scaling in technical terms.</p> <p>Process Optimization: Efficiency can be improved by doing things beforehand. For example, the pizza paste does not need to be made when an order comes in; it can be pre-made.</p> <p>Non-Peak Hours: Pre-preparation should happen during non-peak hours. Doing this around 4:00 AM is good because there won't surely be any pizza orders, preventing the chef from being busy making pizza bases when a regular order comes in.</p>"},{"location":"systemdesign/overview/#ensuring-resilience-avoiding-single-points-of-failure","title":"Ensuring Resilience: Avoiding Single Points of Failure","text":"<p>The initial single-chef setup is not resilient.</p> <p>Single Point of Failure (SPOF): If the chef calls sick, the business faces trouble because this person is a single point of failure.</p> <p>Backups: To address this, a backup chef should be hired in case the primary chef does not come in.</p> <p>Master-Slave Architecture: Keeping backups and avoiding SPOFs maps, for computers, to a master-slave architecture. Hiring a backup significantly reduces the chance of losing business.</p>"},{"location":"systemdesign/overview/#expanding-capacity-horizontal-scaling","title":"Expanding Capacity: Horizontal Scaling","text":"<p>If the business continues to grow, more resources must be added.</p> <p>Hiring More Resources: The backup chef should be made full-time, and more chefs should be hired (e.g., 10 chefs plus a few in backup).</p> <p>Horizontal Scaling: This process maps to horizontal scaling, which involves buying more machines of similar types to get more work done.</p>"},{"location":"systemdesign/overview/#specialization-and-microservices-architecture","title":"Specialization and Microservices Architecture","text":"<p>When multiple resources are available, efficiency is achieved through specialization.</p> <p>Inefficient Routing: Simply assigning orders randomly (e.g., sending garlic bread to a chef specializing in pizza) is not the most efficient way to use employees.</p> <p>Routing by Strength: The optimal approach is to route orders based on strengths. If chefs 1 and 3 are pizza experts and chef 2 specializes in garlic bread, all garlic bread orders go to chef 2, and pizza orders go to chefs 1 and 3.</p> <p>Benefits of Specialization: This routing simplifies the system. For example, changing the garlic bread recipe only requires notifying chef 2, and chef 2 is the sole person to ask for the status of garlic bread orders.</p> <p>Microservice Architecture: Creating specialist teams (e.g., scaling the pizza team differently than the garlic bread team) and dividing responsibilities is known as microservice architecture. In this architecture, responsibilities are well-defined and stay within the business use case. This structure makes the business highly scalable as specialists can be scaled easily.</p>"},{"location":"systemdesign/overview/#geographical-distribution-and-fault-tolerance","title":"Geographical Distribution and Fault Tolerance","text":"<p>Even a successful, scalable pizza shop (a single location) is vulnerable to local failure (e.g., electricity outage or loss of license).</p> <p>Distribution: To achieve greater fault tolerance, one must distribute the system; not putting all resources in one basket or even one shop.</p> <p>Distributed System: The solution is to buy a separate shop in a different place. This is a major step that introduces complexity, particularly concerning communication and routing requests between the shops. This defines a distributed system.</p> <p>Quicker Response Times: A clear advantage of distribution is that orders local to a specific shop's range can be served by that shop, leading to quicker response times. Large distributed systems like Facebook use local servers worldwide for this reason. Distribution makes the system more fault tolerant.</p>"},{"location":"systemdesign/overview/#intelligent-routing-load-balancing","title":"Intelligent Routing: Load Balancing","text":"<p>When multiple shops exist, a decision must be made about where to send a customer's request.</p> <p>Central Authority: Customers should not bear the responsibility of routing requests; a central place or authority is needed to route them.</p> <p>Routing Parameter: The routing should not be random. The primary parameter is the total time it takes for the customer to get the pizza.</p> <p>Intelligent Decisions: The central authority needs real-time updates to make intelligent business decisions (leading to more money). For example, if Pizza Shop Two takes 1 hour 5 minutes total and Shop One takes 1 hour 15 minutes, the request should go to Shop Two.</p> <p>Load Balancer: The mechanism that routes requests in a smart way is called a load balancer.</p>"},{"location":"systemdesign/overview/#flexibility-and-extensibility-decoupling","title":"Flexibility and Extensibility: Decoupling","text":"<p>To make the system flexible to change, responsibilities must be separated.</p> <p>Separation of Concerns: The delivery agent and the pizza shop have little in common. The delivery agent only cares about quickly delivering goods, and the shop doesn't care if the goods are picked up by an agent or the customer.</p> <p>Decoupling: This requires separating out concerns, such as having different managers for the pizza shop and the delivery agents. This is called decoupling the system, which allows separate systems to be handled more efficiently.</p> <p>Metrics and Logging: To track efficiency and diagnose issues (like a faulty oven or bike), it is crucial to log everything (what happened and when). Events must be condensed to find sense, which results in metrics.</p> <p>Extensibility: Decoupling ensures the system remains extensible. For example, the delivery agent service doesn't need to know if it is delivering a pizza or a burger. This ability to decouple allows businesses (like Amazon) to scale out their offerings.</p>"},{"location":"systemdesign/overview/#high-level-vs-low-level-system-design","title":"High Level vs. Low Level System Design","text":"<p>The process of finding technical solutions by mapping them from a business scenario is integral to system design.</p> <p>High Level Design (HLD): This is the high-level solution developed by scaling the restaurant. HLD focuses on topics like deploying on servers and determining how two systems interact with each other.</p> <p>Low Level Design (LLD): The counterpart to HLD, LLD focuses more on how to actually code the stuff. This includes making classes, objects, functions, and signatures. Knowledge of LLD is crucial for senior engineers to write efficient and clean code.</p>"},{"location":"systemdesign/scaling/","title":"Horizonatal vs Vertical Scaling","text":""},{"location":"systemdesign/scaling/#initial-system-setup-and-deployment","title":"Initial System Setup and Deployment","text":"<p>Core Functionality: System design begins with an algorithm (code) running on a computer. This code functions like a normal function, taking input and providing an output.</p> <p>Monetization and Access: If the code is useful, people may pay to use it, but the developer cannot give their physical computer to everyone.</p> <p>Exposing the Code: The code is exposed using a protocol that runs on the internet, typically through an API (Application Programmable Interface).</p> <p>Communication Flow:        The user sends a request to the system.        The code runs, giving an output that is returned as a response.</p> <p>Local Infrastructure Needs: Setting up this computer might require configuring endpoints and connecting to a database (even if the database resides within the desktop itself).</p> <p>Reliability and Cloud Hosting: To ensure the service does not go down (e.g., due to power loss or someone pulling the plug), the service should be hosted on the cloud.</p>"},{"location":"systemdesign/scaling/#understanding-the-cloud","title":"Understanding the Cloud","text":"<p>The cloud is fundamentally a set of computers that a provider (like Amazon Web Services - AWS, the most popular example) offers for money.</p> <p>Cloud providers give computation power\u2014essentially a desktop they have somewhere else that can run the algorithm.</p> <p>Developers can store and run their algorithm often by using a remote login into that computer.</p> <p>Using the cloud allows the developer to focus on business requirements because the cloud solution providers handle the configuration, settings, and reliability to a large extent.</p>"},{"location":"systemdesign/scaling/#the-need-for-scalability","title":"The Need for Scalability","text":"<p>When many users begin using the algorithm, the single machine running the code may no longer be able to handle all the incoming connections.</p> <p>Scalability is the ability to handle more requests. This is achieved by increasing resources, often summarized as \"throwing more money at the problem\".</p> <p>Two Solutions for Scaling:</p> <ol> <li>Buy a bigger machine.</li> <li>Buy more machines.</li> </ol>"},{"location":"systemdesign/scaling/#mechanisms-of-scaling","title":"Mechanisms of Scaling","text":"<p>The two scaling solutions correspond to two key mechanisms:</p> <p>Vertical Scaling (Buying Bigger Machines)</p> <p>The computer is made larger, allowing it to process requests faster. Communication between components uses interprocess communication (IPC), which is quite fast.    Because all data resides on one system, the data is consistent.</p> <p>Horizontal Scaling (Buying More Machines)</p> <p>Requests are distributed randomly among multiple machines. To process more requests overall because there are more resources available. The system is resilient; if one machine fails, requests can be redirected to the other machines.This scales well; the number of servers thrown at the problem scales almost linearly with the number of users added, overcoming hardware limitations inherent in vertical scaling.</p>"},{"location":"systemdesign/scaling/#detailed-comparison-of-vertical-vs-horizontal-scaling","title":"Detailed Comparison of Vertical vs. Horizontal Scaling","text":"Feature Horizontal Scaling (More Machines) Vertical Scaling (Bigger Machine) Load Balancing Required (to distribute requests among machines). Not required (single machine). Failure/Resilience Resilient (failure of one machine allows redirection). Single point of failure (SPOF). Communication Slow: Uses network calls/IO, specifically Remote Procedure Calls (RPC) between services. Fast: Uses interprocess communication (IPC). Data Consistency Real Issue: Difficult to maintain complex data integrity (e.g., atomic transactions may require impractical locking of all servers/databases). Requires some form of loose transactional guarantee. Consistent: All data resides on a single system. Hardware Limits Scales well: The amount of servers can be increased almost linearly. Limited: Subject to hardware limitations; the computer cannot be made infinitely big."},{"location":"systemdesign/scaling/#real-world-approach-and-system-design-trade-offs","title":"Real-World Approach and System Design Trade-offs","text":"<p>In the real world, both vertical and horizontal scaling techniques are used. The hybrid solution is essentially horizontal scaling, but each individual machine used is as large a box as is financially and technically feasible.</p> <p>From Vertical Scaling: Fast interprocess communication and data consistency (e.g., consistent cache, no dirty reads/writes).</p> <p>From Horizontal Scaling: Resilience (if one server crashes, others take over) and the ability to scale well beyond hardware limits.</p> <p>Initially, a system may utilize vertical scaling as much as possible; later, as the user base grows and trusts the service, scaling should shift toward horizontal scaling.</p> <p>System design is the process of making trade-offs to build a system that meets requirements, focusing primarily on scalability, resilience, and consistency.</p>"},{"location":"systemdesign/scaling/#analogy","title":"Analogy","text":"<p>Thinking about scaling a computer system is like managing a pizza kitchen. Vertical scaling is buying a single, massive, state-of-the-art oven\u2014it cooks pizzas incredibly fast and keeps all your ingredients in one place (consistent data), but if the oven breaks, you can't make any pizza (single point of failure). </p> <p>Horizontal scaling is buying many smaller, cheaper ovens\u2014if one oven breaks, the others keep cooking (resilience), and you can add new ovens indefinitely as demand grows, but coordinating the ingredients and timing across all those separate ovens (data consistency and network communication) becomes much harder. </p> <p>The best solution is often a hybrid: buying several very large, high-quality ovens (horizontal scaling of highly vertically scaled nodes).</p>"},{"location":"tooling/airflow_install_onpremise/","title":"Apache Airflow Setup (On-Premise)","text":"<p>We will be using MySQL as backend database, RabbitMQ as message broker, Celery as task queuing system and Flower UI for monitoring celery cluster.</p> <ul> <li>Operating System: RHEL/CENTOS/UBUNTU</li> <li>Python Version: 3.8  </li> <li>Backend Database: MySQL  </li> <li>Executor: Celery as task queing system</li> <li>Celery Backend (Broker): RabbitMQ as message broker</li> <li>Flower UI: To monitor celery cluster.</li> </ul> <p>Note: All commands are executed from the home directory (<code>~/</code>).</p> <p>More details and step-by-step instructions will follow in the next sections. Make sure you install the following packages before starting installation</p> <p>Required Python Packages</p> <pre><code>yum install libmysqlclient-dev python3 python3-dev build-essential libssl-dev libffi-dev libxml2-dev libxslt1-dev zlib1g-dev freetds-bin krb5-user ldap-utils libsasl2-2 libsasl2-modules libssl1.1 locales lsb-release sasl2-bin sqlite3 unixodbc\n</code></pre> <p>Database Setup</p> <pre><code>yum -y install @mysql\n\n# Start MySQL service and create symlink\nsystemctl status mysqld\nsystemctl start mysqld\nsystemctl enable --now mysqld\nsystemctl status mysqld\n\n# Change root password and configure MySQL\nmysql_secure_installation  # {password:-root}\n\n# Create airflow user and grant privileges\nmysql -u\"root\" -p\"root\"\n\n# Inside MySQL prompt\nCREATE DATABASE airflow;\nCREATE USER 'airflowuser'@'%' IDENTIFIED BY 'airflowuser';\nGRANT ALL PRIVILEGES ON airflow.* TO 'airflowuser'@'%';\nCREATE USER 'airflowuser'@'localhost' IDENTIFIED BY 'airflowuser';\nGRANT ALL PRIVILEGES ON airflow.* TO 'airflowuser'@'localhost';\nFLUSH PRIVILEGES;\n</code></pre> <p>Rabbitmq Setup</p> <pre><code># Extract the RabbitMQ setup file\nDownload the setup file before installing rabbitmq install erlang package first\n\n# Download the rabbitmq.conf file\nwget https://github.com/manish-chet/BigDataSetupfiles/tree/main/airflow\ncopy the file in rabbitmq/etc/conf directory\nedit the rabbitmq.conf with IP and details\n\n#Set environment variables in rc\nexport RABBITMQ_LOG_BASE=/data/rabbitmq/rabbitmq/logs\nexport PATH=$PATH:/data/rabbitmq/rabbitmq/rabbitmq_server-3.13.7/sbin\n\n#Enable rabbitmq plugins\nrabbitmq-plugins enable rabbitmq_management  \n\n#Shutdown rabbitmq\nsbin/rabbitmqctl shutdown\n\n#Rabbitmq status\nrabbitmqctl status\n\n#Start rabbitmq in detached mdoe\nrabbitmq-server -detached\n\n#Create user\nrabbitmqctl add_user airflow airflow\nrabbitmqctl set_user_tags airflow administrator\nrabbitmqctl set_permissions -p / airflow \".*\" \".*\" \".*\"\nrabbitmqctl eval 'application:set_env(rabbit, consumer_timeout, undefined).'\n</code></pre> <p>Airflow configuration</p> <pre><code># Create virtualenv \npip install virtualenv\npython3.8 -m pip install --upgrade pip\nvirtualenv -p python3.8 airflow_env\nsource airflow_env/bin/activate\n\n# Install necessary packages\npip install \"apache-airflow==2.10.2\" --constraint constraints.txt\npip install 'apache-airflow[mysql]'\npip install 'apache-airflow[celery]'\npip install 'apache-airflow[rabbitmq]'\npip install 'apache-airflow[crypto]'\npip install 'apache-airflow[password]'\n\n# Check airflow version\nairflow version\n\n# Download the airflow.cfg and edit the hostname and IP details\nwget https://github.com/manish-chet/BigDataSetupfiles/tree/main/airflow\n\n\n# Create users\nairflow create_user -r Admin -u airflow -e your_email@domain.com -f Airflow -l Admin -p password\nairflow users create -r Admin -u manishkumar2.c -f manishkumar -l chetpalli -e manishkumar2.c@email.com -p manish123\n\n#Initialize DB\nairflow db migrate\n\n#Initialize Webserver\nairflow webserver -D\n\n#Initialize Scheduler\nairflow scheduler -D\n\n#Initialize Flower\nairflow celery worker -D\n\n#Initialize Celery\nairflow celery flower -D\n\n#Initialize Trigger\nairflow triggerer -D\n</code></pre> <p>Worker Node Addition</p> <pre><code># Create virtualenv \npip install virtualenv\npython3.8 -m pip install --upgrade pip\nvirtualenv -p python3.8 airflow_env\nsource airflow_env/bin/activate\n\n# Install necessary packages\npip install \"apache-airflow==2.10.2\" --constraint constraints.txt\npip install 'apache-airflow[mysql]'\npip install 'apache-airflow[celery]'\npip install 'apache-airflow[rabbitmq]'\npip install 'apache-airflow[crypto]'\npip install 'apache-airflow[password]'\n\n# Check airflow version\nairflow version\n\n# Download the airflow.cfg and edit the hostname and IP details\nwget https://github.com/manish-chet/BigDataSetupfiles/tree/main/airflow\n\n\nAfter this you need to go to master node and run below MySQL command so that user from worker node can connect to the database on master node.\n# replace hostname here with your remote worker ip address\nsudo mysql -e \"CREATE USER 'airflow'@'hostname' IDENTIFIED BY 'password'; GRANT ALL PRIVILEGES ON airflowdb.* TO 'airflow'@'hostname';\"\n\n# Initialize the airflow database on worker node \nairflow initdb\n\n# Start the worker\nairflow celery worker -D\n\nYou should be able to see the worker node coming up on Flower interface at YOUR_MASTER_IP_ADDRESS:5555\n</code></pre> <p>Airflow DB Cleanup DAG</p> <pre><code>wget https://github.com/manish-chet/BigDataSetupfiles/tree/main/airflow\n</code></pre> <p>Airflow Log Cleanup DAG</p> <pre><code>wget https://github.com/manish-chet/BigDataSetupfiles/tree/main/airflow\n</code></pre>"},{"location":"tooling/igniteinstall/","title":"Ignite Multi Node Setup","text":""},{"location":"tooling/igniteinstall/#prerequisites","title":"Prerequisites","text":"<ul> <li>3 VMs (Ensure pre-requisites like SELinux disabled, firewall off, THP disabled, etc.)</li> <li>Java 11 or higher</li> <li>Enough RAM for data</li> <li>Network connectivity</li> </ul>"},{"location":"tooling/igniteinstall/#download-setup","title":"Download setup","text":"<p>Download tar file from apache ignite <pre><code>Download ignite 2 bin file from https://ignite.apache.org/download.cgi\nunzip apache-ignite-2.8.1-bin\n</code></pre></p>"},{"location":"tooling/igniteinstall/#configuration","title":"Configuration","text":"<p>Set ENV variables <pre><code>export IGNITE_JMX_PORT=9999\n#ignite global index inline size in bytes\n#default is 10\nexport IGNITE_MAX_INDEX_PAYLOAD_SIZE=100\n\n#Set the following in bin/ignite.sh\nJVM_OPTS=\"$JVM_OPTS -Xms8g -Xmx8g -server -XX:MaxMetaspaceSize=256m -XX:+UseG1GC -XX:+DisableExplicitGC -XX:+AlwaysPreTouch -XX:+ScavengeBeforeFullGC -XX:MaxDirectMemorySize=2048m\"\n</code></pre></p> <p>Setup igniteconfig.xml <pre><code>Copy the following xml in ignitework directory\nwget https://github.com/manish-chet/BigDataSetupfiles/tree/main/ignite/config\nChange the hosts on specific server\n</code></pre></p>"},{"location":"tooling/igniteinstall/#start-the-nodes","title":"Start the nodes","text":"<p>start the node <pre><code>nohup ignite.sh config/igniteconfig.xml &gt; out.log 2&gt;&amp;1  &amp;\n</code></pre></p> <p>Check the node using baseline <pre><code>[ignite@hostname ~]$ control.sh --baseline\nWARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.apache.ignite.internal.util.GridUnsafe$2 (file:/home/ignite/apache-ignite-2.8.1-bin/libs/ignite-core-2.8.1.jar) to field java.nio.Buffer.address\nWARNING: Please consider reporting this to the maintainers of org.apache.ignite.internal.util.GridUnsafe$2\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\nControl utility [ver. 2.8.1#20200521-sha1:86422096]\n2020 Copyright(C) Apache Software Foundation\nUser: ignite\nTime: 2025-05-29T19:11:31.029058\nCommand [BASELINE] started\nArguments: --baseline\n--------------------------------------------------------------------------------\nCluster state: active\nCurrent topology version: 3\nBaseline auto adjustment enabled: softTimeout=0\nBaseline auto-adjust are not scheduled\nCurrent topology version: 3 (Coordinator: ConsistentId=ign01, Order=1)\nBaseline nodes:\nConsistentId=ign01, State=ONLINE, Order=1\nConsistentId=ign02, State=ONLINE, Order=3\nConsistentId=ign03, State=ONLINE, Order=2\n--------------------------------------------------------------------------------\nNumber of baseline nodes: 3\nOther nodes not found.\nCommand [BASELINE] finished with code: 0\nControl utility has completed execution at: 2025-05-29T19:11:31.295162\nExecution time: 266 ms\n</code></pre></p>"},{"location":"tooling/kafka_install_zookeeper/","title":"Kafka Installation with ZooKeeper","text":""},{"location":"tooling/kafka_install_zookeeper/#prerequisites","title":"Prerequisites","text":"<ul> <li>3 VMs (Ensure pre-requisites like SELinux disabled, firewall off, THP disabled, etc.)</li> <li>Java 8 or higher</li> <li>Sufficient disk space</li> <li>Network connectivity</li> </ul>"},{"location":"tooling/kafka_install_zookeeper/#vm-settings","title":"VM settings","text":"<p>Kernel &amp; OS tuning</p> <pre><code>vm.swappiness = 1\nvm.dirty_background_ratio = 10 # Consider 5 for certain workloads\nvm.dirty_ratio = 20\n</code></pre> <p>Networking Parameters </p> <pre><code>net.core.wmem_default = 131072\nnet.core.rmem_default = 131072\nnet.core.wmem_max  = 2097152\nnet.core.rmem_max  = 2097152\nnet.ipv4.tcp_window_scaling = 1\nnet.ipv4.tcp_wmem = 4096 65536 2048000\nnet.ipv4.tcp_rmem = 4096 65536 2048000\nnet.ipv4.tcp_max_syn_backlog = 4096\nnet.core.netdev_max_backlog = 5000\n</code></pre> <p>GC Tuning</p> <pre><code>(for 64GB system with 5GB heap)\n-XX:MaxGCPauseMillis=20\n-XX:InitiatingHeapOccupancyPercent=35\n</code></pre>"},{"location":"tooling/kafka_install_zookeeper/#certs-creation","title":"Certs Creation","text":"<p>Creating certificates for SASL_SSL</p> <pre><code>#!/bin/\n# Generates several self-signed keys &lt;name&gt;.cer, &lt;name&gt;.jks, and &lt;name&gt;.p12.\n# Truststore is set with name truststore.jks and set password of password12345\n# Usage: createKey.sh &lt;user&gt; &lt;password&gt;\n#createKey.sh somebody password123\n# -ext \"SAN=DNS:\"\nexport NAME=$1\nexport IP1=$2\nexport PASSWORD=7ecETGlHjzs\nexport STORE_PASSWORD=7ecETGlHjzs\necho \"Creating key for $NAME using password $PASSWORD\"\nkeytool -genkey -alias $NAME -keyalg RSA -keysize 4096 -dname \"CN=$NAME,OU=RRA,O=ABC,L=ABC,ST=ABC,C=IN\" -ext \"SAN=DNS:$NAME,IP:$IP1\" -keypass $PASSWORD -keystore $NAME.jks -storepass $PASSWORD -validity 7200\nkeytool -export -keystore $NAME.jks -storepass $PASSWORD -alias $NAME -file $NAME.cer\nkeytool -import -trustcacerts -file $NAME.cer -alias $NAME -keystore truststore.jks -storepass $STORE_PASSWORD -noprompt\necho \"Done creating key for $NAME\"\nkeytool -list -keystore truststore.jks -storepass $STORE_PASSWORD -noprompt\n\n-------------JKStoPEM--------------\n/opt/jdk1.8.0_151/bin/keytool -exportcert -alias hostname1.com -keystore truststore.jks -storepass 7ecETGlHjzs -file hostname1.crt\n/opt/jdk1.8.0_151/bin/keytool -exportcert -alias hostname2.com -keystore truststore.jks -storepass 7ecETGlHjzs -file hostname2.crt\n/opt/jdk1.8.0_151/bin/keytool -exportcert -alias hostname3.com -keystore truststore.jks -storepass 7ecETGlHjzs -file hostname3.crt\nopenssl x509 -inform der -in hostname1.crt -out hostname1.pem\nopenssl x509 -inform der -in hostname2.crt -out hostname2.pem\nopenssl x509 -inform der -in hostname3.crt -out hostname3.pem\ncat *.pem &gt; truststore_combined.pem\n\nTo execute - use  cert.sh hostname\n</code></pre>"},{"location":"tooling/kafka_install_zookeeper/#zookeeper-installation","title":"Zookeeper Installation","text":"<p>Download tar file from apache zookeeper</p> <pre><code>wget https://downloads.apache.org/zookeeper/zookeeper-3.8.1/apache-zookeeper-3.8.1-bin.tar.gz\ntar -xzf apache-zookeeper-3.8.1-bin.tar.gz\ncd apache-zookeeper-3.8.1-bin/conf/\n</code></pre> <p>Configure zookeeper properties</p> <pre><code>tickTime=1000\ninitLimit=10\nsyncLimit=5\ndataDir=/home/testing/apache-zookeeper-3.8.1-bin/zkdata\nmaxClientCnxns=120\nmaxCnxns=120\nssl.client.enable=true\n#portUnification=true\n#client.portUnification=true\n#multiAddress.reachabilityCheckEnabled=true\n#quorumListenOnAllIPs=true\n#4lw.commands.whitelist=*\nadmin.enableServer=false\nauthProvider.sasl=org.apache.zookeeper.server.auth.SASLAuthenticationProvider\nzookeeper.superUser=superadmin\nsecureClientPort=12182\nclientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty\nserverCnxnFactory=org.apache.zookeeper.server.NettyServerCnxnFactory \nauthProvider.x509=org.apache.zookeeper.server.auth.X509AuthenticationProvider\nssl.keyStore.location=/home/testing/certs/localhost1.jks\nssl.keyStore.password=7ecETGlHjzs\nssl.trustStore.location=/home/testing/certs/truststore.jks\nssl.trustStore.password=7ecETGlHjzs\nsslQuorum=true\nssl.quorum.keyStore.location=/home/testing/certs/localhost1.jks\nssl.quorum.keyStore.password=7ecETGlHjzs\nssl.quorum.trustStore.location=/home/testing/certs/truststore.jks\nssl.quorum.trustStore.password=7ecETGlHjzs\nsessionRequireClientSASLAuth=true\n#jute.maxbuffer=50000000\nDigestAuthenticationProvider.digestAlg=SHA3-512\nsecureClientPortAddress=localhost1\nserver.1=localhost1:4888:5888\nserver.2=localhost2:4888:5888\nserver.3=localhost3:4888:5888\n</code></pre> <p>create jaas conf file for authentication</p> <pre><code>Server{\norg.apache.zookeeper.server.auth.DigestLoginModule required\nuser_superadmin=\"SuperSecret123\"\nuser_bob=\"bobsecret\"\nuser_kafka=\"kafkasecret\";\n};\nClient{\norg.apache.zookeeper.server.auth.DigestLoginModule required\nusername=\"bob\"\npassword=\"bobsecret\";\n};\n</code></pre> <p>Configure the Java Env variables</p> <pre><code>export ZOO_LOG_DIR=/home/testing/apache-zookeeper-3.8.1-bin/zklogs\n\nexport ZK_SERVER_HEAP=1024\n\nexport SERVER_JVM_FLAGS=\"$SERVER_JVMFLAGS -Dzookeeper.db.autocreate=false -Djava.security.auth.login.config=/home/testing/apache-zookeeper-3.8.1-bin/conf/jaas.conf\"\n\n#export ZOO_DATADIR_AUTOCREATE_DISABLE=1\n\nexport CLIENT_JVMFLAGS=\"$CLIENT_JVMFLAGS -Dzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty -Dzookeeper.ssl.trustStore.location=/home/testing/certs/truststore.jks -Dzookeeper.ssl.trustStore.password=7ecETGlHjzs -Dzookeeper.ssl.keyStore.location=/home/testing/certs/localhost1.jks -Dzookeeper.ssl.keyStore.password=7ecETGlHjzs -Dzookeeper.client.secure=true -Djava.security.auth.login.config=/home/testing/apache-zookeeper-3.8.1-bin/conf/jaas.conf\"\n\nexport JVMFLAGS=\"-Djava.security.auth.login.config=/home/testing/apache-zookeeper-3.8.1-bin/conf/jaas.conf\"\n</code></pre> <p>Create Id for each zk node</p> <pre><code>echo 1 &gt; /data/kafka/zookeeper/data/myid\n# Change the value for each node (e.g., 1, 2, 3)\n</code></pre> <p>Start zookeper server one each node</p> <pre><code>bin/zkServer.sh start\n</code></pre> <p>Create ZK TLS properties file</p> <pre><code>zookeeper.ssl.client.enable=true\nzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty\nzookeeper.ssl.keystore.location=/root/certs/localhost.jks\nzookeeper.ssl.keystore.password=7ecETGlHjzs\nzookeeper.ssl.truststore.location=/root/certs/truststore.jks\nzookeeper.ssl.truststore.password=7ecETGlHjzs\n</code></pre> <p>Login to cli and verify the status </p> <pre><code>bin/zkCli.sh -server hostname1:12182\n</code></pre>"},{"location":"tooling/kafka_install_zookeeper/#kafka-installation","title":"Kafka Installation","text":"<p>Download tar file from apache kafka</p> <pre><code>wget https://downloads.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz\ntar -xzf kafka_2.13-3.6.1.tgz\ncd kafka_2.13-3.6.1\n</code></pre> <p>Configure server.properties</p> <pre><code>broker.id=1\nlisteners=SASL_SSL://hostname:6667\nlistener.security.protocol.map=SASL_SSL:SASL_SSL\nadvertised.listeners=SASL_SSL://hostname:6667\nauthorizer.class.name=kafka.security.authorizer.AclAuthorizer\nsasl.enabled.mechanisms=SCRAM-SHA-512\nsasl.mechanism.inter.broker.protocol=SCRAM-SHA-512\nsecurity.inter.broker.protocol=SASL_SSL\nssl.client.auth=required\n#ssl.endpoint.identification.algorithm=\nssl.keystore.location=/root/certs/hostname.jks\nssl.keystore.password=7ecETGlHjzs\nssl.truststore.location=/root/certs/truststore.jks\nssl.truststore.password=7ecETGlHjzs\nsuper.users=User:admin\nzookeeper.connect=hostname:12182,hostname2:12182,hostnamedb:12182\nzookeeper.ssl.client.enable=true\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=18000 \nzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty\nzookeeper.ssl.keystore.location=/root/certs/hostname.jks\nzookeeper.ssl.keystore.password=7ecETGlHjzs\nzookeeper.ssl.truststore.location=/root/certs/truststore.jks\nzookeeper.ssl.truststore.password=7ecETGlHjzs\n</code></pre> <p>Create kafka_jaas.conf file for authentication</p> <pre><code>Client{\norg.apache.zookeeper.server.auth.DigestLoginModule required\nusername=\"bob\"\npassword=\"bobsecret\";\n};\nKafkaServer{\norg.apache.kafka.common.security.scram.ScramLoginModule required\nusername=\"admin\"\npassword=\"password\";\n};\nKafkaClient{\norg.apache.kafka.common.security.scram.ScramLoginModule required\nusername=\"admin\"\npassword=\"password\";\n};\n</code></pre> <p>Configure KAFKA_OPTS and KAFKA-ENV properties</p> <pre><code>export KAFKA_HOME=/root/kafka_2.13-3.4.0\n\nexport KAFKA_OPTS=\"-Djava.security.auth.login.config=/root/kafka_2.13-3.4.0/config/kafka_jaas.conf -Dzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty  -Dzookeeper.client.secure=true  -Dzookeeper.ssl.truststore.location=/root/certs/truststore.jks -Dzookeeper.ssl.truststore.password=7ecETGlHjzs\"\n\nexport KAFKA_HEAP_OPTS=\"-Xmx8G -Xms8G\"\n\n#export JMX_PORT=9999\n\n#export JMX_PROMETHEUS_PORT=9991\n\n#export KAFKA_JMX_OPTS=\"-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -javaagent:/root/certs/jmx_prometheus_javaagent-0.20.0.jar=$JMX_PROMETHEUS_PORT:/root/certs/kafka_broker.yml\"\n</code></pre> <p>Create Admin user for Kafka Server</p> <pre><code>bin/kafka-configs.sh --zk-tls-config-file /home/testing/certs/zk_tls_config.properties --zookeeper zkhost:12182 --alter --add-config 'SCRAM-SHA-512=[password='password']' --entity-type users --entity-name admin\n</code></pre> <p>Start the Kafka Server</p> <pre><code>bin/kafka-server-start.sh -daemon config/server.properties\n</code></pre> <p>Kafka ACl commands for authorization</p> <pre><code># Create Admin User\nkafka-configs.sh --zookeeper hostname1:12182 \\\n--alter --add-config 'SCRAM-SHA-512=[password=\"password\"]' \\\n--entity-type users --entity-name admin\n\n# Grant Producer Rights\nkafka-acls.sh --authorizer-properties zookeeper.connect=hostname1:12182 \\\n--add --allow-principal User:dlkdeveloper --producer \\\n--topic TEST --resource-pattern-type prefixed\n\n# Grant Consumer Rights\nkafka-acls.sh --authorizer-properties zookeeper.connect=hostname1:12182 \\\n--add --allow-principal User:$1 --consumer \\\n--group $1 --topic $2 --resource-pattern-type prefixed\n\n# List ACLs\nkafka-acls.sh --list --authorizer-properties zookeeper.connect=hostname1:12182\n\n# List Topics\nkafka-topics.sh --list \\\n--command-config /data1/kafkacerts/admin.properties \\\n--bootstrap-server hostname2:6667\n\n# Delete Topics\nkafka-topics.sh --delete --topic DL_TEST \\\n--bootstrap-server hostname1:6667 \\\n--command-config /data1/kafkacerts/admin.properties\n</code></pre>"},{"location":"tooling/kubernetes_install/","title":"Kubernetes Cluster Installation Guide","text":""},{"location":"tooling/kubernetes_install/#sanity-setup-and-pre-requisites","title":"Sanity Setup and Pre-requisites","text":"<p>Perform Sanity Checks on All Hosts</p> <pre><code># 1. Disable SELinux on all hosts.\n# 2. Docker user should have root access.\n# 3. Add host entries.\n# 4. Disable swap on all hosts.\n# 5. Enable passwordless SSH from docker user and root.\n</code></pre>"},{"location":"tooling/kubernetes_install/#master-node-setup","title":"Master Node Setup","text":"<p>SSH into the Master Node</p> <pre><code>ssh user@master-node\n</code></pre> <p>Disable Swap</p> <pre><code>swapoff -a\nsudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab\n</code></pre> <p>Configure Networking for Kubernetes</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\nsudo modprobe br_netfilter\n\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv4.ip_forward                 = 1\nEOF\n\nsudo sysctl --system\nlsmod | grep br_netfilter\nlsmod | grep overlay\nsysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward\nsysctl -p\n</code></pre> <p>Install Container Runtime (containerd)</p> RPM-based InstallationTar-based Installation <pre><code>yum install -y containerd-*.rpm\n</code></pre> <pre><code>curl -LO https://github.com/containerd/containerd/releases/download/v1.7.14/containerd-1.7.14-linux-amd64.tar.gz\nsudo tar Cxzvf /usr/local containerd-1.7.14-linux-amd64.tar.gz\ncurl -LO https://raw.githubusercontent.com/containerd/containerd/main/containerd.service\nsudo mkdir -p /usr/local/lib/systemd/system/\nsudo mv containerd.service /usr/local/lib/systemd/system/\nsudo mkdir -p /etc/containerd\ncontainerd config default | sudo tee /etc/containerd/config.toml\nsudo sed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml\n</code></pre> <p>Enable and Start containerd</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable --now containerd\nsystemctl status containerd\n</code></pre> <p>Install Runc</p> RPM-based InstallationTar-based Installation <pre><code>yum install -y runc\n</code></pre> <pre><code>curl -LO https://github.com/opencontainers/runc/releases/download/v1.1.12/runc.amd64\nsudo install -m 755 runc.amd64 /usr/local/sbin/runc\n</code></pre> <p>Install CNI Plugin</p> <pre><code>curl -LO https://github.com/containernetworking/plugins/releases/download/v1.5.0/cni-plugins-linux-amd64-v1.5.0.tgz\nsudo mkdir -p /opt/cni/bin\nsudo tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.5.0.tgz\n</code></pre> <p>Install Kubernetes Components</p> <pre><code># Download rpm from official kubernetes documentation\nyum install -y kube*.rpm\nkubeadm version\nkubelet --version\nkubectl version --client\n</code></pre> <p>Configure crictl for Containerd</p> <pre><code>sudo crictl config runtime-endpoint unix:///var/run/containerd/containerd.sock\n</code></pre> <p>Initialize Kubernetes Control Plane</p> <pre><code># Load necessary Kubernetes images before initializing\nkubeadm config images list\n# Example images:\n# registry.k8s.io/kube-apiserver:v1.30.1\n# registry.k8s.io/kube-controller-manager:v1.30.1\n# registry.k8s.io/kube-scheduler:v1.30.1\n# registry.k8s.io/kube-proxy:v1.30.1\n# registry.k8s.io/coredns/coredns:v1.11.1\n# registry.k8s.io/pause:3.9\n# registry.k8s.io/etcd:3.5.12-0\n\nkubeadm init --kubernetes-version=v1.30.1 \\\n  --control-plane-endpoint \"hostIP:6443\" \\\n  --upload-certs \\\n  --pod-network-cidr=10.244.0.0/16 \\\n  --apiserver-advertise-address=hostIP\n</code></pre> <p>Set Up kubeconfig for kubectl</p> <pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre> <p>Install Calico for Networking</p> <pre><code>wget https://github.com/manish-chet/DataEngineering/blob/main/kubernetes/calico_edited.yaml\nkubectl apply -f calico_edited.yaml\n</code></pre>"},{"location":"tooling/kubernetes_install/#worker-node-setup","title":"Worker Node Setup","text":"<p>Repeat Master node Setup Steps and Join Cluster</p> <p><pre><code># Repeat the above steps on all worker nodes, then join them to the cluster:\nsudo kubeadm join hostIP:6443 --token xxxxx --discovery-token-ca-cert-hash sha256:xxx\n</code></pre> If you need the join command again, run the following on the master node: <pre><code>kubeadm token create --print-join-command\n</code></pre></p>"},{"location":"tooling/kubernetes_install/#validation","title":"Validation","text":"<p>Validate Cluster Status</p> <pre><code>kubectl get nodes\nkubectl get pods -A\n</code></pre>"},{"location":"tooling/kubernetes_install/#references","title":"References","text":"<ul> <li>Official Kubernetes Documentation</li> <li>Mirantis Kubernetes Guide</li> <li>Containerd Setup Guide</li> <li>Kubernetes Networking</li> <li>Kubernetes Dashboard</li> </ul>"},{"location":"tooling/spark3standalone/","title":"Sparkstandalone Multi Node Setup","text":"<p>This setup includes Apache Spark cluster standalone installation and integration with hadoop on 3 Nodes.</p>"},{"location":"tooling/spark3standalone/#install-java","title":"Install Java","text":"<p>Install java on all 3 machines.</p>"},{"location":"tooling/spark3standalone/#configure-host-files","title":"Configure Host Files","text":"<p>Configure host names and host files for each node.</p> <p>Open the hosts file and Add IP addresses and hostnames of each node in the cluster.</p> <pre><code>IP1 master\nIP2 worker\nIP3 worker\n</code></pre>"},{"location":"tooling/spark3standalone/#configure-ssh","title":"Configure SSH","text":"<p>You need to install open ssh on each node and you need to configure a passwordless ssh connection between nodes.</p>"},{"location":"tooling/spark3standalone/#installing-and-confirguring-apache-spark","title":"Installing and Confirguring Apache Spark","text":"<p>We will do these steps on all nodes !!!</p> <p>Download Apache Spark</p> <pre><code>wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.2-bin-hadoop2.7.tgz\n</code></pre> <p>Extract the file</p> <pre><code>tar xvf spark-3.2.2-bin-hadoop2.7.tgz\n</code></pre> <p>Open .bashrc file and ddd following lines into .bashrc file</p> <pre><code>export JAVA_HOME=/home/spark/zulujdk11\nexport HADOOP_HOME=/home/spark/hadoop-2.7.7\nexport PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$PATH\nexport PATH=$PATH:/home/spark/spark-3.2.2-bin-hadoop2.7/bin\nexport SPARK_HOME=/home/spark/spark-3.2.2-bin-hadoop2.7\nexport SPARK_DAEMON_MEMORY=8g\nexport PYSPARK_PYTHON=/usr/bin/python3\n</code></pre> <p>Create spark-env.sh file by copying spark-env.sh.template and open this file </p> <pre><code>cd /home/spark/spark-3.2.2-bin-hadoop2.7/conf\n\ncp hadoopconfig.xml files into conf directory\ncp spark-env.sh.template spark.env.sh\n\nvim spark.env.sh\n</code></pre> <p>Add the following lines into spark-env.sh file </p> <p><pre><code>export SPARK_WORKER_CORES=32 #, to set the number of cores to use on this machine\nexport SPARK_WORKER_MEMORY=100g #, to set how much total memory workers have to give executors (e.g. 1000m, 2g)\nexport SPARK_WORKER_WEBUI_PORT=7090\nexport SPARK_WORKER_PORT=40774\nexport SPARK_MASTER_WEBUI_PORT=30000\nexport SPARK_WORKER_OPTS=\"-Dspark.worker.cleanup.enabled=true -Dspark.worker.cleanup.interval=7200 -Dspark.worker.cleanup.appDataTtl=259200\"\nexport SPARK_EXECUTOR_CORES=4 #, Number of cores for the executors (Default: 1).\nexport SPARK_EXECUTOR_MEMORY=30G #, Memory per Executor (e.g. 1000M, 2G) (Default: 1G)\nexport JAVA_HOME=/data5/spark/zulujdk11\nexport HADOOP_HOME=/home/spark/hadoop-2.7.7\nexport SPARK_LOCAL_IP=IP #, to set the IP address Spark binds to on this node\nexport SPARK_LOCAL_DIRS=/dev/shm/spark,/run/spark, \nexport SPARK_MASTER_HOST=IP\nexport PYSPARK_PYTHON=/usr/bin/python3\nexport PYSPARK_DRIVER_PYTHON=/usr/bin/python3\n</code></pre> Add following content in spark-defaults.conf</p> <pre><code>spark.master  spark://IP:7077\nspark.eventLog.enabled true\nspark.eventLog.dir file:///data2/spark/eventlogs\nspark.history.fs.logDirectory file:///data2/spark/eventlogs \nspark.history.ui.port 28080\nspark.history.fs.cleaner.enabled true\nspark.history.fs.cleaner.interval 1d\nspark.history.fs.cleaner.maxAge 3d\nspark.eventLog.rolling.enabled true\nspark.eventLog.rolling.maxFileSize 128m\nspark.eventLog.compress true\nspark.eventLog.compression.codec snappy\n</code></pre> <p>Create a bash script for delgation token and kinit</p> <pre><code>export PATH=/home/spark/spark-3.2.2-bin-hadoop2.7/bin:$PATH\nexport JAVA_HOME=/home/spark/zulujdk11\nkinit -kt /home/spark/keytabs/spark.keytab spark@REAL.com\nrm -rf /SPARKLOCK/spark_standalone/spark/keytabs/spark.token\nspark-submit --class org.apache.hadoop.hdfs.tools.DelegationTokenFetcher \"\" --renewer null spark/keytabs/spark.token\n#hdfs fetchdt --renewer null /home/spark/keytabs/spark.token\nfind /dev/shm/spark2 -type d -mmin +360 \\-exec rm -rf {} \\;\nfind /tmp -type d -mtime +1 -exec rm -rf {} \\;\n</code></pre>"},{"location":"tooling/spark3standalone/#start-spark-cluster","title":"Start Spark Cluster","text":"<p>start Master on master node in $SPARK_HOME/sbin directory</p> <pre><code>./start-master.sh\n</code></pre> <p>start worker on worker node in $SPARK_HOME/sbin directory</p> <pre><code>./start-worker.sh spark://IP:7077\n</code></pre> <p>Congratulations :)</p> <p>Your spark cluster is up and running !!!</p>"}]}